#!/usr/bin/env python3
#!/usr/bin/env python3
"""
AI Agent åç«¯æœåŠ¡ä¸»å…¥å£
æä¾› API æ¥å£ç”¨äº MCP é…ç½®ã€æ¨¡å‹é…ç½®å’Œè¯­éŸ³å¤„ç†
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path
import sys
from src.models import (
            LLMConfigCreate, LLMConfigUpdate, LLMConfigResponse, 
            MCPConfigCreate, MCPConfigUpdate, MCPConfigResponse,
            HealthResponse, SpeechRecognitionResponse,
            SpeechSynthesisResponse, SpeechSynthesisRequest, VoiceChatResponse,
            ChatRequest, ChatResponse, SystemStatusResponse
        )

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ Python è·¯å¾„ä¸­
project_root = Path(__file__).parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
def get_managers():
    """å»¶è¿Ÿå¯¼å…¥ç®¡ç†å™¨ç±»"""
    try:
        from src.config import ConfigManager
        from src.mcp import MCPManager
        from src.llm import LLMManager
        from src.speech import SpeechProcessor
        from src.logger import setup_logger, get_logger
        from src.utils import validate_config, generate_response_id
        
        return {
            'ConfigManager': ConfigManager,
            'MCPManager': MCPManager,
            'LLMManager': LLMManager,
            'SpeechProcessor': SpeechProcessor,
            'setup_logger': setup_logger,
            'get_logger': get_logger,
            'validate_config': validate_config,
            'generate_response_id': generate_response_id,
            # å¯¼å‡ºæ‰€æœ‰å“åº”æ¨¡å‹
            'HealthResponse': HealthResponse,
            'SpeechRecognitionResponse': SpeechRecognitionResponse,
            'SpeechSynthesisResponse': SpeechSynthesisResponse,
            'SpeechSynthesisRequest': SpeechSynthesisRequest,
            'VoiceChatResponse': VoiceChatResponse,
            'ChatRequest': ChatRequest,
            'ChatResponse': ChatResponse,
            'MCPConfigCreate': MCPConfigCreate,
            'MCPConfigUpdate': MCPConfigUpdate,
            'MCPConfigResponse': MCPConfigResponse,
            'LLMConfigCreate': LLMConfigCreate,
            'LLMConfigUpdate': LLMConfigUpdate,
            'LLMConfigResponse': LLMConfigResponse,
            'SystemStatusResponse': SystemStatusResponse
        }
    except ImportError as e:
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«é”™è¯¯ä¿¡æ¯çš„å­—å…¸
        return {'error': f"å¯¼å…¥å¤±è´¥: {str(e)}"}

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="AI Agent Backend API",
    description="AIæ™ºèƒ½ä»£ç†åç«¯æœåŠ¡ï¼Œæ”¯æŒMCPé…ç½®ã€æ¨¡å‹ç®¡ç†å’Œè¯­éŸ³å¤„ç†",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡ï¼ˆåœ¨startupæ—¶åˆå§‹åŒ–ï¼‰
managers = None
config_manager = None
mcp_manager = None
llm_manager = None
speech_processor = None
logger = None

def get_response_models():
    """è·å–å“åº”æ¨¡å‹ç±»"""
    global managers
    if managers is None or 'error' in managers:
        managers = get_managers()
    return managers

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    global managers, config_manager, mcp_manager, llm_manager, speech_processor, logger
    
    # è·å–ç®¡ç†å™¨ç±»
    managers = get_managers()

    # æ£€æŸ¥å¯¼å…¥æ˜¯å¦æˆåŠŸ
    if 'error' in managers:
        print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {managers['error']}")
        raise RuntimeError(f"æ¨¡å—å¯¼å…¥å¤±è´¥: {managers['error']}")
    
    # è®¾ç½®æ—¥å¿—
    managers['setup_logger']()
    logger = managers['get_logger'](__name__)
    
    logger.info("ğŸš€ AI Agent Backend æ­£åœ¨å¯åŠ¨...")
    
    try:
        # åˆå§‹åŒ–ç®¡ç†å™¨å®ä¾‹
        config_manager = managers['ConfigManager']()
        mcp_manager = managers['MCPManager']()
        llm_manager = managers['LLMManager']()
        speech_processor = managers['SpeechProcessor']()
        
        # ä¾æ¬¡åˆå§‹åŒ–
        await config_manager.initialize()
        logger.info("âœ… é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        await mcp_manager.initialize(config_manager.get_mcp_configs())
        logger.info("âœ… MCP ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        await llm_manager.initialize(config_manager.get_llm_configs())
        logger.info("âœ… LLM ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        await speech_processor.initialize()
        logger.info("âœ… è¯­éŸ³å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        logger.info("ğŸ‰ AI Agent Backend å¯åŠ¨æˆåŠŸï¼")
        
    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨å¤±è´¥: {str(e)}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶çš„æ¸…ç†"""
    if logger:
        logger.info("ğŸ”„ AI Agent Backend æ­£åœ¨å…³é—­...")
    
    try:
        if speech_processor:
            await speech_processor.cleanup()
        if llm_manager:
            await llm_manager.cleanup()
        if mcp_manager:
            await mcp_manager.cleanup()
        if config_manager:
            await config_manager.cleanup()
        
        if logger:
            logger.info("âœ… AI Agent Backend å·²å®‰å…¨å…³é—­")
        
    except Exception as e:
        if logger:
            logger.error(f"âŒ å…³é—­æ—¶å‡ºé”™: {str(e)}")

# ==================== ç³»ç»ŸçŠ¶æ€æ¥å£ ====================

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    models = get_response_models()

    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    HealthResponse = models['HealthResponse']
    
    if logger:
        logger.info("ğŸ” æ‰§è¡Œå¥åº·æ£€æŸ¥")
    
    try:
        status = {
            "status": "healthy",
            "timestamp": datetime.utcnow(),
            "version": "2.0.0",
            "components": {
                "config_manager": await config_manager.health_check() if config_manager else {"healthy": False},
                "mcp_manager": await mcp_manager.health_check() if mcp_manager else {"healthy": False},
                "llm_manager": await llm_manager.health_check() if llm_manager else {"healthy": False},
                "speech_processor": await speech_processor.health_check() if speech_processor else {"healthy": False}
            }
        }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç»„ä»¶ä¸å¥åº·
        unhealthy_components = [
            name for name, health in status["components"].items() 
            if not health.get("healthy", False)
        ]
        
        if unhealthy_components:
            status["status"] = "degraded"
            if logger:
                logger.warning(f"âš ï¸ éƒ¨åˆ†ç»„ä»¶ä¸å¥åº·: {unhealthy_components}")
        
        if logger:
            logger.info("âœ… å¥åº·æ£€æŸ¥å®Œæˆ")
        
        return HealthResponse(**status)
        
    except Exception as e:
        if logger:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/status", response_model=SystemStatusResponse)
async def get_system_status():
    """è·å–ç³»ç»Ÿè¯¦ç»†çŠ¶æ€"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    SystemStatusResponse = models['SystemStatusResponse']
    
    if logger:
        logger.info("ğŸ“Š è·å–ç³»ç»ŸçŠ¶æ€")
    
    try:
        status = {
            "success": True,
            "timestamp": datetime.utcnow(),
            "uptime": datetime.utcnow(),  # å®é™…åº”ç”¨ä¸­åº”è¯¥è®°å½•å¯åŠ¨æ—¶é—´
            "mcp_tools": await mcp_manager.get_tools_status() if mcp_manager else {},
            "llm_models": await llm_manager.get_models_status() if llm_manager else {},
            "active_sessions": await get_active_sessions_count(),
            "system_metrics": await get_system_metrics()
        }
        
        if logger:
            logger.info("âœ… ç³»ç»ŸçŠ¶æ€è·å–å®Œæˆ")
        
        return SystemStatusResponse(**status)
        
    except Exception as e:
        if logger:
            logger.error(f"âŒ è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== MCP é…ç½®æ¥å£ ====================

@app.get("/api/mcp/configs", response_model=List[MCPConfigResponse])
async def get_mcp_configs():
    """è·å–æ‰€æœ‰ MCP å·¥å…·é…ç½®"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    MCPConfigResponse = models['MCPConfigResponse']
    
    if logger:
        logger.info("ğŸ“‹ è·å– MCP é…ç½®åˆ—è¡¨")
    
    try:
        configs = await mcp_manager.get_all_configs()
        if logger:
            logger.info(f"âœ… è·å–åˆ° {len(configs)} ä¸ª MCP é…ç½®")
        return configs
        
    except Exception as e:
        logger.error(f"âŒ è·å– MCP é…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/mcp/configs", response_model=MCPConfigResponse)
async def create_mcp_config(config: MCPConfigCreate):
    """åˆ›å»ºæ–°çš„ MCP å·¥å…·é…ç½®"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    MCPConfigResponse = models['MCPConfigResponse']
    
    if logger:
        logger.info(f"â• åˆ›å»º MCP é…ç½®: {config.name}")
    
    try:
        # éªŒè¯é…ç½®
        await validate_config(config.dict(), "mcp")
        
        # åˆ›å»ºé…ç½®
        result = await mcp_manager.create_config(config)
        
        # ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
        await config_manager.save_mcp_config(result)
        
        logger.info(f"âœ… MCP é…ç½®åˆ›å»ºæˆåŠŸ: {result.name}")
        return result
        
    except ValueError as e:
        logger.error(f"âŒ MCP é…ç½®éªŒè¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"âŒ åˆ›å»º MCP é…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/mcp/configs/{config_id}", response_model=MCPConfigResponse)
async def update_mcp_config(config_id: str, config: MCPConfigUpdate):
    """æ›´æ–° MCP å·¥å…·é…ç½®"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    MCPConfigResponse = models['MCPConfigResponse']
    
    if logger:
        logger.info(f"âœï¸ æ›´æ–° MCP é…ç½®: {config_id}")
    
    try:
        # éªŒè¯é…ç½®å­˜åœ¨
        existing_config = await mcp_manager.get_config(config_id)
        if not existing_config:
            raise HTTPException(status_code=404, detail=f"MCP é…ç½®ä¸å­˜åœ¨: {config_id}")
        
        # æ›´æ–°é…ç½®
        result = await mcp_manager.update_config(config_id, config)
        
        # ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
        await config_manager.save_mcp_config(result)
        
        logger.info(f"âœ… MCP é…ç½®æ›´æ–°æˆåŠŸ: {config_id}")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ æ›´æ–° MCP é…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/mcp/configs/{config_id}")
async def delete_mcp_config(config_id: str):
    """åˆ é™¤ MCP å·¥å…·é…ç½®"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    MCPConfigResponse = models['MCPConfigResponse']
    
    if logger:
        logger.info(f"ğŸ—‘ï¸ åˆ é™¤ MCP é…ç½®: {config_id}")
    
    try:
        # åœæ­¢ç›¸å…³çš„ MCP å®¢æˆ·ç«¯
        await mcp_manager.stop_client(config_id)
        
        # åˆ é™¤é…ç½®
        await mcp_manager.delete_config(config_id)
        
        # ä»é…ç½®æ–‡ä»¶ä¸­åˆ é™¤
        await config_manager.delete_mcp_config(config_id)
        
        logger.info(f"âœ… MCP é…ç½®åˆ é™¤æˆåŠŸ: {config_id}")
        return {"message": f"MCP é…ç½® {config_id} å·²åˆ é™¤"}
        
    except Exception as e:
        logger.error(f"âŒ åˆ é™¤ MCP é…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/mcp/configs/{config_id}/test")
async def test_mcp_config(config_id: str):  
    """æµ‹è¯• MCP å·¥å…·é…ç½®"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    MCPConfigResponse = models['MCPConfigResponse']
    
    if logger:
        logger.info(f"ğŸ§ª æµ‹è¯• MCP é…ç½®: {config_id}")
    
    try:
        result = await mcp_manager.test_config(config_id)
        
        if result["success"]:
            logger.info(f"âœ… MCP é…ç½®æµ‹è¯•æˆåŠŸ: {config_id}")
        else:
            logger.warning(f"âš ï¸ MCP é…ç½®æµ‹è¯•å¤±è´¥: {config_id} - {result.get('error')}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯• MCP é…ç½®å¼‚å¸¸: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== LLM æ¨¡å‹é…ç½®æ¥å£ ====================

@app.get("/api/llm/configs", response_model=List[LLMConfigResponse])
async def get_llm_configs():
    """è·å–æ‰€æœ‰ LLM æ¨¡å‹é…ç½®"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    LLMConfigResponse = models['LLMConfigResponse']
    
    if logger:
        logger.info("ğŸ“‹ è·å– LLM é…ç½®åˆ—è¡¨")
    
    try:
        configs = await llm_manager.get_all_configs()
        if logger:
            logger.info(f"âœ… è·å–åˆ° {len(configs)} ä¸ª LLM é…ç½®")
        return configs
        
    except Exception as e:
        logger.error(f"âŒ è·å– LLM é…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/llm/configs", response_model=LLMConfigResponse)
async def create_llm_config(config: LLMConfigCreate):
    """åˆ›å»ºæ–°çš„ LLM æ¨¡å‹é…ç½®"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    LLMConfigResponse = models['LLMConfigResponse']
    
    if logger:
        logger.info(f"â• åˆ›å»º LLM é…ç½®: {config.name}")
    
    try:
        # éªŒè¯é…ç½®
        await validate_config(config.dict(), "llm")
        
        # åˆ›å»ºé…ç½®
        result = await llm_manager.create_config(config)
        
        # ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
        await config_manager.save_llm_config(result)
        
        logger.info(f"âœ… LLM é…ç½®åˆ›å»ºæˆåŠŸ: {result.name}")
        return result
        
    except ValueError as e:
        logger.error(f"âŒ LLM é…ç½®éªŒè¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"âŒ åˆ›å»º LLM é…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/llm/configs/{config_id}", response_model=LLMConfigResponse)
async def update_llm_config(config_id: str, config: LLMConfigUpdate):
    """æ›´æ–° LLM æ¨¡å‹é…ç½®"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    LLMConfigResponse = models['LLMConfigResponse']
    
    if logger:
        logger.info(f"âœï¸ æ›´æ–° LLM é…ç½®: {config_id}")
    
    try:
        # éªŒè¯é…ç½®å­˜åœ¨
        existing_config = await llm_manager.get_config(config_id)
        if not existing_config:
            raise HTTPException(status_code=404, detail=f"LLM é…ç½®ä¸å­˜åœ¨: {config_id}")
        
        # æ›´æ–°é…ç½®
        result = await llm_manager.update_config(config_id, config)
        
        # ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
        await config_manager.save_llm_config(result)
        
        logger.info(f"âœ… LLM é…ç½®æ›´æ–°æˆåŠŸ: {config_id}")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ æ›´æ–° LLM é…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/llm/configs/{config_id}")
async def delete_llm_config(config_id: str):
    """åˆ é™¤ LLM æ¨¡å‹é…ç½®"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    LLMConfigResponse = models['LLMConfigResponse']
    
    if logger:
        logger.info(f"ğŸ—‘ï¸ åˆ é™¤ LLM é…ç½®: {config_id}")
    
    try:
        # åˆ é™¤é…ç½®
        await llm_manager.delete_config(config_id)
        
        # ä»é…ç½®æ–‡ä»¶ä¸­åˆ é™¤
        await config_manager.delete_llm_config(config_id)
        
        logger.info(f"âœ… LLM é…ç½®åˆ é™¤æˆåŠŸ: {config_id}")
        return {"message": f"LLM é…ç½® {config_id} å·²åˆ é™¤"}
        
    except Exception as e:
        logger.error(f"âŒ åˆ é™¤ LLM é…ç½®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/llm/configs/{config_id}/test")
async def test_llm_config(config_id: str):  
    """æµ‹è¯• LLM æ¨¡å‹é…ç½®"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    LLMConfigResponse = models['LLMConfigResponse']
    
    if logger:
        logger.info(f"ğŸ§ª æµ‹è¯• LLM é…ç½®: {config_id}")
    
    try:
        result = await llm_manager.test_config(config_id)
        
        if result["success"]:
            logger.info(f"âœ… LLM é…ç½®æµ‹è¯•æˆåŠŸ: {config_id}")
        else:
            logger.warning(f"âš ï¸ LLM é…ç½®æµ‹è¯•å¤±è´¥: {config_id} - {result.get('error')}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯• LLM é…ç½®å¼‚å¸¸: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== è¯­éŸ³å¤„ç†æ¥å£ ====================

@app.post("/api/speech/recognize", response_model=SpeechRecognitionResponse)
async def recognize_speech(
    audio_file: UploadFile = File(...),
    language: Optional[str] = "zh-CN",
    use_asr_model: Optional[str] = None
):
    """è¯­éŸ³è¯†åˆ«æ¥å£"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    SpeechRecognitionResponse = models['SpeechRecognitionResponse']
    
    request_id = generate_response_id()
    logger.info(f"ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ« [è¯·æ±‚ID: {request_id}] - æ–‡ä»¶: {audio_file.filename}")
    
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not audio_file.content_type.startswith('audio/'):
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å¿…é¡»æ˜¯éŸ³é¢‘æ ¼å¼")
        
        # è¯»å–éŸ³é¢‘æ•°æ®
        audio_data = await audio_file.read()
        logger.info(f"ğŸ“ éŸ³é¢‘æ–‡ä»¶è¯»å–å®Œæˆ [è¯·æ±‚ID: {request_id}] - å¤§å°: {len(audio_data)} bytes")
        
        # æ‰§è¡Œè¯­éŸ³è¯†åˆ«
        result = await speech_processor.recognize(
            audio_data=audio_data,
            language=language,
            model_name=use_asr_model,
            request_id=request_id
        )
        
        logger.info(f"âœ… è¯­éŸ³è¯†åˆ«å®Œæˆ [è¯·æ±‚ID: {request_id}] - æ–‡æœ¬é•¿åº¦: {len(result.text)}")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥ [è¯·æ±‚ID: {request_id}]: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/speech/synthesize", response_model=SpeechSynthesisResponse)
async def synthesize_speech(request: SpeechSynthesisRequest):
    """è¯­éŸ³åˆæˆæ¥å£"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    SpeechSynthesisResponse = models['SpeechSynthesisResponse']
    
    request_id = generate_response_id()
    logger.info(f"ğŸ”Š å¼€å§‹è¯­éŸ³åˆæˆ [è¯·æ±‚ID: {request_id}] - æ–‡æœ¬é•¿åº¦: {len(request.text)}")
    
    try:
        # æ‰§è¡Œè¯­éŸ³åˆæˆ
        result = await speech_processor.synthesize(
            text=request.text,
            voice=request.voice,
            language=request.language,
            speed=request.speed,
            pitch=request.pitch,
            tts_model=request.tts_model,
            request_id=request_id
        )
        
        logger.info(f"âœ… è¯­éŸ³åˆæˆå®Œæˆ [è¯·æ±‚ID: {request_id}] - éŸ³é¢‘å¤§å°: {len(result.audio_data)} bytes")
        return result
        
    except Exception as e:
        logger.error(f"âŒ è¯­éŸ³åˆæˆå¤±è´¥ [è¯·æ±‚ID: {request_id}]: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/chat/voice", response_model=VoiceChatResponse)
async def voice_chat(
    audio_file: UploadFile = File(...),
    llm_model: Optional[str] = None,
    system_prompt: Optional[str] = None,
    session_id: Optional[str] = None
):
    """è¯­éŸ³å¯¹è¯æ¥å£ï¼ˆè¯­éŸ³è¾“å…¥ + æ–‡æœ¬å’Œè¯­éŸ³è¾“å‡ºï¼‰"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    VoiceChatResponse = models['VoiceChatResponse']
    
    request_id = generate_response_id()
    logger.info(f"ğŸ’¬ å¼€å§‹è¯­éŸ³å¯¹è¯ [è¯·æ±‚ID: {request_id}] - ä¼šè¯ID: {session_id}")
    
    try:
        # 1. è¯­éŸ³è¯†åˆ«
        audio_data = await audio_file.read()
        logger.info(f"ğŸ¤ æ‰§è¡Œè¯­éŸ³è¯†åˆ« [è¯·æ±‚ID: {request_id}]")
        
        recognition_result = await speech_processor.recognize(
            audio_data=audio_data,
            request_id=request_id
        )
        
        user_text = recognition_result.text
        logger.info(f"ğŸ”¤ è¯†åˆ«ç»“æœ [è¯·æ±‚ID: {request_id}]: {user_text}")
        
        # 2. LLM å¯¹è¯
        logger.info(f"ğŸ¤– è°ƒç”¨ LLM æ¨¡å‹ [è¯·æ±‚ID: {request_id}]")
        
        chat_response = await llm_manager.chat(
            model_name=llm_model,
            message=user_text,
            system_prompt=system_prompt,
            session_id=session_id,
            request_id=request_id
        )
        
        response_text = chat_response.content
        logger.info(f"ğŸ’­ LLM å“åº” [è¯·æ±‚ID: {request_id}]: {response_text[:100]}...")
        
        # 3. è¯­éŸ³åˆæˆ
        logger.info(f"ğŸ”Š æ‰§è¡Œè¯­éŸ³åˆæˆ [è¯·æ±‚ID: {request_id}]")
        
        synthesis_result = await speech_processor.synthesize(
            text=response_text,
            request_id=request_id
        )
        
        logger.info(f"âœ… è¯­éŸ³å¯¹è¯å®Œæˆ [è¯·æ±‚ID: {request_id}]")
        
        return VoiceChatResponse(
            request_id=request_id,
            user_text=user_text,
            response_text=response_text,
            response_audio=synthesis_result.audio_data,
            audio_format=synthesis_result.format,
            session_id=session_id or request_id,
            model_used=chat_response.model_name,
            processing_time={
                "recognition": recognition_result.processing_time,
                "llm_chat": chat_response.processing_time,
                "synthesis": synthesis_result.processing_time
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ è¯­éŸ³å¯¹è¯å¤±è´¥ [è¯·æ±‚ID: {request_id}]: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== LLM å¯¹è¯æ¥å£ ====================

@app.post("/api/chat/text", response_model=ChatResponse)
async def text_chat(request: ChatRequest):
    """æ–‡æœ¬å¯¹è¯æ¥å£"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    ChatResponse = models['ChatResponse']
    
    request_id = generate_response_id()
    logger.info(f"ğŸ’¬ å¼€å§‹æ–‡æœ¬å¯¹è¯ [è¯·æ±‚ID: {request_id}] - æ¨¡å‹: {request.model_name}")
    
    try:
        result = await llm_manager.chat(
            model_name=request.model_name,
            message=request.message,
            system_prompt=request.system_prompt,
            session_id=request.session_id,
            stream=request.stream,
            tools=request.tools,
            request_id=request_id
        )
        
        logger.info(f"âœ… æ–‡æœ¬å¯¹è¯å®Œæˆ [è¯·æ±‚ID: {request_id}]")
        return result
        
    except Exception as e:
        logger.error(f"âŒ æ–‡æœ¬å¯¹è¯å¤±è´¥ [è¯·æ±‚ID: {request_id}]: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/chat/stream")
async def stream_chat(request: ChatRequest):
    """æµå¼æ–‡æœ¬å¯¹è¯æ¥å£"""
    models = get_response_models()
    
    if 'error' in models:
        raise HTTPException(status_code=500, detail=f"æ¨¡å—å¯¼å…¥é”™è¯¯: {models['error']}")
    
    ChatResponse = models['ChatResponse']
    
    request_id = generate_response_id()
    logger.info(f"ğŸŒŠ å¼€å§‹æµå¼å¯¹è¯ [è¯·æ±‚ID: {request_id}] - æ¨¡å‹: {request.model_name}")
    
    try:
        async def generate():
            async for chunk in llm_manager.stream_chat(
                model_name=request.model_name,
                message=request.message,
                system_prompt=request.system_prompt,
                session_id=request.session_id,
                tools=request.tools,
                request_id=request_id
            ):
                yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
            
            logger.info(f"âœ… æµå¼å¯¹è¯å®Œæˆ [è¯·æ±‚ID: {request_id}]")
        
        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Request-ID": request_id
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ æµå¼å¯¹è¯å¤±è´¥ [è¯·æ±‚ID: {request_id}]: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== è¾…åŠ©å‡½æ•° ====================

async def get_active_sessions_count() -> int:
    """è·å–æ´»è·ƒä¼šè¯æ•°é‡"""
    try:
        if llm_manager:
            return await llm_manager.get_active_sessions_count()
        return 0
    except:
        return 0

async def get_system_metrics() -> Dict[str, Any]:
    """è·å–ç³»ç»ŸæŒ‡æ ‡"""
    try:
        import psutil
        
        return {
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_percent": psutil.disk_usage('/').percent
        }
    except:
        return {}

# ==================== ä¸»ç¨‹åºå…¥å£ ====================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="AI Agent Backend Server")
    parser.add_argument("--host", default="0.0.0.0", help="ç›‘å¬ä¸»æœº")
    parser.add_argument("--port", type=int, default=8000, help="ç›‘å¬ç«¯å£")
    parser.add_argument("--reload", action="store_true", help="å¼€å‘æ¨¡å¼ï¼ˆçƒ­é‡è½½ï¼‰")
    parser.add_argument("--log-level", default="info", help="æ—¥å¿—çº§åˆ«")
    
    args = parser.parse_args()
    
    logger.info(f"ğŸš€ å¯åŠ¨ AI Agent Backend Server")
    logger.info(f"ğŸ“ åœ°å€: http://{args.host}:{args.port}")
    logger.info(f"ğŸ“– API æ–‡æ¡£: http://{args.host}:{args.port}/docs")
    
    uvicorn.run(
        "main:app",
        host=args.host,
        port=args.port,
        reload=args.reload,
        log_level=args.log_level
    )