"""
è¯­éŸ³å¤„ç†ç›¸å…³æ¥å£æ¨¡å—
"""

from fastapi import APIRouter, HTTPException, UploadFile, File
from starlette.responses import StreamingResponse
from typing import Optional
import re
from src.models import (
    SpeechRecognitionResponse, SpeechSynthesisResponse, 
    SpeechSynthesisRequest, VoiceChatResponse
)
from src.utils import generate_response_id
import json
import time
import base64
import asyncio

router = APIRouter()

# å…¨å±€å˜é‡å¼•ç”¨
def get_managers():
    """è·å–å…¨å±€ç®¡ç†å™¨å®ä¾‹"""
    from main import speech_processor, llm_manager, logger, mongodb_manager
    return {
        'speech_processor': speech_processor,
        'llm_manager': llm_manager,
        'logger': logger,
        'mongodb_manager': mongodb_manager
    }

# ==================== è¯­éŸ³å¤„ç†æ¥å£ ====================

@router.post("/api/speech/recognize", response_model=SpeechRecognitionResponse)
async def recognize_speech(
    audio_file: UploadFile = File(...),
    language: Optional[str] = "zh-CN",
    use_asr_model: Optional[str] = None
):
    """è¯­éŸ³è¯†åˆ«æ¥å£"""
    managers = get_managers()
    logger = managers['logger']
    
    request_id = generate_response_id()
    logger.info(f"å¼€å§‹è¯­éŸ³è¯†åˆ« [è¯·æ±‚ID: {request_id}] - æ–‡ä»¶: {audio_file.filename}")
    
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not audio_file.content_type.startswith('audio/'):
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å¿…é¡»æ˜¯éŸ³é¢‘æ ¼å¼")
        
        # è¯»å–éŸ³é¢‘æ•°æ®
        audio_data = await audio_file.read()
        logger.info(f"éŸ³é¢‘æ–‡ä»¶è¯»å–å®Œæˆ [è¯·æ±‚ID: {request_id}] - å¤§å°: {len(audio_data)} bytes")
        
        # æ‰§è¡Œè¯­éŸ³è¯†åˆ«
        result = await managers['speech_processor'].recognize(
            audio_data=audio_data,
            language=language,
            model_name=use_asr_model,
            request_id=request_id
        )
        
        logger.info(f"è¯­éŸ³è¯†åˆ«å®Œæˆ [è¯·æ±‚ID: {request_id}] - æ–‡æœ¬é•¿åº¦: {len(result.text)}")
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥ [è¯·æ±‚ID: {request_id}]: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/api/speech/synthesize", response_model=SpeechSynthesisResponse)
async def synthesize_speech(request: SpeechSynthesisRequest):
    """è¯­éŸ³åˆæˆæ¥å£"""
    managers = get_managers()
    logger = managers['logger']
    
    request_id = generate_response_id()
    logger.info(f"å¼€å§‹è¯­éŸ³åˆæˆ [è¯·æ±‚ID: {request_id}] - æ–‡æœ¬é•¿åº¦: {len(request.text)}")
    
    try:
        # æ‰§è¡Œè¯­éŸ³åˆæˆ
        result = await managers['speech_processor'].synthesize(
            text=request.text,
            voice=request.voice,
            language=request.language,
            speed=request.speed,
            pitch=request.pitch,
            tts_model=request.tts_model,
            request_id=request_id
        )
        
        logger.info(f"è¯­éŸ³åˆæˆå®Œæˆ [è¯·æ±‚ID: {request_id}] - éŸ³é¢‘å¤§å°: {len(result.audio_data)} bytes")
        return result
        
    except Exception as e:
        logger.error(f"è¯­éŸ³åˆæˆå¤±è´¥ [è¯·æ±‚ID: {request_id}]: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/api/chat/voice", response_model=VoiceChatResponse)
async def voice_chat(
    audio_file: UploadFile = File(...),
    llm_model: Optional[str] = None,
    system_prompt: Optional[str] = None,
    session_id: Optional[str] = None
):
    """è¯­éŸ³å¯¹è¯æ¥å£ï¼ˆè¯­éŸ³è¾“å…¥ + æ–‡æœ¬å’Œè¯­éŸ³è¾“å‡ºï¼‰"""
    managers = get_managers()
    logger = managers['logger']
    
    request_id = generate_response_id()
    logger.info(f"å¼€å§‹è¯­éŸ³å¯¹è¯ [è¯·æ±‚ID: {request_id}] - ä¼šè¯ID: {session_id}")
    
    try:
        # 1. è¯­éŸ³è¯†åˆ«
        audio_data = await audio_file.read()
        logger.info(f"æ‰§è¡Œè¯­éŸ³è¯†åˆ« [è¯·æ±‚ID: {request_id}]")
        
        recognition_result = await managers['speech_processor'].recognize(
            audio_data=audio_data,
            request_id=request_id
        )
        
        user_text = recognition_result.text
        user_text = re.sub(r'<\s*\|\s*[^|]+\s*\|\s*>', '', user_text).strip()
        logger.info(f"è¯†åˆ«ç»“æœ [è¯·æ±‚ID: {request_id}]: {user_text}")
        
        # 2. LLM å¯¹è¯
        logger.info(f"è°ƒç”¨ LLM æ¨¡å‹ [è¯·æ±‚ID: {request_id}], è¯·æ±‚å†…å®¹: {user_text}, ç³»ç»Ÿæç¤º: {system_prompt}")
        
        chat_response = await managers['llm_manager'].chat(
            model_name=llm_model,
            message=user_text,
            system_prompt=system_prompt,
            session_id=session_id,
            request_id=request_id
        )
        
        response_text = chat_response["content"]
        logger.info(f"LLM å“åº” [è¯·æ±‚ID: {request_id}]: {response_text[:100]}...")
        
        # 3. è¯­éŸ³åˆæˆ
        logger.info(f"æ‰§è¡Œè¯­éŸ³åˆæˆ [è¯·æ±‚ID: {request_id}]")
        
        synthesis_result = await managers['speech_processor'].synthesize(
            text=response_text,
            request_id=request_id
        )
        
        logger.info(f"è¯­éŸ³å¯¹è¯å®Œæˆ [è¯·æ±‚ID: {request_id}]")
        
        return VoiceChatResponse(
            request_id=request_id,
            user_text=user_text,
            response_text=response_text,
            response_audio=synthesis_result.audio_data,
            audio_format=synthesis_result.format,
            session_id=session_id or request_id,
            model_used=chat_response["model_name"],
            processing_time={
                "recognition": recognition_result.processing_time,
                "llm_chat": chat_response["processing_time"],
                "synthesis": synthesis_result.processing_time
            }
        )
        
    except Exception as e:
        logger.error(f"è¯­éŸ³å¯¹è¯å¤±è´¥ [è¯·æ±‚ID: {request_id}]: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

async def process_system_prompt_with_documents(system_prompt: str, mongodb_manager, logger, request_id: str) -> str:
    """
    å¤„ç†ç³»ç»Ÿæç¤ºè¯ä¸­çš„documentsIdï¼Œå°†å…¶æ›¿æ¢ä¸ºå®é™…çš„æ–‡æ¡£æ•°æ®
    
    Args:
        system_prompt: åŸå§‹ç³»ç»Ÿæç¤ºè¯
        mongodb_manager: MongoDBç®¡ç†å™¨å®ä¾‹
        logger: æ—¥å¿—è®°å½•å™¨
        request_id: è¯·æ±‚ID
    
    Returns:
        å¤„ç†åçš„ç³»ç»Ÿæç¤ºè¯
    """
    if not system_prompt:
        return system_prompt
    
    try:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… {"documentsId":"..."}
        pattern = r'\{"documentsId":\s*"([^"]+)"\}'
        match = re.search(pattern, system_prompt)
        
        if not match:
            logger.info(f"ç³»ç»Ÿæç¤ºè¯ä¸­æœªæ‰¾åˆ°documentsId [è¯·æ±‚ID: {request_id}]")
            return system_prompt
        
        document_id = match.group(1)
        logger.info(f"ä»ç³»ç»Ÿæç¤ºè¯ä¸­æå–åˆ°documentsId: {document_id} [è¯·æ±‚ID: {request_id}]")
        
        # ä½¿ç”¨mongodb_managerè·å–æ–‡æ¡£æ•°æ®
        try:
            document = await mongodb_manager.get_document(document_id)
            if not document:
                logger.warning(f"æœªæ‰¾åˆ°IDä¸º {document_id} çš„æ–‡æ¡£ [è¯·æ±‚ID: {request_id}]")
                return system_prompt
            
            logger.info(f"æˆåŠŸè·å–æ–‡æ¡£: {document.name}, æ•°æ®é¡¹æ•°é‡: {len(document.data_list)} [è¯·æ±‚ID: {request_id}]")
            
            # æ„å»ºæ–°çš„æ•°æ®æ ¼å¼
            document_data = []
            for item in document.data_list:
                page_data = {
                    "page": item.sequence,
                    "content": item.text
                }
                document_data.append(page_data)
            
            logger.info(f"æ„å»ºæ–‡æ¡£æ•°æ®å®Œæˆï¼ŒåŒ…å« {len(document_data)} ä¸ªé¡µé¢ [è¯·æ±‚ID: {request_id}]")
            
            # å°†æ–‡æ¡£æ•°æ®è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            document_data_json = json.dumps(document_data, ensure_ascii=False)
            
            # æ›¿æ¢åŸæ¥çš„{"documentsId":"..."}ä¸ºç”Ÿæˆçš„æ•°æ®
            processed_prompt = re.sub(pattern, document_data_json, system_prompt)
            
            logger.info(f"ç³»ç»Ÿæç¤ºè¯å¤„ç†å®Œæˆï¼Œæ›¿æ¢äº†documentsIdä¸ºå®é™…æ–‡æ¡£æ•°æ® [è¯·æ±‚ID: {request_id}]")
            logger.debug(f"å¤„ç†åçš„ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(processed_prompt)} [è¯·æ±‚ID: {request_id}]")
            
            return processed_prompt
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£ {document_id} å¤±è´¥: {str(e)} [è¯·æ±‚ID: {request_id}]")
            # å¦‚æœè·å–æ–‡æ¡£å¤±è´¥ï¼Œè¿”å›åŸå§‹ç³»ç»Ÿæç¤ºè¯
            return system_prompt
            
    except Exception as e:
        logger.error(f"å¤„ç†ç³»ç»Ÿæç¤ºè¯ä¸­çš„documentsIdå¤±è´¥: {str(e)} [è¯·æ±‚ID: {request_id}]")
        return system_prompt

class TextSegmentProcessor:
    """æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨ï¼Œç¡®ä¿ä¸é‡å¤å¤„ç†ç›¸åŒçš„æ–‡æœ¬æ®µ"""
    
    def __init__(self, request_id: str, logger, min_segment_length: int = 40):
        self.request_id = request_id
        self.logger = logger
        self.min_segment_length = min_segment_length
        self.segment_markers = ["ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", ".", "!", "?", ";", "\n"]
        
        # çŠ¶æ€ç®¡ç†
        self.json_buffer = ""
        self.content_buffer = ""
        self.current_page = None
        self.found_content = False
        self.segment_counter = 0
        self.json_complete = False
        self.last_processed_pos = 0  # ä¿®æ”¹ï¼šä½¿ç”¨ä½ç½®è€Œä¸æ˜¯æ–‡æœ¬æ¥è·Ÿè¸ªå¤„ç†è¿›åº¦
        
        self.logger.info(f"ğŸ”§ åˆå§‹åŒ–æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨ [è¯·æ±‚ID: {request_id}]")
    
    def add_chunk(self, text_chunk: str):
        """æ·»åŠ æ–°çš„æ–‡æœ¬å—"""
        if not text_chunk:
            return
            
        self.logger.debug(f"ğŸ“ æ·»åŠ æ–‡æœ¬å—: '{text_chunk[:50]}...', å½“å‰ç¼“å†²åŒºé•¿åº¦: {len(self.content_buffer)}")
        
        # ç´¯ç§¯åˆ°JSONç¼“å†²åŒº
        self.json_buffer += text_chunk
        
        # å¦‚æœè¿˜æ²¡æœ‰å®ŒæˆJSONè§£æ
        if not self.json_complete:
            self._try_parse_json()
        else:
            # JSONå·²ç»è§£æå®Œæˆï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„JSONå¼€å§‹
            if text_chunk.strip().startswith('{'):
                self.logger.info("ğŸ”„ æ£€æµ‹åˆ°æ–°çš„JSONå“åº”å¼€å§‹ï¼Œé‡ç½®çŠ¶æ€")
                self._reset_for_new_json(text_chunk)
            else:
                # æ¸…ç†å¹¶æ·»åŠ åˆ°content_buffer
                clean_chunk = self._clean_text_chunk(text_chunk)
                if clean_chunk and not clean_chunk.isspace():
                    old_length = len(self.content_buffer)
                    self.content_buffer += clean_chunk
                    self.logger.debug(f"â• æ·»åŠ æ¸…ç†åçš„æ–‡æœ¬: '{clean_chunk[:30]}...', ç¼“å†²åŒºé•¿åº¦: {old_length} -> {len(self.content_buffer)}")
    
    def _try_parse_json(self):
        """å°è¯•è§£æJSON"""
        try:
            # å°è¯•è§£æå®Œæ•´çš„JSON
            if self.json_buffer.strip().endswith('}'):
                parsed = json.loads(self.json_buffer.strip())
                if 'content' in parsed:
                    # JSONè§£æå®Œæˆï¼Œåªå–contentå†…å®¹
                    new_content = parsed['content']
                    self.current_page = parsed.get('page')
                    self.found_content = True
                    self.json_complete = True
                    
                    # ä¿®æ”¹ï¼šåªæ›´æ–°æ–°å¢çš„å†…å®¹ï¼Œä¸é‡ç½®å·²å¤„ç†çš„ä½ç½®
                    if len(new_content) > len(self.content_buffer):
                        self.content_buffer = new_content
                        self.logger.info(f"âœ… å®Œæ•´è§£æJSON - é¡µç : {self.current_page}, å†…å®¹é•¿åº¦: {len(self.content_buffer)}")
                    
                    # æ¸…ç©ºJSONç¼“å†²åŒºï¼Œé¿å…é‡å¤å¤„ç†
                    self.json_buffer = ""
            else:
                # å°è¯•éƒ¨åˆ†è§£æ
                self._try_partial_parse()
        except json.JSONDecodeError:
            # ç»§ç»­ç´¯ç§¯ï¼Œç­‰å¾…æ›´å¤šæ•°æ®
            pass
        except Exception as e:
            self.logger.warning(f"âš ï¸ JSONè§£æè­¦å‘Š: {e}")
    
    def _try_partial_parse(self):
        """å°è¯•éƒ¨åˆ†è§£æJSON"""
        # å°è¯•éƒ¨åˆ†è§£æé¡µç 
        if not self.current_page:
            partial_match = re.search(r'"page":\s*([^,}]+)', self.json_buffer)
            if partial_match:
                try:
                    self.current_page = json.loads(partial_match.group(1).strip())
                    self.logger.info(f"ğŸ“„ æå–åˆ°é¡µç : {self.current_page}")
                except:
                    self.current_page = partial_match.group(1).strip(' "\'')
        
        # å°è¯•éƒ¨åˆ†è§£æcontent
        if not self.found_content:
            content_match = re.search(r'"content":\s*"([^"]*(?:\\.[^"]*)*)"', self.json_buffer)
            if content_match:
                new_content = content_match.group(1).replace('\\"', '"').replace('\\n', '\n')
                # ä¿®æ”¹ï¼šåªåœ¨å†…å®¹çœŸæ­£å¢é•¿æ—¶æ‰æ›´æ–°
                if len(new_content) > len(self.content_buffer):
                    self.content_buffer = new_content
                    self.found_content = True
                    self.logger.info(f"ğŸ“ éƒ¨åˆ†è§£æåˆ°content: {self.content_buffer[:50]}...")
    
    def _reset_for_new_json(self, text_chunk: str):
        """ä¸ºæ–°çš„JSONé‡ç½®çŠ¶æ€"""
        self.json_buffer = text_chunk
        self.json_complete = False
        # ä¿®æ”¹ï¼šä¸é‡ç½®å·²å¤„ç†ä½ç½®ï¼Œé¿å…é‡å¤å¤„ç†
        # self.last_processed_pos ä¿æŒä¸å˜
    
    def _clean_text_chunk(self, text_chunk: str) -> str:
        """æ¸…ç†æ–‡æœ¬å—"""
        clean_chunk = text_chunk
        # ç§»é™¤å¯èƒ½çš„JSONç»“æŸç¬¦
        if '}' in clean_chunk:
            clean_chunk = clean_chunk.split('}')[0]
        # æ¸…ç†è½¬ä¹‰å­—ç¬¦
        clean_chunk = clean_chunk.replace('\\n', '\n').replace('\\"', '"')
        # ç§»é™¤JSONæ ¼å¼å­—ç¬¦
        clean_chunk = re.sub(r'^[",\s]+|[",\s]+$', '', clean_chunk)
        return clean_chunk
    
    def get_next_segment(self) -> tuple[str, bool]:
        """
        è·å–ä¸‹ä¸€ä¸ªå¯å¤„ç†çš„æ–‡æœ¬æ®µ
        è¿”å›: (segment_text, has_more)
        """
        if not self.found_content or len(self.content_buffer) <= self.last_processed_pos:
            return "", False
        
        # ä¿®æ”¹ï¼šæ£€æŸ¥å‰©ä½™æœªå¤„ç†çš„å†…å®¹é•¿åº¦
        remaining_content = self.content_buffer[self.last_processed_pos:]
        if len(remaining_content) < self.min_segment_length:
            return "", False
        
        # æ‰¾åˆ°åˆ†å‰²ç‚¹ï¼ˆåœ¨å‰©ä½™å†…å®¹ä¸­æŸ¥æ‰¾ï¼‰
        best_split_pos = -1
        
        # æŸ¥æ‰¾æœ€ä½³åˆ†å‰²ç‚¹
        for marker in self.segment_markers:
            # åœ¨å‰©ä½™å†…å®¹ä¸­æŸ¥æ‰¾åˆ†å‰²ç‚¹
            pos = remaining_content.find(marker)
            while pos != -1:
                # ç¡®ä¿åˆ†å‰²ç‚¹æ»¡è¶³æœ€å°é•¿åº¦è¦æ±‚
                if pos >= self.min_segment_length - 1:
                    if pos > best_split_pos:
                        best_split_pos = pos
                    break
                # ç»§ç»­æŸ¥æ‰¾ä¸‹ä¸€ä¸ªåˆ†å‰²ç‚¹
                pos = remaining_content.find(marker, pos + 1)
        
        if best_split_pos > 0:
            # æå–æ–‡æœ¬æ®µ
            segment_text = remaining_content[:best_split_pos + 1].strip()
            
            if segment_text:
                # æ›´æ–°å·²å¤„ç†ä½ç½®
                self.last_processed_pos += best_split_pos + 1
                self.segment_counter += 1
                
                self.logger.info(f"âœ‚ï¸ æå–æ–‡æœ¬æ®µ #{self.segment_counter}: '{segment_text[:50]}...', å·²å¤„ç†ä½ç½®: {self.last_processed_pos} / {len(self.content_buffer)}")
                
                return segment_text, True
        
        return "", False
    
    def get_final_segment(self) -> str:
        """è·å–æœ€ç»ˆå‰©ä½™çš„æ–‡æœ¬æ®µ"""
        if self.last_processed_pos < len(self.content_buffer):
            final_text = self.content_buffer[self.last_processed_pos:].strip()
            
            if final_text and len(final_text) > 5:
                self.segment_counter += 1
                self.last_processed_pos = len(self.content_buffer)  # æ ‡è®°ä¸ºå·²å…¨éƒ¨å¤„ç†
                
                self.logger.info(f"ğŸ æå–æœ€ç»ˆæ–‡æœ¬æ®µ #{self.segment_counter}: '{final_text[:50]}...', é•¿åº¦: {len(final_text)}")
                return final_text
        
        return ""
    
    def get_current_page(self):
        """è·å–å½“å‰é¡µç """
        return self.current_page
    
    def get_segment_counter(self):
        """è·å–å·²å¤„ç†çš„æ®µè½æ•°é‡"""
        return self.segment_counter

class SimpleTextSegmentProcessor:
    """ä¿®å¤é‡å¤å†…å®¹çš„ç®€åŒ–æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨"""
    
    def __init__(self, request_id: str, logger, min_segment_length: int = 40):
        self.request_id = request_id
        self.logger = logger
        self.min_segment_length = min_segment_length
        self.segment_markers = ["ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", ".", "!", "?", ";", "\n"]
        
        # çŠ¶æ€ç®¡ç†
        self.text_buffer = ""
        self.last_processed_pos = 0
        self.segment_counter = 0
        
        # é‡å¤å†…å®¹æ£€æµ‹
        self.processed_segments = set()  # å­˜å‚¨å·²å¤„ç†çš„æ–‡æœ¬æ®µ
        
        self.logger.info(f"ğŸ”§ åˆå§‹åŒ–ç®€åŒ–æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨ [è¯·æ±‚ID: {request_id}]")
    
    def add_text(self, text_chunk: str):
        """ç›´æ¥æ·»åŠ æ–‡æœ¬å—"""
        if not text_chunk:
            return
            
        old_length = len(self.text_buffer)
        self.text_buffer += text_chunk
        
        self.logger.debug(f"ğŸ“ æ·»åŠ æ–‡æœ¬å—: '{text_chunk[:50]}...', ç¼“å†²åŒºé•¿åº¦: {old_length} -> {len(self.text_buffer)}")
    
    def _is_duplicate_content(self, segment_text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤å†…å®¹"""
        # æ¸…ç†æ–‡æœ¬ç”¨äºæ¯”è¾ƒï¼ˆç§»é™¤ç©ºç™½å­—ç¬¦å’Œæ ‡ç‚¹ï¼‰
        cleaned_text = ''.join(c for c in segment_text if c.isalnum()).lower()
        
        # æ£€æŸ¥æ˜¯å¦ä¸å·²å¤„ç†çš„æ®µè½é‡å¤
        for processed_segment in self.processed_segments:
            processed_cleaned = ''.join(c for c in processed_segment if c.isalnum()).lower()
            
            # å¦‚æœæ–°æ®µè½å®Œå…¨åŒ…å«åœ¨ä¹‹å‰çš„æ®µè½ä¸­ï¼Œæˆ–è€…é‡å¤åº¦è¶…è¿‡80%
            if cleaned_text in processed_cleaned or processed_cleaned in cleaned_text:
                return True
            
            # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„å­—ç¬¦é‡å¤åº¦ï¼‰
            if len(cleaned_text) > 20 and len(processed_cleaned) > 20:
                common_chars = sum(1 for c in cleaned_text if c in processed_cleaned)
                similarity = common_chars / max(len(cleaned_text), len(processed_cleaned))
                if similarity > 0.8:  # 80%ä»¥ä¸Šé‡å¤è®¤ä¸ºæ˜¯é‡å¤å†…å®¹
                    return True
        
        return False
    
    def get_next_segment(self) -> tuple[str, bool]:
        """è·å–ä¸‹ä¸€ä¸ªå¯å¤„ç†çš„æ–‡æœ¬æ®µï¼Œè‡ªåŠ¨è·³è¿‡é‡å¤å†…å®¹"""
        max_attempts = 10  # é˜²æ­¢æ— é™å¾ªç¯
        attempts = 0
        
        while attempts < max_attempts:
            attempts += 1
            
            if len(self.text_buffer) <= self.last_processed_pos:
                self.logger.debug(f"ğŸ“ æ²¡æœ‰æ–°å†…å®¹å¯å¤„ç†: ç¼“å†²åŒºé•¿åº¦={len(self.text_buffer)}, å·²å¤„ç†ä½ç½®={self.last_processed_pos}")
                return "", False
            
            # æ£€æŸ¥å‰©ä½™æœªå¤„ç†çš„å†…å®¹é•¿åº¦
            remaining_content = self.text_buffer[self.last_processed_pos:]
            if len(remaining_content) < self.min_segment_length:
                self.logger.debug(f"ğŸ“ å‰©ä½™å†…å®¹å¤ªçŸ­: {len(remaining_content)} < {self.min_segment_length}")
                return "", False
            
            # æ‰¾åˆ°åˆ†å‰²ç‚¹
            best_split_pos = -1
            best_marker = ""
            
            for marker in self.segment_markers:
                pos = remaining_content.find(marker)
                while pos != -1:
                    if pos >= self.min_segment_length - 1:
                        if pos > best_split_pos:
                            best_split_pos = pos
                            best_marker = marker
                        break
                    pos = remaining_content.find(marker, pos + 1)
            
            if best_split_pos > 0:
                # æå–æ–‡æœ¬æ®µ
                segment_text = remaining_content[:best_split_pos + 1].strip()
                
                if segment_text:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤å†…å®¹
                    if self._is_duplicate_content(segment_text):
                        self.logger.warning(f"âš ï¸ è·³è¿‡é‡å¤å†…å®¹æ®µè½: '{segment_text[:50]}...'")
                        # è·³è¿‡è¿™ä¸ªé‡å¤æ®µè½ï¼Œç»§ç»­æŸ¥æ‰¾ä¸‹ä¸€ä¸ª
                        self.last_processed_pos += best_split_pos + 1
                        continue
                    
                    # éé‡å¤å†…å®¹ï¼Œæ­£å¸¸å¤„ç†
                    old_pos = self.last_processed_pos
                    self.last_processed_pos += best_split_pos + 1
                    self.segment_counter += 1
                    
                    # è®°å½•å·²å¤„ç†çš„æ®µè½
                    self.processed_segments.add(segment_text)
                    
                    self.logger.info(f"âœ‚ï¸ æå–æ–‡æœ¬æ®µ #{self.segment_counter}: '{segment_text[:50]}...', å·²å¤„ç†ä½ç½®: {old_pos} -> {self.last_processed_pos} / {len(self.text_buffer)}")
                    
                    return segment_text, True
            
            # æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹ï¼Œé€€å‡ºå¾ªç¯
            break
        
        self.logger.debug(f"ğŸ“ æœªæ‰¾åˆ°åˆé€‚çš„éé‡å¤åˆ†å‰²ç‚¹")
        return "", False
    
    def get_final_segment(self) -> str:
        """è·å–æœ€ç»ˆå‰©ä½™çš„æ–‡æœ¬æ®µï¼Œå¦‚æœæ˜¯é‡å¤å†…å®¹åˆ™è·³è¿‡"""
        if self.last_processed_pos < len(self.text_buffer):
            final_text = self.text_buffer[self.last_processed_pos:].strip()
            
            if final_text and len(final_text) > 5:
                # æ£€æŸ¥æœ€ç»ˆæ®µè½æ˜¯å¦ä¸ºé‡å¤å†…å®¹
                if self._is_duplicate_content(final_text):
                    self.logger.warning(f"âš ï¸ è·³è¿‡é‡å¤çš„æœ€ç»ˆæ®µè½: '{final_text[:50]}...'")
                    self.last_processed_pos = len(self.text_buffer)  # æ ‡è®°ä¸ºå·²å¤„ç†
                    return ""
                
                self.segment_counter += 1
                self.last_processed_pos = len(self.text_buffer)
                
                # è®°å½•å·²å¤„ç†çš„æ®µè½
                self.processed_segments.add(final_text)
                
                self.logger.info(f"ğŸ æå–æœ€ç»ˆæ–‡æœ¬æ®µ #{self.segment_counter}: '{final_text[:50]}...', é•¿åº¦: {len(final_text)}")
                return final_text
        
        return ""
    
    def get_segment_counter(self):
        """è·å–å·²å¤„ç†çš„æ®µè½æ•°é‡"""
        return self.segment_counter
    
@router.post("/api/chat/voice/stream")
async def voice_chat_stream(
    audio_file: UploadFile = File(...),
    llm_model: Optional[str] = None,
    system_prompt: Optional[str] = None,
    session_id: Optional[str] = None
):
    """ç®€åŒ–çš„æµå¼è¯­éŸ³å¯¹è¯æ¥å£ï¼ˆè¯­éŸ³è¾“å…¥ + æµå¼æ–‡æœ¬å’Œè¯­éŸ³è¾“å‡ºï¼‰"""
    managers = get_managers()
    logger = managers['logger']
    
    request_id = generate_response_id()
    logger.info(f"ğŸš€ å¼€å§‹ç®€åŒ–æµå¼è¯­éŸ³å¯¹è¯ [è¯·æ±‚ID: {request_id}] - ä¼šè¯ID: {session_id}")
    
    async def create_stream_generator():
        logger.info(f"ğŸ”§ åˆ›å»ºç®€åŒ–æµå¼ç”Ÿæˆå™¨ [è¯·æ±‚ID: {request_id}]")
        
        try:
            # å‘é€å¼€å§‹ä¿¡å·
            start_message = f"data: {json.dumps({'type': 'start', 'request_id': request_id, 'message': 'Stream started'})}\n\n"
            logger.info(f"ğŸ“¡ å‘é€æµå¼å¼€å§‹ä¿¡å· [è¯·æ±‚ID: {request_id}]")
            yield start_message

            # å¤„ç†ç³»ç»Ÿæç¤ºè¯ä¸­çš„documentsId
            processed_system_prompt = system_prompt
            if system_prompt:
                logger.info(f"ğŸ“‹ å¼€å§‹å¤„ç†ç³»ç»Ÿæç¤ºè¯ [è¯·æ±‚ID: {request_id}]")
                processed_system_prompt = await process_system_prompt_with_documents(
                    system_prompt, managers['mongodb_manager'], logger, request_id
                )

            logger.info(f"ğŸ“‹ å¤„ç†åçš„ç³»ç»Ÿæç¤ºè¯ [è¯·æ±‚ID: {request_id}]: {processed_system_prompt}")
            
            # 1. è¯­éŸ³è¯†åˆ«
            try:
                audio_data = await audio_file.read()
                logger.info(f"ğŸ¤ æ‰§è¡Œè¯­éŸ³è¯†åˆ« [è¯·æ±‚ID: {request_id}] - éŸ³é¢‘å¤§å°: {len(audio_data)} bytes")
                
                recognition_result = await managers['speech_processor'].recognize(
                    audio_data=audio_data,
                    request_id=request_id
                )
                
                user_text = recognition_result.text
                user_text = re.sub(r'<\s*\|\s*[^|]+\s*\|\s*>', '', user_text).strip()
                logger.info(f"âœ… è¯†åˆ«ç»“æœ [è¯·æ±‚ID: {request_id}]: {user_text}")
                
                # å‘é€è¯†åˆ«ç»“æœ
                recognition_message = f"data: {json.dumps({'type': 'recognition', 'request_id': request_id, 'text': user_text})}\n\n"
                yield recognition_message
                
            except Exception as e:
                logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥ [è¯·æ±‚ID: {request_id}]: {str(e)}")
                error_message = f"data: {json.dumps({'type': 'error', 'message': f'è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}'})}\n\n"
                yield error_message
                return
            
            # 2. LLM æµå¼å¯¹è¯ - ç®€åŒ–ç‰ˆæœ¬
            try:
                logger.info(f"ğŸ¤– å¼€å§‹ LLM æµå¼å¯¹è¯ [è¯·æ±‚ID: {request_id}], è¯·æ±‚å†…å®¹: {user_text}")
                
                # åˆå§‹åŒ–ç®€åŒ–çš„æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨
                text_processor = SimpleTextSegmentProcessor(request_id, logger)
                
                start_time = time.time()
                chunk_count = 0
                
                async for chunk in managers['llm_manager'].stream_chat(
                    model_name=llm_model,
                    message=user_text,
                    system_prompt=processed_system_prompt,
                    session_id=session_id,
                    request_id=request_id
                ):
                    try:
                        chunk_count += 1
                        
                        # è·å–æ–‡æœ¬å— - ç›´æ¥å¤„ç†çº¯æ–‡æœ¬
                        if isinstance(chunk, dict):
                            text_chunk = chunk.get("content", "")
                        else:
                            text_chunk = str(chunk)
                        
                        if not text_chunk:
                            logger.debug(f"â­ï¸ è·³è¿‡ç©ºæ–‡æœ¬å— [è¯·æ±‚ID: {request_id}] - å— {chunk_count}")
                            continue
                        
                        logger.debug(f"ğŸ“ å¤„ç†æ–‡æœ¬å— [{chunk_count}]: '{text_chunk[:30]}...'")
                        
                        # ç›´æ¥æ·»åŠ æ–‡æœ¬å—åˆ°å¤„ç†å™¨
                        text_processor.add_text(text_chunk)
                        
                        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                        # if chunk_count % 10 == 0:  # æ¯10ä¸ªå—è¾“å‡ºä¸€æ¬¡çŠ¶æ€
                        #     text_processor.debug_state()
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰å¯å¤„ç†çš„æ–‡æœ¬æ®µ
                        while True:
                            segment_text, has_more = text_processor.get_next_segment()
                            if not segment_text:
                                break
                            
                            logger.info(f"ğŸ“¤ å¤„ç†æ–‡æœ¬æ®µ #{text_processor.get_segment_counter()}: '{segment_text[:50]}...' (é•¿åº¦: {len(segment_text)})")
                            
                            # å‘é€æ–‡æœ¬æ®µ
                            segment_id = f"{request_id}_seg_{text_processor.get_segment_counter()}"
                            
                            text_data = {
                                "type": "text",
                                "segment_id": segment_id,
                                "text": segment_text
                            }
                            
                            text_message = f"data: {json.dumps(text_data)}\n\n"
                            logger.info(f"ğŸ“¤ å‘é€æ–‡æœ¬æ®µ [{segment_id}]: {len(segment_text)} å­—ç¬¦")
                            yield text_message
                            
                            # åˆæˆå¹¶å‘é€è¯­éŸ³
                            try:
                                logger.info(f"ğŸµ å¼€å§‹åˆæˆè¯­éŸ³ [{segment_id}]: '{segment_text[:50]}...'")
                                
                                synthesis_result = await managers['speech_processor'].synthesize(
                                    text=segment_text,
                                    request_id=segment_id
                                )
                                
                                # å¤„ç†éŸ³é¢‘æ•°æ®
                                audio_data = synthesis_result.audio_data
                                if isinstance(audio_data, str):
                                    try:
                                        base64.b64decode(audio_data)
                                        audio_base64 = audio_data
                                    except:
                                        audio_base64 = base64.b64encode(audio_data.encode('utf-8')).decode('utf-8')
                                else:
                                    audio_base64 = base64.b64encode(audio_data).decode('utf-8')
                                
                                # å‘é€éŸ³é¢‘åˆ†æ®µ
                                audio_response = {
                                    "type": "audio",
                                    "segment_id": segment_id,
                                    "text": segment_text,
                                    "audio": audio_base64,
                                    "format": synthesis_result.format
                                }
                                
                                audio_message = f"data: {json.dumps(audio_response)}\n\n"
                                logger.info(f"ğŸµâœ… éŸ³é¢‘åˆæˆå®Œæˆ [{segment_id}]: {len(audio_message)} å­—èŠ‚")
                                yield audio_message
                                
                            except Exception as e:
                                logger.error(f"âŒ éŸ³é¢‘åˆæˆå¤±è´¥ [{segment_id}]: {e}")
                                error_message = f"data: {json.dumps({'type': 'error', 'message': f'éŸ³é¢‘åˆæˆå¤±è´¥: {str(e)}'})}\n\n"
                                yield error_message
                            
                            await asyncio.sleep(0.1)
                    
                    except Exception as e:
                        logger.error(f"âŒ å¤„ç†æ–‡æœ¬å—å¤±è´¥: {e}")
                        continue
                
                logger.info(f"âœ… LLMæµå¼å¯¹è¯å®Œæˆ [è¯·æ±‚ID: {request_id}] - æ€»å…±å¤„ç† {chunk_count} ä¸ªæ–‡æœ¬å—")
                
                # å¤„ç†æœ€ç»ˆå‰©ä½™çš„æ–‡æœ¬
                final_text = text_processor.get_final_segment()
                if final_text:
                    logger.info(f"ğŸ å¤„ç†æœ€ç»ˆæ–‡æœ¬æ®µ: '{final_text[:50]}...' (é•¿åº¦: {len(final_text)})")
                    
                    # å‘é€æœ€ç»ˆæ–‡æœ¬æ®µ
                    final_segment_id = f"{request_id}_final"
                    
                    text_data = {
                        "type": "text",
                        "segment_id": final_segment_id,
                        "text": final_text
                    }
                    
                    final_text_message = f"data: {json.dumps(text_data)}\n\n"
                    logger.info(f"ğŸ“¤ å‘é€æœ€ç»ˆæ–‡æœ¬æ®µ [{final_segment_id}]: {len(final_text)} å­—ç¬¦")
                    yield final_text_message
                    
                    # åˆæˆå¹¶å‘é€æœ€ç»ˆè¯­éŸ³
                    try:
                        logger.info(f"ğŸµ å¼€å§‹åˆæˆæœ€ç»ˆè¯­éŸ³ [{final_segment_id}]: '{final_text[:50]}...'")
                        
                        synthesis_result = await managers['speech_processor'].synthesize(
                            text=final_text,
                            request_id=final_segment_id
                        )
                        
                        # å¤„ç†éŸ³é¢‘æ•°æ®
                        audio_data = synthesis_result.audio_data
                        if isinstance(audio_data, str):
                            try:
                                base64.b64decode(audio_data)
                                audio_base64 = audio_data
                            except:
                                audio_base64 = base64.b64encode(audio_data.encode('utf-8')).decode('utf-8')
                        else:
                            audio_base64 = base64.b64encode(audio_data).decode('utf-8')
                        
                        # å‘é€æœ€ç»ˆéŸ³é¢‘åˆ†æ®µ
                        audio_response = {
                            "type": "audio",
                            "segment_id": final_segment_id,
                            "text": final_text,
                            "audio": audio_base64,
                            "format": synthesis_result.format
                        }
                        
                        final_audio_message = f"data: {json.dumps(audio_response)}\n\n"
                        logger.info(f"ğŸµâœ… æœ€ç»ˆéŸ³é¢‘åˆæˆå®Œæˆ [{final_segment_id}]: {len(final_audio_message)} å­—èŠ‚")
                        yield final_audio_message
                        
                    except Exception as e:
                        logger.error(f"âŒ æœ€ç»ˆéŸ³é¢‘åˆæˆå¤±è´¥ [{final_segment_id}]: {e}")
                        error_message = f"data: {json.dumps({'type': 'error', 'message': f'æœ€ç»ˆéŸ³é¢‘åˆæˆå¤±è´¥: {str(e)}'})}\n\n"
                        yield error_message
                
                # å‘é€å®Œæˆä¿¡å·
                processing_time = time.time() - start_time
                done_data = {
                    'type': 'done', 
                    'request_id': request_id, 
                    'processing_time': processing_time,
                    'segments_processed': text_processor.get_segment_counter()
                }
                done_message = f"data: {json.dumps(done_data)}\n\n"
                logger.info(f"ğŸ‰ å¤„ç†å®Œæˆ: {text_processor.get_segment_counter()} ä¸ªæ–‡æœ¬æ®µï¼Œè€—æ—¶ {processing_time:.2f}s")
                yield done_message
                
            except Exception as e:
                logger.error(f"âŒ LLMæµå¼å¯¹è¯å¤±è´¥ [è¯·æ±‚ID: {request_id}]: {str(e)}")
                error_message = f"data: {json.dumps({'type': 'error', 'message': f'LLMå¯¹è¯å¤±è´¥: {str(e)}'})}\n\n"
                yield error_message
            
            logger.info(f"âœ… ç®€åŒ–æµå¼è¯­éŸ³å¯¹è¯å®Œæˆ [è¯·æ±‚ID: {request_id}]")
            
        except Exception as e:
            logger.error(f"âŒ æµå¼è¯­éŸ³å¯¹è¯å¤±è´¥ [è¯·æ±‚ID: {request_id}]: {str(e)}")
            error_message = f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
            yield error_message
        
        logger.info(f"ğŸ”š ç®€åŒ–æµå¼ç”Ÿæˆå™¨ç»“æŸ [è¯·æ±‚ID: {request_id}]")
    
    # è¿”å›SSEæµå¼å“åº”
    logger.info(f"ğŸš€ è¿”å›StreamingResponse [è¯·æ±‚ID: {request_id}]")
    
    response = StreamingResponse(
        create_stream_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "POST, OPTIONS",
            "Access-Control-Allow-Headers": "Content-Type, Authorization",
            "X-Accel-Buffering": "no",
            "Transfer-Encoding": "chunked",
        }
    )
    
    return response