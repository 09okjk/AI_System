"""
è¯­éŸ³å¤„ç†å™¨ - ä¼˜åŒ–ç‰ˆæœ¬
è´Ÿè´£è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³åˆæˆåŠŸèƒ½
åŸºäº SenseVoice å’Œ CosyVoice å®˜æ–¹å®ç°ä¼˜åŒ–
"""

import asyncio
import base64
import tempfile
import time
import uuid
import json
from typing import Dict, Any, Optional, List, Union
from datetime import datetime
from pathlib import Path
import json
import os
import numpy as np

from .logger import get_logger, log_speech_operation
from .models import SpeechRecognitionResponse, SpeechSynthesisResponse, AudioFormat
from .utils import generate_response_id

logger = get_logger(__name__)

# æ·»åŠ éŸ³é¢‘æ ¼å¼è½¬æ¢å‡½æ•°
def convert_audio_to_wav(input_path: str, output_path: Optional[str] = None, sample_rate: int = 16000) -> str:
    """
    å°†ä»»æ„éŸ³é¢‘æ ¼å¼è½¬æ¢ä¸ºWAVæ ¼å¼
    
    Args:
        input_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºWAVæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ç”Ÿæˆä¸€ä¸ªä¸´æ—¶æ–‡ä»¶
        sample_rate: è¾“å‡ºéŸ³é¢‘çš„é‡‡æ ·ç‡
        
    Returns:
        WAVæ–‡ä»¶çš„è·¯å¾„
    """
    try:
        import librosa
        import soundfile as sf
        
        # å¦‚æœæœªæŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œåˆ›å»ºä¸´æ—¶æ–‡ä»¶
        if not output_path:
            temp_dir = Path(tempfile.gettempdir()) / "ai_system_audio"
            temp_dir.mkdir(exist_ok=True)
            output_path = str(temp_dir / f"{uuid.uuid4()}.wav")
        
        # åŠ è½½éŸ³é¢‘æ–‡ä»¶
        logger.info(f"æ­£åœ¨è½¬æ¢éŸ³é¢‘: {input_path} -> {output_path}")
        y, sr = librosa.load(input_path, sr=sample_rate)
        
        # ä¿å­˜ä¸ºWAVæ ¼å¼
        sf.write(output_path, y, sample_rate, subtype='PCM_16')
        logger.info(f"éŸ³é¢‘è½¬æ¢å®Œæˆ: {output_path}")
        
        return output_path
    except Exception as e:
        logger.error(f"éŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
        raise

class SpeechRecognizer:
    """è¯­éŸ³è¯†åˆ«å™¨åŸºç±»"""    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_initialized = False
        self.model = None
        self.device = config.get('device', 'cpu')

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–è¯†åˆ«å™¨"""
        try:
            await self._setup()
            self.is_initialized = True
            logger.info(f"âœ… {self.__class__.__name__} åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False

    async def _setup(self):
        """è®¾ç½®è¯†åˆ«å™¨ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        pass

    async def recognize(self, 
                        audio_data: bytes, 
                        language: str = "zh-CN",
                        **kwargs) -> Dict[str, Any]:
        """è¯†åˆ«è¯­éŸ³ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        raise NotImplementedError

class SenseVoiceRecognizer(SpeechRecognizer):
    """SenseVoice è¯­éŸ³è¯†åˆ«å™¨ - åŸºäºå®˜æ–¹å®ç°ä¼˜åŒ–"""
    
    async def _setup(self):
        """è®¾ç½® SenseVoice - æ ¹æ®å®˜æ–¹æœ€ä½³å®è·µ"""
        try:
            from funasr import AutoModel
            
            # æ ¹æ®å®˜æ–¹æ¨èçš„é…ç½®
            model_config = {
                "model": "iic/SenseVoiceSmall",  # ä½¿ç”¨å®˜æ–¹æ¨èçš„æ¨¡å‹
                "vad_model": "fsmn-vad",
                "punc_model": "ct-punc", 
                "device": self.device,
                "hub": "ms",  # ä½¿ç”¨ modelscope
                "ncpu": self.config.get('ncpu', 4),
                "batch_size": self.config.get('batch_size', 1)
            }
            
            # æ”¯æŒ VAD çš„é…ç½®
            vad_kwargs = {
                "max_single_segment_time": self.config.get('max_single_segment_time', 60000),  # 60ç§’
                "batch_size_s": self.config.get('batch_size_s', 300),  # åŠ¨æ€batchï¼Œæ€»éŸ³é¢‘æ—¶é•¿300ç§’
                "batch_size_threshold_s": self.config.get('batch_size_threshold_s', 60)  # é˜ˆå€¼60ç§’
            }
            
            # åˆå§‹åŒ–æ¨¡å‹
            self.model = AutoModel(
                model=model_config["model"],
                vad_model=model_config["vad_model"],
                vad_kwargs=vad_kwargs,
                punc_model=model_config["punc_model"],
                device=model_config["device"],
                hub=model_config["hub"],
                ncpu=model_config["ncpu"],
                batch_size=model_config["batch_size"]
            )
            
            # å­˜å‚¨æ¨¡å‹é…ç½®ç”¨äºåç»­ä½¿ç”¨
            self.model_config = model_config
            self.vad_kwargs = vad_kwargs
            
            logger.info(f"âœ… SenseVoice æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ - è®¾å¤‡: {self.device}")
            
        except ImportError:
            logger.warning("âš ï¸ FunASR æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè¯†åˆ«å™¨")
            self.model = None
        except Exception as e:
            logger.error(f"âŒ SenseVoice åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def recognize(self, 
                       audio_data: bytes, 
                       language: str = "zh-CN",
                       **kwargs) -> Dict[str, Any]:
        """SenseVoice è¯­éŸ³è¯†åˆ« - ä¼˜åŒ–ç‰ˆæœ¬"""
        start_time = time.time()
        
        try:
            if self.model is None:
                # æ¨¡æ‹Ÿè¯†åˆ«ç»“æœ
                await asyncio.sleep(0.5)
                
                processing_time = time.time() - start_time
                
                return {
                    "text": "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„è¯­éŸ³è¯†åˆ«ç»“æœ",
                    "language": language,
                    "confidence": 0.95,
                    "processing_time": processing_time,
                    "model_used": "mock_sensevoice",
                    "segments": []
                }
            
            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_file.write(audio_data)
                temp_audio_path = temp_file.name
            
            try:
                # ä½¿ç”¨å®˜æ–¹æ¨èçš„å‚æ•°æ‰§è¡Œè¯†åˆ«
                generation_kwargs = {
                    "hotword": kwargs.get('hotword', ''),  # çƒ­è¯
                    "batch_size_s": self.vad_kwargs.get('batch_size_s', 300),
                    "batch_size_threshold_s": self.vad_kwargs.get('batch_size_threshold_s', 60)
                }
                
                # æ·»åŠ è‡ªå®šä¹‰å‚æ•°
                generation_kwargs.update(kwargs)
                
                # æ‰§è¡Œè¯†åˆ«
                result = self.model.generate(
                    input=temp_audio_path,
                    **generation_kwargs
                )
                
                processing_time = time.time() - start_time
                
                # å¤„ç†è¯†åˆ«ç»“æœ
                if result and len(result) > 0:
                    # SenseVoice è¿”å›æ ¼å¼é€šå¸¸æ˜¯åˆ—è¡¨
                    first_result = result[0] if isinstance(result, list) else result
                    
                    # æå–æ–‡æœ¬
                    text = first_result.get('text', '') if isinstance(first_result, dict) else str(first_result)

                    # æå–ç½®ä¿¡åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    confidence = first_result.get('confidence', 0.9) if isinstance(first_result, dict) else 0.9
                    
                    # æå–æ—¶é—´æˆ³ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    segments = first_result.get('timestamps', []) if isinstance(first_result, dict) else []
                    
                else:
                    text = ""
                    confidence = 0.0
                    segments = []
                
                log_speech_operation(
                    logger, "recognition", "sensevoice", 
                    len(audio_data), len(text), processing_time, 
                    True, language
                )
                
                return {
                    "text": text,
                    "language": language,
                    "confidence": confidence,
                    "processing_time": processing_time,
                    "model_used": "sensevoice",
                    "segments": segments,
                    "raw_result": result  # ä¿ç•™åŸå§‹ç»“æœä»¥ä¾¿è°ƒè¯•
                }
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                Path(temp_audio_path).unlink(missing_ok=True)
                
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_speech_operation(
                logger, "recognition", "sensevoice", 
                len(audio_data), 0, processing_time, 
                False, language, error_msg
            )
            
            raise Exception(f"SenseVoice è¯­éŸ³è¯†åˆ«å¤±è´¥: {error_msg}")

class SpeechSynthesizer:
    """è¯­éŸ³åˆæˆå™¨åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_initialized = False
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–åˆæˆå™¨"""
        try:
            await self._setup()
            self.is_initialized = True
            return True
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    async def _setup(self):
        """è®¾ç½®åˆæˆå™¨ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        pass
    
    async def synthesize(self, 
                        text: str, 
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        **kwargs) -> Dict[str, Any]:
        """åˆæˆè¯­éŸ³ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        raise NotImplementedError
class CosyVoiceSynthesizer(SpeechSynthesizer):
    """CosyVoice è¯­éŸ³åˆæˆå™¨ - åŸºäºå®˜æ–¹å®ç°ä¼˜åŒ–ï¼Œæ”¯æŒspeakerç¼“å­˜"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.model = None  # Initialize model attribute
        
        # å‚è€ƒéŸ³é¢‘è®¾ç½® - æ—©æœŸåˆå§‹åŒ–ä»¥æ”¯æŒæµ‹è¯•
        self.reference_audio_path = config.get('reference_audio', None)
        self.reference_text = config.get('reference_text', 'å‚è€ƒéŸ³é¢‘æ–‡æœ¬')
        
        # Speakerç¼“å­˜ç›¸å…³
        self.speaker_cache = {}  # speaker_id -> speaker_info
        self.speaker_cache_dir = config.get('speaker_cache_dir', '/tmp/ai_system_speakers')
        self.default_speaker_id = 'default_speaker'
        
        # ç¡®ä¿speakerç¼“å­˜ç›®å½•å­˜åœ¨
        Path(self.speaker_cache_dir).mkdir(parents=True, exist_ok=True)
    
    async def _setup(self):
        """è®¾ç½® CosyVoice - æ ¹æ®å®˜æ–¹æœ€ä½³å®è·µ"""
        try:
            # å‚è€ƒéŸ³é¢‘è®¾ç½® - æå‰åˆå§‹åŒ–
            self.reference_audio_path = self.config.get('reference_audio', None)
            self.reference_text = self.config.get('reference_text', 'å‚è€ƒéŸ³é¢‘æ–‡æœ¬')
            
            # è®¾ç½® CosyVoice è·¯å¾„ - ä¿®å¤è·¯å¾„é…ç½®
            import sys
            
            # æ·»åŠ  CosyVoice ä¸»ç›®å½•
            cosyvoice_path = self.config.get('cosyvoice_path', 'tools/CosyVoice')
            if cosyvoice_path not in sys.path:
                sys.path.append(cosyvoice_path)
                
            # æ·»åŠ  Matcha-TTS è·¯å¾„ï¼ˆæ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼‰
            matcha_path = os.path.join(cosyvoice_path, 'third_party/Matcha-TTS')
            if os.path.exists(matcha_path) and matcha_path not in sys.path:
                sys.path.append(matcha_path)
                
            logger.info(f"CosyVoice è·¯å¾„: {cosyvoice_path}")
            logger.info(f"Matcha-TTS è·¯å¾„: {matcha_path}")
            logger.info(f"å½“å‰ Python è·¯å¾„: {sys.path[-2:]}")  # æ˜¾ç¤ºæœ€åæ·»åŠ çš„è·¯å¾„
            
            # å¯¼å…¥ CosyVoice - æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            try:
                from cosyvoice.cli.cosyvoice import CosyVoice2
                from cosyvoice.utils.file_utils import load_wav
                import torchaudio
                logger.info("âœ… CosyVoice æ¨¡å—å¯¼å…¥æˆåŠŸ")
            except ImportError as import_err:
                logger.error(f"âŒ CosyVoice æ¨¡å—å¯¼å…¥å¤±è´¥: {import_err}")
                logger.error(f"æ£€æŸ¥è·¯å¾„: {[p for p in sys.path if 'CosyVoice' in p]}")
                raise
            
            # æ¨¡å‹é…ç½®
            model_dir = self.config.get('model_dir', '/home/rh/Program/MCP_Tools/CosyVoice/pretrained_models/CosyVoice2-0.5B')
            logger.info(f"CosyVoice æ¨¡å‹ç›®å½•: {model_dir}")
            logger.info(f"CosyVoice æ¨¡å‹ç›®å½•å­˜åœ¨: {Path(model_dir).exists()}")

            # æ£€æŸ¥æ¨¡å‹ç›®å½•
            if not Path(model_dir).exists():
                raise FileNotFoundError(f"CosyVoice æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            
            # åˆå§‹åŒ–æ¨¡å‹
            self.model = CosyVoice2(
                model_dir=model_dir,
                load_jit=self.config.get('load_jit', False),
                load_trt=self.config.get('load_trt', False),
                fp16=self.config.get('fp16', False)
            )
            
            # ä¿å­˜å·¥å…·å‡½æ•°
            self.load_wav = load_wav
            self.torchaudio = torchaudio
            
            # åˆå§‹åŒ–speakerç¼“å­˜
            await self._initialize_speaker_cache()
            
            logger.info(f"âœ… CosyVoice æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ - æ¨¡å‹è·¯å¾„: {model_dir}")
            logger.info(f"ğŸ“ Speakerç¼“å­˜ç›®å½•: {self.speaker_cache_dir}")
            
        except ImportError as e:
            logger.warning(f"âš ï¸ CosyVoice æœªæ­£ç¡®å®‰è£…: {str(e)}")
            self.model = None
        except Exception as e:
            logger.error(f"âŒ CosyVoice åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def _initialize_speaker_cache(self):
        """åˆå§‹åŒ–speakerç¼“å­˜ç³»ç»Ÿ"""
        try:
            # å¦‚æœæœ‰é»˜è®¤å‚è€ƒéŸ³é¢‘ï¼Œé¢„åŠ è½½ä¸ºé»˜è®¤speaker
            if self.reference_audio_path and Path(self.reference_audio_path).exists():
                logger.info("ğŸ¤ é¢„åŠ è½½é»˜è®¤speaker...")
                await self._add_speaker_to_cache(
                    speaker_id=self.default_speaker_id,
                    reference_audio_path=self.reference_audio_path,
                    reference_text=self.reference_text
                )
                logger.info(f"âœ… é»˜è®¤speakerå·²åŠ è½½: {self.default_speaker_id}")
            
            # åŠ è½½å·²ä¿å­˜çš„speakerç¼“å­˜
            await self._load_speaker_cache_from_disk()
            
        except Exception as e:
            logger.warning(f"âš ï¸ Speakerç¼“å­˜åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    async def _add_speaker_to_cache(self, speaker_id: str, reference_audio_path: str, reference_text: str):
        """æ·»åŠ speakeråˆ°ç¼“å­˜
        
        Args:
            speaker_id: speakeræ ‡è¯†ç¬¦
            reference_audio_path: å‚è€ƒéŸ³é¢‘è·¯å¾„
            reference_text: å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬
        """
        try:
            if self.model is None:
                logger.warning("æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè·³è¿‡speakerç¼“å­˜")
                return
            
            # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ ¼å¼å¹¶è½¬æ¢
            if Path(reference_audio_path).suffix.lower() != '.wav':
                logger.info(f"è½¬æ¢å‚è€ƒéŸ³é¢‘æ ¼å¼: {reference_audio_path}")
                reference_audio_path = convert_audio_to_wav(
                    reference_audio_path, 
                    sample_rate=self.model.sample_rate
                )
            
            # åŠ è½½éŸ³é¢‘
            reference_audio = self.load_wav(reference_audio_path, self.model.sample_rate)
            
            # æ ¹æ®CosyVoice2å®˜æ–¹æœ€ä½³å®è·µï¼Œä½¿ç”¨add_zero_shot_spkä¿å­˜speakerä¿¡æ¯
            # æ³¨æ„ï¼šè¿™æ˜¯åŸºäºå®˜æ–¹æ–‡æ¡£çš„å®ç°ï¼Œå¦‚æœå®é™…APIä¸åŒï¼Œéœ€è¦è°ƒæ•´
            if hasattr(self.model, 'add_zero_shot_spk'):
                speaker_info = self.model.add_zero_shot_spk(reference_audio, reference_text)
                logger.info(f"âœ… ä½¿ç”¨add_zero_shot_spkä¿å­˜speaker: {speaker_id}")
            else:
                # å¦‚æœæ¨¡å‹æ²¡æœ‰add_zero_shot_spkæ–¹æ³•ï¼Œæˆ‘ä»¬ä¿å­˜åŸå§‹æ•°æ®
                speaker_info = {
                    'reference_audio': reference_audio,
                    'reference_text': reference_text,
                    'audio_path': reference_audio_path,
                    'sample_rate': self.model.sample_rate
                }
                logger.info(f"âœ… ä¿å­˜speakeråŸå§‹ä¿¡æ¯: {speaker_id}")
            
            # ä¿å­˜åˆ°å†…å­˜ç¼“å­˜
            self.speaker_cache[speaker_id] = {
                'info': speaker_info,
                'reference_text': reference_text,
                'audio_path': reference_audio_path,
                'created_at': time.time()
            }
            
            # æŒä¹…åŒ–ä¿å­˜
            await self._save_speaker_to_disk(speaker_id)
            
            logger.info(f"ğŸ¤ Speakerå·²æ·»åŠ åˆ°ç¼“å­˜: {speaker_id}")
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ speakeråˆ°ç¼“å­˜å¤±è´¥: {str(e)}")
            raise
    
    async def _get_speaker_from_cache(self, speaker_id: str) -> Optional[Dict[str, Any]]:
        """ä»ç¼“å­˜è·å–speakerä¿¡æ¯"""
        return self.speaker_cache.get(speaker_id)
    
    async def _save_speaker_to_disk(self, speaker_id: str):
        """ä¿å­˜speakerä¿¡æ¯åˆ°ç£ç›˜"""
        try:
            if speaker_id not in self.speaker_cache:
                return
            
            cache_file = Path(self.speaker_cache_dir) / f"{speaker_id}.json"
            speaker_data = self.speaker_cache[speaker_id].copy()
            
            # ä¸ä¿å­˜éŸ³é¢‘tensorï¼Œåªä¿å­˜å…ƒæ•°æ®
            speaker_data['info'] = {
                'audio_path': speaker_data['audio_path'],
                'reference_text': speaker_data['reference_text'],
                'sample_rate': getattr(self.model, 'sample_rate', 22050),
                'created_at': speaker_data['created_at']
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(speaker_data, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ğŸ’¾ Speakerä¿¡æ¯å·²ä¿å­˜åˆ°ç£ç›˜: {cache_file}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ä¿å­˜speakeråˆ°ç£ç›˜å¤±è´¥: {str(e)}")
    
    async def _load_speaker_cache_from_disk(self):
        """ä»ç£ç›˜åŠ è½½speakerç¼“å­˜"""
        try:
            cache_dir = Path(self.speaker_cache_dir)
            if not cache_dir.exists():
                return
            
            for cache_file in cache_dir.glob("*.json"):
                try:
                    speaker_id = cache_file.stem
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        speaker_data = json.load(f)
                    
                    # éªŒè¯éŸ³é¢‘æ–‡ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨
                    audio_path = speaker_data['info']['audio_path']
                    if Path(audio_path).exists():
                        # é‡æ–°åŠ è½½speakeråˆ°ç¼“å­˜
                        await self._add_speaker_to_cache(
                            speaker_id=speaker_id,
                            reference_audio_path=audio_path,
                            reference_text=speaker_data['reference_text']
                        )
                        logger.debug(f"ğŸ“‚ ä»ç£ç›˜åŠ è½½speaker: {speaker_id}")
                    else:
                        logger.warning(f"âš ï¸ SpeakeréŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {audio_path}")
                        # åˆ é™¤æ— æ•ˆçš„ç¼“å­˜æ–‡ä»¶
                        cache_file.unlink(missing_ok=True)
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ åŠ è½½speakerç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_file}: {str(e)}")
                    
        except Exception as e:
            logger.warning(f"âš ï¸ ä»ç£ç›˜åŠ è½½speakerç¼“å­˜å¤±è´¥: {str(e)}")
    
    def list_cached_speakers(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰ç¼“å­˜çš„speaker"""
        return list(self.speaker_cache.keys())
    
    async def synthesize(self, 
                        text: str, 
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        synthesis_mode: str = "instruct",
                        speaker_id: Optional[str] = None,
                        **kwargs) -> Dict[str, Any]:
        """CosyVoice è¯­éŸ³åˆæˆ - æ”¯æŒå¤šç§åˆæˆæ¨¡å¼å’Œspeakerç¼“å­˜"""
        start_time = time.time()
        
        try:
            if self.model is None:
                # æ¨¡æ‹Ÿåˆæˆç»“æœ
                await asyncio.sleep(1.5)
                
                processing_time = time.time() - start_time
                
                # åˆ›å»ºæ¨¡æ‹ŸéŸ³é¢‘æ•°æ®ï¼ˆé™éŸ³ï¼‰
                sample_rate = 22050
                duration = max(len(text) / 10, 1.0)  # åŸºäºæ–‡æœ¬é•¿åº¦ä¼°ç®—æ—¶é•¿
                audio_samples = int(sample_rate * duration)
                mock_audio = b'\x00' * (audio_samples * 2)  # 16ä½PCM
                
                audio_base64 = base64.b64encode(mock_audio).decode('utf-8')
                
                return {
                    "audio_data": audio_base64,
                    "format": AudioFormat.WAV,
                    "duration": duration,
                    "processing_time": processing_time,
                    "model_used": "mock_cosyvoice",
                    "synthesis_mode": synthesis_mode,
                    "speaker_id": speaker_id or "mock_speaker"
                }
            
            # è·å–æˆ–åˆ›å»ºspeaker
            speaker_info = await self._get_or_create_speaker(kwargs, speaker_id)
            if speaker_info:
                kwargs['speaker_info'] = speaker_info
                kwargs['speaker_id'] = speaker_info.get('speaker_id', speaker_id)
            
            # æ ¹æ®åˆæˆæ¨¡å¼é€‰æ‹©ä¸åŒçš„æ–¹æ³•
            if synthesis_mode == "zero_shot":
                result = await self._zero_shot_synthesis(text, kwargs)
            elif synthesis_mode == "cross_lingual":
                result = await self._cross_lingual_synthesis(text, kwargs)
            elif synthesis_mode == "instruct":
                result = await self._instruct_synthesis(text, kwargs)
            else:
                # é»˜è®¤ä½¿ç”¨é›¶æ ·æœ¬åˆæˆ
                result = await self._zero_shot_synthesis(text, kwargs)
            
            processing_time = time.time() - start_time
            result["processing_time"] = processing_time
            result["synthesis_mode"] = synthesis_mode
            result["speaker_id"] = kwargs.get('speaker_id', speaker_id or 'default')
            
            log_speech_operation(
                logger, "synthesis", "cosyvoice", 
                len(text), len(result["audio_data"]), processing_time, 
                True, language
            )
            
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_speech_operation(
                logger, "synthesis", "cosyvoice", 
                len(text), 0, processing_time, 
                False, language, error_msg
            )
            
            raise Exception(f"CosyVoice åˆæˆå¤±è´¥: {error_msg}")
    
    async def _get_or_create_speaker(self, kwargs: Dict[str, Any], speaker_id: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """è·å–æˆ–åˆ›å»ºspeakerä¿¡æ¯"""
        try:
            # å¦‚æœæŒ‡å®šäº†speaker_idï¼Œä¼˜å…ˆä½¿ç”¨ç¼“å­˜çš„speaker
            if speaker_id:
                cached_speaker = await self._get_speaker_from_cache(speaker_id)
                if cached_speaker:
                    logger.info(f"ğŸ¤ ä½¿ç”¨ç¼“å­˜çš„speaker: {speaker_id}")
                    return cached_speaker
                else:
                    logger.warning(f"âš ï¸ æŒ‡å®šçš„speakerä¸å­˜åœ¨äºç¼“å­˜ä¸­: {speaker_id}")
            
            # æ£€æŸ¥æ˜¯å¦æä¾›äº†æ–°çš„å‚è€ƒéŸ³é¢‘
            reference_audio_path = kwargs.get('reference_audio')
            reference_text = kwargs.get('reference_text')
            
            if reference_audio_path and reference_text:
                # ç”Ÿæˆspeaker_idï¼ˆå¦‚æœæœªæä¾›ï¼‰
                if not speaker_id:
                    import hashlib
                    # åŸºäºéŸ³é¢‘è·¯å¾„å’Œæ–‡æœ¬ç”Ÿæˆå”¯ä¸€ID
                    content = f"{reference_audio_path}_{reference_text}"
                    speaker_id = hashlib.md5(content.encode()).hexdigest()[:12]
                
                # æ£€æŸ¥è¯¥speakeræ˜¯å¦å·²ç»ç¼“å­˜
                cached_speaker = await self._get_speaker_from_cache(speaker_id)
                if cached_speaker:
                    logger.info(f"ğŸ¤ ä½¿ç”¨å·²ç¼“å­˜çš„speaker: {speaker_id}")
                    return cached_speaker
                
                # åˆ›å»ºæ–°çš„speaker
                logger.info(f"ğŸ¤ åˆ›å»ºæ–°çš„speaker: {speaker_id}")
                await self._add_speaker_to_cache(speaker_id, reference_audio_path, reference_text)
                return await self._get_speaker_from_cache(speaker_id)
            
            # ä½¿ç”¨é»˜è®¤speaker
            if self.default_speaker_id in self.speaker_cache:
                logger.info(f"ğŸ¤ ä½¿ç”¨é»˜è®¤speaker: {self.default_speaker_id}")
                return self.speaker_cache[self.default_speaker_id]
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•speakerï¼Œè¿”å›Noneï¼ˆä½¿ç”¨åŸå§‹æ–¹å¼ï¼‰
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„speakerï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼å¤„ç†å‚è€ƒéŸ³é¢‘")
            return None
            
        except Exception as e:
            logger.error(f"âŒ è·å–æˆ–åˆ›å»ºspeakerå¤±è´¥: {str(e)}")
            return None
    
    async def _zero_shot_synthesis(self, text: str, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """é›¶æ ·æœ¬è¯­éŸ³åˆæˆ - æ”¯æŒspeakerç¼“å­˜"""
        # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜çš„speakerä¿¡æ¯
        speaker_info = kwargs.get('speaker_info')
        
        if speaker_info and 'info' in speaker_info:
            # ä½¿ç”¨ç¼“å­˜çš„speaker
            logger.info(f"ğŸ¤ ä½¿ç”¨ç¼“å­˜speakerè¿›è¡Œé›¶æ ·æœ¬åˆæˆ")
            cached_info = speaker_info['info']
            
            if hasattr(self.model, 'inference_zero_shot_with_spk') and 'reference_audio' in cached_info:
                # å¦‚æœæ¨¡å‹æ”¯æŒé¢„å¤„ç†çš„speakerä¿¡æ¯
                output_audio = None
                stream = kwargs.get('stream', False)
                
                for i, result in enumerate(self.model.inference_zero_shot_with_spk(
                    text, cached_info, stream=stream
                )):
                    output_audio = result['tts_speech']
                    if not stream:
                        break
                        
                if output_audio is None:
                    raise Exception("é›¶æ ·æœ¬åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
                    
                return await self._process_output_audio(output_audio)
            
            else:
                # ä½¿ç”¨åŸå§‹éŸ³é¢‘ä¿¡æ¯
                reference_audio = cached_info.get('reference_audio')
                reference_text = speaker_info['reference_text']
                
                if reference_audio is not None:
                    output_audio = None
                    stream = kwargs.get('stream', False)
                    
                    for i, result in enumerate(self.model.inference_zero_shot(
                        text, reference_text, reference_audio, stream=stream
                    )):
                        output_audio = result['tts_speech']
                        if not stream:
                            break
                    
                    if output_audio is None:
                        raise Exception("é›¶æ ·æœ¬åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
                    
                    return await self._process_output_audio(output_audio)
        
        # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
        logger.info("ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿé›¶æ ·æœ¬åˆæˆæ–¹å¼")
        return await self._traditional_zero_shot_synthesis(text, kwargs)
    
    async def _traditional_zero_shot_synthesis(self, text: str, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼ ç»Ÿé›¶æ ·æœ¬è¯­éŸ³åˆæˆï¼ˆåŸå§‹å®ç°ï¼‰"""
        # è·å–å‚è€ƒéŸ³é¢‘
        reference_audio_path = kwargs.get('reference_audio', self.reference_audio_path)
        reference_text = kwargs.get('reference_text', self.reference_text)
        
        if not reference_audio_path or not Path(reference_audio_path).exists():
            raise FileNotFoundError(f"å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {reference_audio_path}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢éŸ³é¢‘æ ¼å¼
        reference_audio_ext = Path(reference_audio_path).suffix.lower()
        if reference_audio_ext != '.wav':
            logger.info(f"å‚è€ƒéŸ³é¢‘éWAVæ ¼å¼ ({reference_audio_ext})ï¼Œè¿›è¡Œæ ¼å¼è½¬æ¢")
            try:
                reference_audio_path = convert_audio_to_wav(reference_audio_path, sample_rate=self.model.sample_rate)
            except Exception as e:
                logger.error(f"å‚è€ƒéŸ³é¢‘è½¬æ¢å¤±è´¥: {str(e)}")
                raise ValueError(f"å‚è€ƒéŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
        
        # åŠ è½½å‚è€ƒéŸ³é¢‘
        reference_audio = self.load_wav(reference_audio_path, self.model.sample_rate)
        
        # æ‰§è¡Œåˆæˆ
        output_audio = None
        stream = kwargs.get('stream', False)
        
        for i, result in enumerate(self.model.inference_zero_shot(
            text, reference_text, reference_audio, stream=stream
        )):
            output_audio = result['tts_speech']
            if not stream:  # éæµå¼æ¨¡å¼åªå–ç¬¬ä¸€ä¸ªç»“æœ
                break
        
        if output_audio is None:
            raise Exception("é›¶æ ·æœ¬åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
        
        return await self._process_output_audio(output_audio)
    
    async def _cross_lingual_synthesis(self, text: str, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """è·¨è¯­è¨€è¯­éŸ³åˆæˆ - æ”¯æŒspeakerç¼“å­˜"""
        # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜çš„speakerä¿¡æ¯
        speaker_info = kwargs.get('speaker_info')
        
        if speaker_info and 'info' in speaker_info:
            logger.info(f"ğŸ¤ ä½¿ç”¨ç¼“å­˜speakerè¿›è¡Œè·¨è¯­è¨€åˆæˆ")
            cached_info = speaker_info['info']
            
            # ä½¿ç”¨åŸå§‹éŸ³é¢‘ä¿¡æ¯
            reference_audio = cached_info.get('reference_audio')
            
            if reference_audio is not None:
                output_audio = None
                stream = kwargs.get('stream', False)
                
                for i, result in enumerate(self.model.inference_cross_lingual(
                    text, reference_audio, stream=stream
                )):
                    output_audio = result['tts_speech']
                    if not stream:
                        break
                
                if output_audio is None:
                    raise Exception("è·¨è¯­è¨€åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
                
                return await self._process_output_audio(output_audio)
        
        # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
        logger.info("ğŸ”„ å›é€€åˆ°ä¼ ç»Ÿè·¨è¯­è¨€åˆæˆæ–¹å¼")
        return await self._traditional_cross_lingual_synthesis(text, kwargs)
    
    async def _traditional_cross_lingual_synthesis(self, text: str, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼ ç»Ÿè·¨è¯­è¨€è¯­éŸ³åˆæˆï¼ˆåŸå§‹å®ç°ï¼‰"""
        reference_audio_path = kwargs.get('reference_audio', self.reference_audio_path)
        
        if not reference_audio_path or not Path(reference_audio_path).exists():
            raise FileNotFoundError(f"å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {reference_audio_path}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢éŸ³é¢‘æ ¼å¼
        reference_audio_ext = Path(reference_audio_path).suffix.lower()
        if reference_audio_ext != '.wav':
            logger.info(f"å‚è€ƒéŸ³é¢‘éWAVæ ¼å¼ ({reference_audio_ext})ï¼Œè¿›è¡Œæ ¼å¼è½¬æ¢")
            try:
                reference_audio_path = convert_audio_to_wav(reference_audio_path, sample_rate=self.model.sample_rate)
            except Exception as e:
                logger.error(f"å‚è€ƒéŸ³é¢‘è½¬æ¢å¤±è´¥: {str(e)}")
                raise ValueError(f"å‚è€ƒéŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
        
        # åŠ è½½å‚è€ƒéŸ³é¢‘
        reference_audio = self.load_wav(reference_audio_path, self.model.sample_rate)
        
        # æ‰§è¡Œè·¨è¯­è¨€åˆæˆ
        output_audio = None
        stream = kwargs.get('stream', False)
        
        for i, result in enumerate(self.model.inference_cross_lingual(
            text, reference_audio, stream=stream
        )):
            output_audio = result['tts_speech']
            if not stream:
                break
        
        if output_audio is None:
            raise Exception("è·¨è¯­è¨€åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
        
        return await self._process_output_audio(output_audio)
    
    async def _instruct_synthesis(self, text: str, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """æŒ‡ä»¤å¼è¯­éŸ³åˆæˆ - æ”¯æŒspeakerç¼“å­˜"""
        # ä¼˜å…ˆä½¿ç”¨ç¼“å­˜çš„speakerä¿¡æ¯
        speaker_info = kwargs.get('speaker_info')
        instruction = kwargs.get('instruction', 'ç”¨æ¸©å’Œçš„ä¸­æ–‡å¥³å£°æœ—è¯»')
        
        if speaker_info and 'info' in speaker_info:
            logger.info(f"ğŸ¤ ä½¿ç”¨ç¼“å­˜speakerè¿›è¡ŒæŒ‡ä»¤å¼åˆæˆ")
            cached_info = speaker_info['info']
            
            # ä½¿ç”¨åŸå§‹éŸ³é¢‘ä¿¡æ¯
            reference_audio = cached_info.get('reference_audio')
            
            if reference_audio is not None:
                output_audio = None
                stream = kwargs.get('stream', False)
                
                # æ‰§è¡ŒæŒ‡ä»¤å¼åˆæˆ - ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å inference_instruct2
                for i, result in enumerate(self.model.inference_instruct2(
                    text, instruction, reference_audio, stream=stream
                )):
                    output_audio = result['tts_speech']
                    if not stream:
                        break
                
                if output_audio is None:
                    raise Exception("æŒ‡ä»¤å¼åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
                
                return await self._process_output_audio(output_audio)
        
        # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
        logger.info("ğŸ”„ å›é€€åˆ°ä¼ ç»ŸæŒ‡ä»¤å¼åˆæˆæ–¹å¼")
        return await self._traditional_instruct_synthesis(text, kwargs)
    
    async def _traditional_instruct_synthesis(self, text: str, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼ ç»ŸæŒ‡ä»¤å¼è¯­éŸ³åˆæˆï¼ˆåŸå§‹å®ç°ï¼‰"""
        reference_audio_path = kwargs.get('reference_audio', self.reference_audio_path)
        instruction = kwargs.get('instruction', 'ç”¨æ¸©å’Œçš„ä¸­æ–‡å¥³å£°æœ—è¯»')
        
        if not reference_audio_path or not Path(reference_audio_path).exists():
            raise FileNotFoundError(f"å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {reference_audio_path}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢éŸ³é¢‘æ ¼å¼
        reference_audio_ext = Path(reference_audio_path).suffix.lower()
        if reference_audio_ext != '.wav':
            logger.info(f"å‚è€ƒéŸ³é¢‘éWAVæ ¼å¼ ({reference_audio_ext})ï¼Œè¿›è¡Œæ ¼å¼è½¬æ¢")
            try:
                reference_audio_path = convert_audio_to_wav(reference_audio_path, sample_rate=self.model.sample_rate)
            except Exception as e:
                logger.error(f"å‚è€ƒéŸ³é¢‘è½¬æ¢å¤±è´¥: {str(e)}")
                raise ValueError(f"å‚è€ƒéŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
        
        # åŠ è½½å‚è€ƒéŸ³é¢‘
        reference_audio = self.load_wav(reference_audio_path, self.model.sample_rate)
        
        # æ‰§è¡ŒæŒ‡ä»¤å¼åˆæˆ - ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å inference_instruct2
        output_audio = None
        stream = kwargs.get('stream', False)
        
        for i, result in enumerate(self.model.inference_instruct2(
            text, instruction, reference_audio, stream=stream
        )):
            output_audio = result['tts_speech']
            if not stream:
                break
        
        if output_audio is None:
            raise Exception("æŒ‡ä»¤å¼åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
        
        return await self._process_output_audio(output_audio)
    
    async def _process_output_audio(self, output_audio) -> Dict[str, Any]:
        """å¤„ç†è¾“å‡ºéŸ³é¢‘"""
        import io
        
        # è½¬æ¢ä¸ºå­—èŠ‚æ•°æ®
        buffer = io.BytesIO()
        
        try:
            # æ˜ç¡®æŒ‡å®šéŸ³é¢‘æ ¼å¼å’Œä½æ·±åº¦
            self.torchaudio.save(
                buffer, 
                output_audio, 
                self.model.sample_rate, 
                format='wav',
                bits_per_sample=16,
                encoding='PCM_S'
            )
            buffer.seek(0)
            audio_data = buffer.read()
            
            # ç¼–ç ä¸ºbase64
            audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            
            # è®¡ç®—æ—¶é•¿
            duration = output_audio.shape[1] / self.model.sample_rate
            
            return {
                "audio_data": audio_base64,
                "format": AudioFormat.WAV,
                "duration": duration,
                "model_used": "cosyvoice",
                "sample_rate": self.model.sample_rate
            }
        except Exception as e:
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            logger.error(f"âŒ éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")
            raise Exception(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")
    
    # Public speaker management methods
    async def add_speaker(self, speaker_id: str, reference_audio_path: str, reference_text: str) -> bool:
        """æ·»åŠ æ–°çš„speakeråˆ°ç¼“å­˜
        
        Args:
            speaker_id: speakeræ ‡è¯†ç¬¦
            reference_audio_path: å‚è€ƒéŸ³é¢‘è·¯å¾„
            reference_text: å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬
            
        Returns:
            bool: æ·»åŠ æ˜¯å¦æˆåŠŸ
        """
        try:
            await self._add_speaker_to_cache(speaker_id, reference_audio_path, reference_text)
            logger.info(f"âœ… Speakerå·²æ·»åŠ : {speaker_id}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ speakerå¤±è´¥: {str(e)}")
            return False
    
    async def remove_speaker(self, speaker_id: str) -> bool:
        """ä»ç¼“å­˜ä¸­ç§»é™¤speaker
        
        Args:
            speaker_id: speakeræ ‡è¯†ç¬¦
            
        Returns:
            bool: ç§»é™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if speaker_id in self.speaker_cache:
                del self.speaker_cache[speaker_id]
                
                # åˆ é™¤ç£ç›˜æ–‡ä»¶
                cache_file = Path(self.speaker_cache_dir) / f"{speaker_id}.json"
                cache_file.unlink(missing_ok=True)
                
                logger.info(f"ğŸ—‘ï¸ Speakerå·²ç§»é™¤: {speaker_id}")
                return True
            else:
                logger.warning(f"âš ï¸ Speakerä¸å­˜åœ¨: {speaker_id}")
                return False
        except Exception as e:
            logger.error(f"âŒ ç§»é™¤speakerå¤±è´¥: {str(e)}")
            return False
    
    def get_speaker_info(self, speaker_id: str) -> Optional[Dict[str, Any]]:
        """è·å–speakerä¿¡æ¯
        
        Args:
            speaker_id: speakeræ ‡è¯†ç¬¦
            
        Returns:
            Dict: speakerä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        speaker_data = self.speaker_cache.get(speaker_id)
        if speaker_data:
            return {
                'speaker_id': speaker_id,
                'reference_text': speaker_data['reference_text'],
                'audio_path': speaker_data['audio_path'],
                'created_at': speaker_data['created_at']
            }
        return None
    
    async def validate_speaker_consistency(self, speaker_id: str, test_texts: List[str]) -> Dict[str, Any]:
        """éªŒè¯speakerçš„éŸ³è‰²ä¸€è‡´æ€§
        
        Args:
            speaker_id: speakeræ ‡è¯†ç¬¦
            test_texts: æµ‹è¯•æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            Dict: éªŒè¯ç»“æœ
        """
        if speaker_id not in self.speaker_cache:
            return {"error": f"Speakerä¸å­˜åœ¨: {speaker_id}"}
        
        try:
            results = []
            for i, text in enumerate(test_texts):
                result = await self.synthesize(
                    text=text,
                    speaker_id=speaker_id,
                    synthesis_mode="zero_shot"
                )
                results.append({
                    "text": text,
                    "audio_data": result["audio_data"],
                    "duration": result["duration"],
                    "processing_time": result["processing_time"]
                })
            
            return {
                "speaker_id": speaker_id,
                "test_count": len(test_texts),
                "results": results,
                "success": True
            }
            
        except Exception as e:
            return {
                "speaker_id": speaker_id,
                "error": str(e),
                "success": False
            }

class MockSynthesizer(SpeechSynthesizer):
    """æ¨¡æ‹Ÿè¯­éŸ³åˆæˆå™¨"""
    
    async def _setup(self):
        """è®¾ç½®æ¨¡æ‹Ÿåˆæˆå™¨"""
        logger.info("âœ… æ¨¡æ‹Ÿåˆæˆå™¨è®¾ç½®æˆåŠŸ")
    
    async def synthesize(self, 
                       text: str, 
                       voice: Optional[str] = None,
                       language: str = "zh-CN",
                       speed: float = 1.0,
                       pitch: float = 8.0,
                       **kwargs) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿè¯­éŸ³åˆæˆ"""
        logger.info(f"ğŸ”Š å¼€å§‹æ¨¡æ‹Ÿè¯­éŸ³åˆæˆ - æ–‡æœ¬é•¿åº¦: {len(text)}")
        start_time = time.time()
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„é™éŸ³éŸ³é¢‘
        await asyncio.sleep(0.2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        # ç”ŸæˆéŸ³é¢‘æ—¶é•¿
        sample_rate = 16000
        duration = min(len(text) / 5, 10)  # æœ€é•¿10ç§’
        duration = max(duration, 1.0)  # æœ€çŸ­1ç§’
        
        # åˆ›å»ºé™éŸ³éŸ³é¢‘
        samples = int(duration * sample_rate)
        audio_data = bytes(samples * 2)  # 16-bit PCM
        
        processing_time = time.time() - start_time
        
        return {
            "audio_data": base64.b64encode(audio_data).decode('utf-8'),
            "format": AudioFormat.PCM_16,
            "duration": duration,
            "processing_time": processing_time,
            "model_used": "mock_synthesizer"
        }

class SpeechProcessor:
    """è¯­éŸ³å¤„ç†å™¨ä¸»ç±» - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self):
        self.recognizers: Dict[str, SpeechRecognizer] = {}
        self.synthesizers: Dict[str, SpeechSynthesizer] = {}
        self.default_recognizer = None
        self.default_synthesizer = None
        self.is_initialized = False
        
        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è¯»å–é…ç½®
        self.config = self._load_config()

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        return {
            # CosyVoice é…ç½®
            'cosyvoice': {
                'model_dir': os.getenv('COSYVOICE_MODEL_DIR', '/home/rh/Program/MCP_Tools/CosyVoice/pretrained_models/CosyVoice2-0.5B'),
                'cosyvoice_path': os.getenv('COSYVOICE_PATH', 'tools/CosyVoice'),
                'reference_audio': os.getenv('COSYVOICE_REF_AUDIO', None),
                'reference_text': os.getenv('COSYVOICE_REF_TEXT', 'å‚è€ƒéŸ³é¢‘æ–‡æœ¬'),
                'speaker_cache_dir': os.getenv('COSYVOICE_SPEAKER_CACHE_DIR', '/tmp/ai_system_speakers'),
                'load_jit': os.getenv('COSYVOICE_LOAD_JIT', 'false').lower() == 'true',
                'load_trt': os.getenv('COSYVOICE_LOAD_TRT', 'false').lower() == 'true',
                'fp16': os.getenv('COSYVOICE_FP16', 'false').lower() == 'true'
            },
            # SenseVoice é…ç½®
            'sensevoice': {
                'model': os.getenv('SENSEVOICE_MODEL', 'iic/SenseVoiceSmall'),
                'max_single_segment_time': int(os.getenv('SENSEVOICE_MAX_SEGMENT_TIME', '60000')),
                'batch_size_s': int(os.getenv('SENSEVOICE_BATCH_SIZE_S', '300')),
                'batch_size_threshold_s': int(os.getenv('SENSEVOICE_BATCH_THRESHOLD_S', '60')),
                'ncpu': int(os.getenv('SENSEVOICE_NCPU', '4')),
                'batch_size': int(os.getenv('SENSEVOICE_BATCH_SIZE', '1'))
            },
            # é€šç”¨é…ç½®
            'device': os.getenv('SPEECH_DEVICE', 'cpu')
        }

    async def initialize(self):
        """åˆå§‹åŒ–è¯­éŸ³å¤„ç†å™¨"""
        logger.info("ğŸ”§ åˆå§‹åŒ–è¯­éŸ³å¤„ç†å™¨ - ä¼˜åŒ–ç‰ˆæœ¬")
        
        # å°è¯•åˆå§‹åŒ–å¯ç”¨çš„è¯†åˆ«å™¨
        await self._try_initialize_recognizers()
        
        # å°è¯•åˆå§‹åŒ–å¯ç”¨çš„åˆæˆå™¨
        await self._try_initialize_synthesizers()
        
        self.is_initialized = True
        
        available_recognizers = list(self.recognizers.keys())
        available_synthesizers = list(self.synthesizers.keys())
        
        logger.info(f"âœ… è¯­éŸ³å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - å¯ç”¨è¯†åˆ«å™¨: {available_recognizers}")
        logger.info(f"  - é»˜è®¤è¯†åˆ«å™¨: {self.default_recognizer}")
        logger.info(f"  - å¯ç”¨åˆæˆå™¨: {available_synthesizers}")
        logger.info(f"  - é»˜è®¤åˆæˆå™¨: {self.default_synthesizer}")
        
        if not available_recognizers and not available_synthesizers:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„è¯­éŸ³å¤„ç†å¼•æ“ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

    async def _try_initialize_recognizers(self):
        """å°è¯•åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨"""
        
        # ä¼˜å…ˆå°è¯• SenseVoice
        try:
            config = self.config['sensevoice'].copy()
            config['device'] = self.config['device']
            
            recognizer = SenseVoiceRecognizer(config)
            if await recognizer.initialize():
                self.recognizers['sensevoice'] = recognizer
                if self.default_recognizer is None:
                    self.default_recognizer = 'sensevoice'
                logger.info("âœ… SenseVoice è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ SenseVoice è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
        # å¦‚æœæ²¡æœ‰å¯ç”¨çš„è¯†åˆ«å™¨ï¼Œæ·»åŠ æ¨¡æ‹Ÿè¯†åˆ«å™¨
        if not self.recognizers:
            recognizer = SenseVoiceRecognizer({'device': 'cpu'})  # æ¨¡æ‹Ÿæ¨¡å¼
            await recognizer.initialize()
            self.recognizers['mock'] = recognizer
            self.default_recognizer = 'mock'
            logger.info("âœ… æ¨¡æ‹Ÿè¯†åˆ«å™¨å·²å¯ç”¨")

    async def _try_initialize_synthesizers(self):
        """å°è¯•åˆå§‹åŒ–è¯­éŸ³åˆæˆå™¨"""
        
        # åªåˆå§‹åŒ– CosyVoice
        try:
            config = self.config['cosyvoice'].copy()
            config['device'] = self.config['device']
            
            # æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
            if Path(config['model_dir']).exists():
                synthesizer = CosyVoiceSynthesizer(config)
                if await synthesizer.initialize():
                    self.synthesizers['cosyvoice'] = synthesizer
                    self.default_synthesizer = 'cosyvoice'
                    logger.info("âœ… CosyVoice åˆæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
                else:
                    logger.error("âŒ CosyVoice åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥")
                    raise RuntimeError("CosyVoice åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥")
            else:
                logger.error(f"âŒ CosyVoice æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {config['model_dir']}")
                raise FileNotFoundError(f"CosyVoice æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {config['model_dir']}")
        except Exception as e:
            logger.error(f"âŒ CosyVoice åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    async def recognize(self, 
                       audio_data: bytes,
                       language: str = "zh-CN",
                       model_name: Optional[str] = None,
                       request_id: Optional[str] = None,
                       **kwargs) -> SpeechRecognitionResponse:
        """è¯­éŸ³è¯†åˆ« - ä¼˜åŒ–ç‰ˆ"""
        if not self.is_initialized:
            raise RuntimeError("è¯­éŸ³å¤„ç†å™¨æœªåˆå§‹åŒ–")
        
        # é€‰æ‹©è¯†åˆ«å™¨
        recognizer_name = model_name or self.default_recognizer
        if recognizer_name not in self.recognizers:
            # å›é€€åˆ°é»˜è®¤è¯†åˆ«å™¨
            recognizer_name = self.default_recognizer
            logger.warning(f"âš ï¸ æŒ‡å®šçš„è¯†åˆ«å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è¯†åˆ«å™¨: {recognizer_name}")
        
        recognizer = self.recognizers[recognizer_name]
        
        try:
            logger.info(f"ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ« - æ¨¡å‹: {recognizer_name}, è¯­è¨€: {language}")
            
            result = await recognizer.recognize(audio_data, language, **kwargs)
            
            logger.info(f"âœ… è¯­éŸ³è¯†åˆ«å®Œæˆ - æ–‡æœ¬é•¿åº¦: {len(result['text'])}")
            
            return SpeechRecognitionResponse(
                success=True,
                text=result["text"],
                language=result["language"],
                confidence=result["confidence"],
                processing_time=result["processing_time"],
                model_used=result["model_used"],
                request_id=request_id or generate_response_id(),
                timestamp=datetime.utcnow(),
                segments=result.get("segments", [])
            )
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")
            # è¿”å›é”™è¯¯å“åº”è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return SpeechRecognitionResponse(
                success=False,
                text="",
                language=language,
                confidence=0.0,
                processing_time=0.0,
                model_used=recognizer_name,
                request_id=request_id or generate_response_id(),
                timestamp=datetime.utcnow(),
                message=f"è¯†åˆ«å¤±è´¥: {str(e)}"
            )

    async def synthesize(self, 
                        text: str,
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        tts_model: Optional[str] = None,
                        request_id: Optional[str] = None,
                        **kwargs) -> SpeechSynthesisResponse:
        """è¯­éŸ³åˆæˆ - ä¼˜åŒ–ç‰ˆ"""
        if not self.is_initialized:
            raise RuntimeError("è¯­éŸ³å¤„ç†å™¨æœªåˆå§‹åŒ–")
        
        # å¼ºåˆ¶ä½¿ç”¨CosyVoiceåˆæˆå™¨ï¼Œå¿½ç•¥å…¶ä»–è®¾ç½®
        synthesizer_name = 'cosyvoice'
        if synthesizer_name not in self.synthesizers:
            raise ValueError(f"CosyVoiceè¯­éŸ³åˆæˆå™¨ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿CosyVoice2-0.5bæ¨¡å‹å·²å®‰è£…")
        
        synthesizer = self.synthesizers[synthesizer_name]
        
        try:
            logger.info(f"ğŸ”Š å¼€å§‹è¯­éŸ³åˆæˆ - æ¨¡å‹: {synthesizer_name}, æ–‡æœ¬é•¿åº¦: {len(text)}")
            
            result = await synthesizer.synthesize(
                text=text,
                voice=voice,
                language=language,
                speed=speed,
                pitch=pitch,
                **kwargs
            )
            
            logger.info(f"âœ… è¯­éŸ³åˆæˆå®Œæˆ - éŸ³é¢‘æ—¶é•¿: {result['duration']:.2f}s")
            
            return SpeechSynthesisResponse(
                success=True,
                audio_data=result["audio_data"],
                format=result["format"],
                duration=result["duration"],
                processing_time=result["processing_time"],
                model_used=result["model_used"],
                request_id=request_id or generate_response_id(),
                timestamp=datetime.utcnow(),
                synthesis_mode=result.get("synthesis_mode", "default"),
                sample_rate=result.get("sample_rate", 16000)
            )
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³åˆæˆå¤±è´¥: {str(e)}")
            raise

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            return {
                "healthy": self.is_initialized and (len(self.recognizers) > 0 or len(self.synthesizers) > 0),
                "recognizers": {
                    "available": list(self.recognizers.keys()),
                    "default": self.default_recognizer,
                    "count": len(self.recognizers)
                },
                "synthesizers": {
                    "available": list(self.synthesizers.keys()),
                    "default": self.default_synthesizer,
                    "count": len(self.synthesizers)
                },
                "config": {
                    "device": self.config['device'],
                    "cosyvoice_model_dir": self.config['cosyvoice']['model_dir'],
                    "sensevoice_model": self.config['sensevoice']['model']
                }
            }
        
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }

    async def get_available_voices(self, synthesizer_name: Optional[str] = None) -> Dict[str, List[str]]:
        """è·å–å¯ç”¨çš„å£°éŸ³åˆ—è¡¨"""
        voices = {}
        
        target_synthesizers = [synthesizer_name] if synthesizer_name else self.synthesizers.keys()
        
        for name in target_synthesizers:
            if name in self.synthesizers:
                synthesizer = self.synthesizers[name]
                if hasattr(synthesizer, 'get_available_voices'):
                    voices[name] = await synthesizer.get_available_voices()
                else:
                    voices[name] = ["default"]
        
        return voices

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†è¯­éŸ³å¤„ç†å™¨èµ„æº")
        
        # æ¸…ç†è¯†åˆ«å™¨
        for recognizer in self.recognizers.values():
            if hasattr(recognizer, 'cleanup'):
                try:
                    await recognizer.cleanup()
                except Exception as e:
                    logger.warning(f"æ¸…ç†è¯†åˆ«å™¨æ—¶å‡ºé”™: {str(e)}")
        
        # æ¸…ç†åˆæˆå™¨
        for synthesizer in self.synthesizers.values():
            if hasattr(synthesizer, 'cleanup'):
                try:
                    await synthesizer.cleanup()
                except Exception as e:
                    logger.warning(f"æ¸…ç†åˆæˆå™¨æ—¶å‡ºé”™: {str(e)}")
        
        self.recognizers.clear()
        self.synthesizers.clear()
        self.default_recognizer = None
        self.default_synthesizer = None
        self.is_initialized = False
        
        logger.info("âœ… è¯­éŸ³å¤„ç†å™¨èµ„æºæ¸…ç†å®Œæˆ")

# å…¨å±€å®ä¾‹
speech_processor = SpeechProcessor()

# ä¾¿æ·å‡½æ•°
async def initialize_speech_processor():
    """åˆå§‹åŒ–å…¨å±€è¯­éŸ³å¤„ç†å™¨"""
    await speech_processor.initialize()

async def recognize_speech(audio_data: bytes, **kwargs) -> SpeechRecognitionResponse:
    """ä¾¿æ·çš„è¯­éŸ³è¯†åˆ«å‡½æ•°"""
    return await speech_processor.recognize(audio_data, **kwargs)

async def synthesize_speech(text: str, **kwargs) -> SpeechSynthesisResponse:
    """ä¾¿æ·çš„è¯­éŸ³åˆæˆå‡½æ•°"""
    return await speech_processor.synthesize(text, **kwargs)