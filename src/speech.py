"""
è¯­éŸ³å¤„ç†å™¨
è´Ÿè´£è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³åˆæˆåŠŸèƒ½
"""

import asyncio
import base64
import tempfile
import time
import uuid
from typing import Dict, Any, Optional
from datetime import datetime
from pathlib import Path
import json

from .logger import get_logger, log_speech_operation
from .models import SpeechRecognitionResponse, SpeechSynthesisResponse, AudioFormat
from .utils import generate_response_id, save_file_async, load_file_async

logger = get_logger(__name__)

class SpeechRecognizer:
    """è¯­éŸ³è¯†åˆ«å™¨åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_initialized = False
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–è¯†åˆ«å™¨"""
        try:
            await self._setup()
            self.is_initialized = True
            return True
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    async def _setup(self):
        """è®¾ç½®è¯†åˆ«å™¨ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        pass
    
    async def recognize(self, 
                       audio_data: bytes, 
                       language: str = "zh-CN",
                       **kwargs) -> Dict[str, Any]:
        """è¯†åˆ«è¯­éŸ³ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        raise NotImplementedError

class SensVoiceRecognizer(SpeechRecognizer):
    """SensVoice è¯­éŸ³è¯†åˆ«å™¨"""
    
    async def _setup(self):
        """è®¾ç½® SensVoice"""
        try:
            # è¿™é‡Œé›†æˆæ‚¨ä¹‹å‰åˆ›å»ºçš„ SensVoice MCP å®¢æˆ·ç«¯
            # æˆ–è€…ç›´æ¥å¯¼å…¥ funasr
            from funasr import AutoModel
            
            # åˆå§‹åŒ–æ¨¡å‹
            self.model = AutoModel(
                model="paraformer-zh",
                vad_model="fsmn-vad",
                punc_model="ct-punc",
                device=self.config.get('device', 'cpu')
            )
            
            logger.info("âœ… SensVoice æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
        except ImportError:
            logger.warning("âš ï¸ FunASR æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè¯†åˆ«å™¨")
            self.model = None
        except Exception as e:
            logger.error(f"âŒ SensVoice åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def recognize(self, 
                       audio_data: bytes, 
                       language: str = "zh-CN",
                       **kwargs) -> Dict[str, Any]:
        """SensVoice è¯­éŸ³è¯†åˆ«"""
        start_time = time.time()
        
        try:
            if self.model is None:
                # æ¨¡æ‹Ÿè¯†åˆ«ç»“æœ
                await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                
                processing_time = time.time() - start_time
                
                return {
                    "text": "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„è¯­éŸ³è¯†åˆ«ç»“æœ",
                    "language": language,
                    "confidence": 0.95,
                    "processing_time": processing_time,
                    "model_used": "mock_sensvoice"
                }
            
            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_file.write(audio_data)
                temp_audio_path = temp_file.name
            
            try:
                # æ‰§è¡Œè¯†åˆ«
                result = self.model.generate(
                    input=temp_audio_path,
                    **kwargs
                )
                
                processing_time = time.time() - start_time
                
                # æå–è¯†åˆ«ç»“æœ
                if result and len(result) > 0:
                    text = result[0].get('text', '')
                    confidence = result[0].get('confidence', 0.0)
                else:
                    text = ""
                    confidence = 0.0
                
                log_speech_operation(
                    logger, "recognition", "sensvoice", 
                    len(audio_data), len(text), processing_time, 
                    True, language
                )
                
                return {
                    "text": text,
                    "language": language,
                    "confidence": confidence,
                    "processing_time": processing_time,
                    "model_used": "sensvoice"
                }
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                Path(temp_audio_path).unlink(missing_ok=True)
                
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_speech_operation(
                logger, "recognition", "sensvoice", 
                len(audio_data), 0, processing_time, 
                False, language, error_msg
            )
            
            raise Exception(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {error_msg}")

class WhisperRecognizer(SpeechRecognizer):
    """Whisper è¯­éŸ³è¯†åˆ«å™¨"""
    
    async def _setup(self):
        """è®¾ç½® Whisper"""
        try:
            import whisper
            
            model_size = self.config.get('model_size', 'base')
            self.model = whisper.load_model(model_size)
            
            logger.info(f"âœ… Whisper æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_size}")
            
        except ImportError:
            logger.warning("âš ï¸ Whisper æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè¯†åˆ«å™¨")
            self.model = None
        except Exception as e:
            logger.error(f"âŒ Whisper åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def recognize(self, 
                       audio_data: bytes, 
                       language: str = "zh",
                       **kwargs) -> Dict[str, Any]:
        """Whisper è¯­éŸ³è¯†åˆ«"""
        start_time = time.time()
        
        try:
            if self.model is None:
                # æ¨¡æ‹Ÿè¯†åˆ«ç»“æœ
                await asyncio.sleep(1.0)
                
                processing_time = time.time() - start_time
                
                return {
                    "text": "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„ Whisper è¯†åˆ«ç»“æœ",
                    "language": language,
                    "confidence": 0.90,
                    "processing_time": processing_time,
                    "model_used": "mock_whisper"
                }
            
            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_file.write(audio_data)
                temp_audio_path = temp_file.name
            
            try:
                # æ‰§è¡Œè¯†åˆ«
                result = self.model.transcribe(
                    temp_audio_path,
                    language=language if language != "zh-CN" else "zh",
                    **kwargs
                )
                
                processing_time = time.time() - start_time
                
                text = result.get('text', '')
                
                # Whisper ä¸ç›´æ¥æä¾›ç½®ä¿¡åº¦ï¼Œä½¿ç”¨å¹³å‡æ¦‚ç‡
                segments = result.get('segments', [])
                confidence = 0.0
                if segments:
                    confidences = [seg.get('avg_logprob', 0.0) for seg in segments]
                    confidence = sum(confidences) / len(confidences)
                    confidence = max(0.0, min(1.0, (confidence + 1.0) / 2.0))  # è½¬æ¢ä¸º0-1èŒƒå›´
                
                log_speech_operation(
                    logger, "recognition", "whisper", 
                    len(audio_data), len(text), processing_time, 
                    True, language
                )
                
                return {
                    "text": text,
                    "language": language,
                    "confidence": confidence,
                    "processing_time": processing_time,
                    "model_used": "whisper"
                }
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                Path(temp_audio_path).unlink(missing_ok=True)
                
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_speech_operation(
                logger, "recognition", "whisper", 
                len(audio_data), 0, processing_time, 
                False, language, error_msg
            )
            
            raise Exception(f"Whisper è¯†åˆ«å¤±è´¥: {error_msg}")

class SpeechSynthesizer:
    """è¯­éŸ³åˆæˆå™¨åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_initialized = False
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–åˆæˆå™¨"""
        try:
            await self._setup()
            self.is_initialized = True
            return True
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    async def _setup(self):
        """è®¾ç½®åˆæˆå™¨ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        pass
    
    async def synthesize(self, 
                        text: str, 
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        **kwargs) -> Dict[str, Any]:
        """åˆæˆè¯­éŸ³ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        raise NotImplementedError

class CosyVoiceSynthesizer(SpeechSynthesizer):
    """CosyVoice è¯­éŸ³åˆæˆå™¨"""
    
    async def _setup(self):
        """è®¾ç½® CosyVoice"""
        try:
            # è¿™é‡Œé›†æˆæ‚¨ä¹‹å‰åˆ›å»ºçš„ CosyVoice MCP å®¢æˆ·ç«¯
            # æˆ–è€…ç›´æ¥å¯¼å…¥ CosyVoice
            import sys
            sys.path.append('third_party/Matcha-TTS')
            
            from cosyvoice.cli.cosyvoice import CosyVoice2
            from cosyvoice.utils.file_utils import load_wav
            import torchaudio
            
            model_dir = self.config.get('model_dir', 'pretrained_models/CosyVoice2-0.5B')
            
            self.model = CosyVoice2(model_dir, load_jit=False, load_trt=False, fp16=False)
            self.load_wav = load_wav
            self.torchaudio = torchaudio
            
            logger.info("âœ… CosyVoice æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
        except ImportError:
            logger.warning("âš ï¸ CosyVoice æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿåˆæˆå™¨")
            self.model = None
        except Exception as e:
            logger.error(f"âŒ CosyVoice åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def synthesize(self, 
                        text: str, 
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        **kwargs) -> Dict[str, Any]:
        """CosyVoice è¯­éŸ³åˆæˆ"""
        start_time = time.time()
        
        try:
            if self.model is None:
                # æ¨¡æ‹Ÿåˆæˆç»“æœ
                await asyncio.sleep(1.5)
                
                processing_time = time.time() - start_time
                
                # åˆ›å»ºæ¨¡æ‹ŸéŸ³é¢‘æ•°æ®ï¼ˆé™éŸ³ï¼‰
                sample_rate = 22050
                duration = 2.0  # 2ç§’
                audio_samples = int(sample_rate * duration)
                mock_audio = b'\x00' * (audio_samples * 2)  # 16ä½PCM
                
                audio_base64 = base64.b64encode(mock_audio).decode('utf-8')
                
                return {
                    "audio_data": audio_base64,
                    "format": AudioFormat.WAV,
                    "duration": duration,
                    "processing_time": processing_time,
                    "model_used": "mock_cosyvoice"
                }
            
            # ä½¿ç”¨é¢„è®¾çš„å‚è€ƒéŸ³é¢‘ï¼ˆéœ€è¦æä¾›ï¼‰
            reference_audio_path = self.config.get('reference_audio', 'reference.wav')
            reference_text = self.config.get('reference_text', 'å‚è€ƒéŸ³é¢‘æ–‡æœ¬')
            
            if not Path(reference_audio_path).exists():
                # å¦‚æœæ²¡æœ‰å‚è€ƒéŸ³é¢‘ï¼Œä½¿ç”¨ SFT æ¨¡å¼ï¼ˆå¦‚æœæ”¯æŒï¼‰
                logger.warning("âš ï¸ æœªæ‰¾åˆ°å‚è€ƒéŸ³é¢‘ï¼Œä½¿ç”¨é»˜è®¤åˆæˆæ¨¡å¼")
                
                # æ¨¡æ‹Ÿè¾“å‡º
                processing_time = time.time() - start_time
                mock_audio = b'\x00' * (22050 * 2 * 2)  # 2ç§’é™éŸ³
                audio_base64 = base64.b64encode(mock_audio).decode('utf-8')
                
                return {
                    "audio_data": audio_base64,
                    "format": AudioFormat.WAV,
                    "duration": 2.0,
                    "processing_time": processing_time,
                    "model_used": "cosyvoice_sft"
                }
            
            # åŠ è½½å‚è€ƒéŸ³é¢‘
            reference_audio = self.load_wav(reference_audio_path, 16000)
            
            # æ‰§è¡Œåˆæˆ
            output_audio = None
            for i, result in enumerate(self.model.inference_zero_shot(
                text, reference_text, reference_audio, stream=False
            )):
                output_audio = result['tts_speech']
                break  # åªå–ç¬¬ä¸€ä¸ªç»“æœ
            
            if output_audio is None:
                raise Exception("åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
            
            processing_time = time.time() - start_time
            
            # è½¬æ¢ä¸ºå­—èŠ‚æ•°æ®
            import io
            buffer = io.BytesIO()
            self.torchaudio.save(buffer, output_audio, self.model.sample_rate, format='wav')
            buffer.seek(0)
            audio_data = buffer.read()
            
            # ç¼–ç ä¸ºbase64
            audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            
            # è®¡ç®—æ—¶é•¿
            duration = output_audio.shape[1] / self.model.sample_rate
            
            log_speech_operation(
                logger, "synthesis", "cosyvoice", 
                len(text), len(audio_data), processing_time, 
                True, language
            )
            
            return {
                "audio_data": audio_base64,
                "format": AudioFormat.WAV,
                "duration": duration,
                "processing_time": processing_time,
                "model_used": "cosyvoice"
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_speech_operation(
                logger, "synthesis", "cosyvoice", 
                len(text), 0, processing_time, 
                False, language, error_msg
            )
            
            raise Exception(f"CosyVoice åˆæˆå¤±è´¥: {error_msg}")

class EdgeTTSSynthesizer(SpeechSynthesizer):
    """Edge TTS è¯­éŸ³åˆæˆå™¨"""
    
    async def _setup(self):
        """è®¾ç½® Edge TTS"""
        try:
            import edge_tts
            self.edge_tts = edge_tts
            
            logger.info("âœ… Edge TTS åˆå§‹åŒ–æˆåŠŸ")
            
        except ImportError:
            logger.warning("âš ï¸ Edge TTS æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿåˆæˆå™¨")
            self.edge_tts = None
        except Exception as e:
            logger.error(f"âŒ Edge TTS åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def synthesize(self, 
                        text: str, 
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        **kwargs) -> Dict[str, Any]:
        """Edge TTS è¯­éŸ³åˆæˆ"""
        start_time = time.time()
        
        try:
            if self.edge_tts is None:
                # æ¨¡æ‹Ÿåˆæˆç»“æœ
                await asyncio.sleep(0.8)
                
                processing_time = time.time() - start_time
                
                # åˆ›å»ºæ¨¡æ‹ŸéŸ³é¢‘æ•°æ®
                mock_audio = b'\x00' * (16000 * 2 * 2)  # 2ç§’ï¼Œ16kHzï¼Œ16ä½
                audio_base64 = base64.b64encode(mock_audio).decode('utf-8')
                
                return {
                    "audio_data": audio_base64,
                    "format": AudioFormat.WAV,
                    "duration": 2.0,
                    "processing_time": processing_time,
                    "model_used": "mock_edge_tts"
                }
            
            # é€‰æ‹©å£°éŸ³
            if not voice:
                voice = self._get_default_voice(language)
            
            # åˆ›å»ºTTSå®ä¾‹
            tts = self.edge_tts.Communicate(text, voice)
            
            # ç”ŸæˆéŸ³é¢‘
            audio_data = b""
            async for chunk in tts.stream():
                if chunk["type"] == "audio":
                    audio_data += chunk["data"]
            
            processing_time = time.time() - start_time
            
            # ç¼–ç ä¸ºbase64
            audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            
            # ä¼°ç®—æ—¶é•¿ï¼ˆç®€å•ä¼°ç®—ï¼Œå®é™…å¯èƒ½éœ€è¦è§£æéŸ³é¢‘æ–‡ä»¶ï¼‰
            duration = len(text) * 0.1  # ç®€å•ä¼°ç®—ï¼šæ¯ä¸ªå­—ç¬¦0.1ç§’
            
            log_speech_operation(
                logger, "synthesis", "edge_tts", 
                len(text), len(audio_data), processing_time, 
                True, language
            )
            
            return {
                "audio_data": audio_base64,
                "format": AudioFormat.MP3,  # Edge TTS é»˜è®¤è¾“å‡ºMP3
                "duration": duration,
                "processing_time": processing_time,
                "model_used": "edge_tts"
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_speech_operation(
                logger, "synthesis", "edge_tts", 
                len(text), 0, processing_time, 
                False, language, error_msg
            )
            
            raise Exception(f"Edge TTS åˆæˆå¤±è´¥: {error_msg}")
    
    def _get_default_voice(self, language: str) -> str:
        """è·å–é»˜è®¤å£°éŸ³"""
        voice_map = {
            "zh-CN": "zh-CN-XiaoxiaoNeural",
            "zh-TW": "zh-TW-HsiaoyuNeural", 
            "en-US": "en-US-AriaNeural",
            "ja-JP": "ja-JP-NanamiNeural",
            "ko-KR": "ko-KR-SunHiNeural"
        }
        return voice_map.get(language, "zh-CN-XiaoxiaoNeural")

class SpeechProcessor:
    """è¯­éŸ³å¤„ç†å™¨ä¸»ç±»"""
    
    def __init__(self):
        self.recognizers: Dict[str, SpeechRecognizer] = {}
        self.synthesizers: Dict[str, SpeechSynthesizer] = {}
        self.default_recognizer = None
        self.default_synthesizer = None
        self.is_initialized = False
        
        # å¯ç”¨çš„å¼•æ“
        self.available_recognizers = {
            "sensvoice": SensVoiceRecognizer,
            "whisper": WhisperRecognizer
        }
        
        self.available_synthesizers = {
            "cosyvoice": CosyVoiceSynthesizer,
            "edge_tts": EdgeTTSSynthesizer
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–è¯­éŸ³å¤„ç†å™¨"""
        logger.info("ğŸ”§ åˆå§‹åŒ–è¯­éŸ³å¤„ç†å™¨")
        
        # åˆå§‹åŒ–è¯†åˆ«å™¨
        await self._initialize_recognizers()
        
        # åˆå§‹åŒ–åˆæˆå™¨
        await self._initialize_synthesizers()
        
        self.is_initialized = True
        logger.info("âœ… è¯­éŸ³å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def _initialize_recognizers(self):
        """åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨"""
        for name, recognizer_class in self.available_recognizers.items():
            try:
                config = {
                    "device": "cpu",  # å¯ä»¥ä»é…ç½®æ–‡ä»¶è¯»å–
                    "model_size": "base"  # Whisper é…ç½®
                }
                
                recognizer = recognizer_class(config)
                if await recognizer.initialize():
                    self.recognizers[name] = recognizer
                    if self.default_recognizer is None:
                        self.default_recognizer = name
                    logger.info(f"âœ… è¯­éŸ³è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ: {name}")
                else:
                    logger.warning(f"âš ï¸ è¯­éŸ³è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {name}")
            
            except Exception as e:
                logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å™¨åˆå§‹åŒ–å¼‚å¸¸ [{name}]: {str(e)}")
    
    async def _initialize_synthesizers(self):
        """åˆå§‹åŒ–è¯­éŸ³åˆæˆå™¨"""
        for name, synthesizer_class in self.available_synthesizers.items():
            try:
                config = {
                    "model_dir": "pretrained_models/CosyVoice2-0.5B",  # CosyVoice é…ç½®
                    "reference_audio": "reference.wav",
                    "reference_text": "å‚è€ƒéŸ³é¢‘æ–‡æœ¬"
                }
                
                synthesizer = synthesizer_class(config)
                if await synthesizer.initialize():
                    self.synthesizers[name] = synthesizer
                    if self.default_synthesizer is None:
                        self.default_synthesizer = name
                    logger.info(f"âœ… è¯­éŸ³åˆæˆå™¨åˆå§‹åŒ–æˆåŠŸ: {name}")
                else:
                    logger.warning(f"âš ï¸ è¯­éŸ³åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥: {name}")
            
            except Exception as e:
                logger.error(f"âŒ è¯­éŸ³åˆæˆå™¨åˆå§‹åŒ–å¼‚å¸¸ [{name}]: {str(e)}")
    
    async def recognize(self, 
                       audio_data: bytes,
                       language: str = "zh-CN",
                       model_name: Optional[str] = None,
                       request_id: Optional[str] = None) -> SpeechRecognitionResponse:
        """è¯­éŸ³è¯†åˆ«"""
        if not self.is_initialized:
            raise RuntimeError("è¯­éŸ³å¤„ç†å™¨æœªåˆå§‹åŒ–")
        
        # é€‰æ‹©è¯†åˆ«å™¨
        recognizer_name = model_name or self.default_recognizer
        if recognizer_name not in self.recognizers:
            raise ValueError(f"è¯­éŸ³è¯†åˆ«å™¨ä¸å¯ç”¨: {recognizer_name}")
        
        recognizer = self.recognizers[recognizer_name]
        
        try:
            logger.info(f"ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ« - æ¨¡å‹: {recognizer_name}, è¯­è¨€: {language}")
            
            result = await recognizer.recognize(
                audio_data=audio_data,
                language=language
            )
            
            logger.info(f"âœ… è¯­éŸ³è¯†åˆ«å®Œæˆ - æ–‡æœ¬é•¿åº¦: {len(result['text'])}")
            
            return SpeechRecognitionResponse(
                success=True,
                text=result["text"],
                language=result["language"],
                confidence=result["confidence"],
                processing_time=result["processing_time"],
                model_used=result["model_used"],
                request_id=request_id or generate_response_id(),
                timestamp=datetime.utcnow()
            )
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")
            raise
    
    async def synthesize(self, 
                        text: str,
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        tts_model: Optional[str] = None,
                        request_id: Optional[str] = None) -> SpeechSynthesisResponse:
        """è¯­éŸ³åˆæˆ"""
        if not self.is_initialized:
            raise RuntimeError("è¯­éŸ³å¤„ç†å™¨æœªåˆå§‹åŒ–")
        
        # é€‰æ‹©åˆæˆå™¨
        synthesizer_name = tts_model or self.default_synthesizer
        if synthesizer_name not in self.synthesizers:
            raise ValueError(f"è¯­éŸ³åˆæˆå™¨ä¸å¯ç”¨: {synthesizer_name}")
        
        synthesizer = self.synthesizers[synthesizer_name]
        
        try:
            logger.info(f"ğŸ”Š å¼€å§‹è¯­éŸ³åˆæˆ - æ¨¡å‹: {synthesizer_name}, æ–‡æœ¬é•¿åº¦: {len(text)}")
            
            result = await synthesizer.synthesize(
                text=text,
                voice=voice,
                language=language,
                speed=speed,
                pitch=pitch
            )
            
            logger.info(f"âœ… è¯­éŸ³åˆæˆå®Œæˆ - éŸ³é¢‘æ—¶é•¿: {result['duration']:.2f}s")
            
            return SpeechSynthesisResponse(
                success=True,
                audio_data=result["audio_data"],
                format=result["format"],
                duration=result["duration"],
                processing_time=result["processing_time"],
                model_used=result["model_used"],
                request_id=request_id or generate_response_id(),
                timestamp=datetime.utcnow()
            )
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³åˆæˆå¤±è´¥: {str(e)}")
            raise
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            return {
                "healthy": len(self.recognizers) > 0 or len(self.synthesizers) > 0,
                "recognizers": {
                    "available": list(self.recognizers.keys()),
                    "default": self.default_recognizer
                },
                "synthesizers": {
                    "available": list(self.synthesizers.keys()),
                    "default": self.default_synthesizer
                }
            }
        
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†è¯­éŸ³å¤„ç†å™¨èµ„æº")
        
        self.recognizers.clear()
        self.synthesizers.clear()
        self.default_recognizer = None
        self.default_synthesizer = None
        self.is_initialized = False
        
        logger.info("âœ… è¯­éŸ³å¤„ç†å™¨èµ„æºæ¸…ç†å®Œæˆ")