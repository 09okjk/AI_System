"""
è¯­éŸ³å¤„ç†å™¨ - ä¼˜åŒ–ç‰ˆæœ¬
è´Ÿè´£è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³åˆæˆåŠŸèƒ½
åŸºäº SenseVoice å’Œ CosyVoice å®˜æ–¹å®ç°ä¼˜åŒ–
"""

import asyncio
import base64
import tempfile
import time
import uuid
from typing import Dict, Any, Optional, List, Union
from datetime import datetime
from pathlib import Path
import json
import os
import numpy as np

from .logger import get_logger, log_speech_operation
from .models import SpeechRecognitionResponse, SpeechSynthesisResponse, AudioFormat
from .utils import generate_response_id

logger = get_logger(__name__)

# æ·»åŠ éŸ³é¢‘æ ¼å¼è½¬æ¢å‡½æ•°
def convert_audio_to_wav(input_path: str, output_path: Optional[str] = None, sample_rate: int = 16000) -> str:
    """
    å°†ä»»æ„éŸ³é¢‘æ ¼å¼è½¬æ¢ä¸ºWAVæ ¼å¼
    
    Args:
        input_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºWAVæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ç”Ÿæˆä¸€ä¸ªä¸´æ—¶æ–‡ä»¶
        sample_rate: è¾“å‡ºéŸ³é¢‘çš„é‡‡æ ·ç‡
        
    Returns:
        WAVæ–‡ä»¶çš„è·¯å¾„
    """
    try:
        import librosa
        import soundfile as sf
        
        # å¦‚æœæœªæŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œåˆ›å»ºä¸´æ—¶æ–‡ä»¶
        if not output_path:
            temp_dir = Path(tempfile.gettempdir()) / "ai_system_audio"
            temp_dir.mkdir(exist_ok=True)
            output_path = str(temp_dir / f"{uuid.uuid4()}.wav")
        
        # åŠ è½½éŸ³é¢‘æ–‡ä»¶
        logger.info(f"æ­£åœ¨è½¬æ¢éŸ³é¢‘: {input_path} -> {output_path}")
        y, sr = librosa.load(input_path, sr=sample_rate)
        
        # ä¿å­˜ä¸ºWAVæ ¼å¼
        sf.write(output_path, y, sample_rate, subtype='PCM_16')
        logger.info(f"éŸ³é¢‘è½¬æ¢å®Œæˆ: {output_path}")
        
        return output_path
    except Exception as e:
        logger.error(f"éŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
        raise

class SpeechRecognizer:
    """è¯­éŸ³è¯†åˆ«å™¨åŸºç±»"""    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_initialized = False
        self.model = None
        self.device = config.get('device', 'cpu')

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–è¯†åˆ«å™¨"""
        try:
            await self._setup()
            self.is_initialized = True
            logger.info(f"âœ… {self.__class__.__name__} åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False

    async def _setup(self):
        """è®¾ç½®è¯†åˆ«å™¨ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        pass

    async def recognize(self, 
                        audio_data: bytes, 
                        language: str = "zh-CN",
                        **kwargs) -> Dict[str, Any]:
        """è¯†åˆ«è¯­éŸ³ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        raise NotImplementedError

class SenseVoiceRecognizer(SpeechRecognizer):
    """SenseVoice è¯­éŸ³è¯†åˆ«å™¨ - åŸºäºå®˜æ–¹å®ç°ä¼˜åŒ–"""
    
    async def _setup(self):
        """è®¾ç½® SenseVoice - æ ¹æ®å®˜æ–¹æœ€ä½³å®è·µ"""
        try:
            from funasr import AutoModel
            
            # æ ¹æ®å®˜æ–¹æ¨èçš„é…ç½®
            model_config = {
                "model": "iic/SenseVoiceSmall",  # ä½¿ç”¨å®˜æ–¹æ¨èçš„æ¨¡å‹
                "vad_model": "fsmn-vad",
                "punc_model": "ct-punc", 
                "device": self.device,
                "hub": "ms",  # ä½¿ç”¨ modelscope
                "ncpu": self.config.get('ncpu', 4),
                "batch_size": self.config.get('batch_size', 1)
            }
            
            # æ”¯æŒ VAD çš„é…ç½®
            vad_kwargs = {
                "max_single_segment_time": self.config.get('max_single_segment_time', 60000),  # 60ç§’
                "batch_size_s": self.config.get('batch_size_s', 300),  # åŠ¨æ€batchï¼Œæ€»éŸ³é¢‘æ—¶é•¿300ç§’
                "batch_size_threshold_s": self.config.get('batch_size_threshold_s', 60)  # é˜ˆå€¼60ç§’
            }
            
            # åˆå§‹åŒ–æ¨¡å‹
            self.model = AutoModel(
                model=model_config["model"],
                vad_model=model_config["vad_model"],
                vad_kwargs=vad_kwargs,
                punc_model=model_config["punc_model"],
                device=model_config["device"],
                hub=model_config["hub"],
                ncpu=model_config["ncpu"],
                batch_size=model_config["batch_size"]
            )
            
            # å­˜å‚¨æ¨¡å‹é…ç½®ç”¨äºåç»­ä½¿ç”¨
            self.model_config = model_config
            self.vad_kwargs = vad_kwargs
            
            logger.info(f"âœ… SenseVoice æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ - è®¾å¤‡: {self.device}")
            
        except ImportError:
            logger.warning("âš ï¸ FunASR æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè¯†åˆ«å™¨")
            self.model = None
        except Exception as e:
            logger.error(f"âŒ SenseVoice åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def recognize(self, 
                       audio_data: bytes, 
                       language: str = "zh-CN",
                       **kwargs) -> Dict[str, Any]:
        """SenseVoice è¯­éŸ³è¯†åˆ« - ä¼˜åŒ–ç‰ˆæœ¬"""
        start_time = time.time()
        
        try:
            if self.model is None:
                # æ¨¡æ‹Ÿè¯†åˆ«ç»“æœ
                await asyncio.sleep(0.5)
                
                processing_time = time.time() - start_time
                
                return {
                    "text": "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„è¯­éŸ³è¯†åˆ«ç»“æœ",
                    "language": language,
                    "confidence": 0.95,
                    "processing_time": processing_time,
                    "model_used": "mock_sensevoice",
                    "segments": []
                }
            
            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_file.write(audio_data)
                temp_audio_path = temp_file.name
            
            try:
                # ä½¿ç”¨å®˜æ–¹æ¨èçš„å‚æ•°æ‰§è¡Œè¯†åˆ«
                generation_kwargs = {
                    "hotword": kwargs.get('hotword', ''),  # çƒ­è¯
                    "batch_size_s": self.vad_kwargs.get('batch_size_s', 300),
                    "batch_size_threshold_s": self.vad_kwargs.get('batch_size_threshold_s', 60)
                }
                
                # æ·»åŠ è‡ªå®šä¹‰å‚æ•°
                generation_kwargs.update(kwargs)
                
                # æ‰§è¡Œè¯†åˆ«
                result = self.model.generate(
                    input=temp_audio_path,
                    **generation_kwargs
                )
                
                processing_time = time.time() - start_time
                
                # å¤„ç†è¯†åˆ«ç»“æœ
                if result and len(result) > 0:
                    # SenseVoice è¿”å›æ ¼å¼é€šå¸¸æ˜¯åˆ—è¡¨
                    first_result = result[0] if isinstance(result, list) else result
                    
                    # æå–æ–‡æœ¬
                    text = first_result.get('text', '') if isinstance(first_result, dict) else str(first_result)
                    
                    # æå–ç½®ä¿¡åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    confidence = first_result.get('confidence', 0.9) if isinstance(first_result, dict) else 0.9
                    
                    # æå–æ—¶é—´æˆ³ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    segments = first_result.get('timestamps', []) if isinstance(first_result, dict) else []
                    
                else:
                    text = ""
                    confidence = 0.0
                    segments = []
                
                log_speech_operation(
                    logger, "recognition", "sensevoice", 
                    len(audio_data), len(text), processing_time, 
                    True, language
                )
                
                return {
                    "text": text,
                    "language": language,
                    "confidence": confidence,
                    "processing_time": processing_time,
                    "model_used": "sensevoice",
                    "segments": segments,
                    "raw_result": result  # ä¿ç•™åŸå§‹ç»“æœä»¥ä¾¿è°ƒè¯•
                }
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                Path(temp_audio_path).unlink(missing_ok=True)
                
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_speech_operation(
                logger, "recognition", "sensevoice", 
                len(audio_data), 0, processing_time, 
                False, language, error_msg
            )
            
            raise Exception(f"SenseVoice è¯­éŸ³è¯†åˆ«å¤±è´¥: {error_msg}")

class SpeechSynthesizer:
    """è¯­éŸ³åˆæˆå™¨åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_initialized = False
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–åˆæˆå™¨"""
        try:
            await self._setup()
            self.is_initialized = True
            return True
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    async def _setup(self):
        """è®¾ç½®åˆæˆå™¨ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        pass
    
    async def synthesize(self, 
                        text: str, 
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        **kwargs) -> Dict[str, Any]:
        """åˆæˆè¯­éŸ³ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        raise NotImplementedError
class CosyVoiceSynthesizer(SpeechSynthesizer):
    """CosyVoice è¯­éŸ³åˆæˆå™¨ - åŸºäºå®˜æ–¹å®ç°ä¼˜åŒ–"""
    
    async def _setup(self):
        """è®¾ç½® CosyVoice - æ ¹æ®å®˜æ–¹æœ€ä½³å®è·µ"""
        try:
            # è®¾ç½® CosyVoice è·¯å¾„
            import sys
            cosyvoice_path = self.config.get('cosyvoice_path', 'tools/CosyVoice')
            if cosyvoice_path not in sys.path:
                sys.path.append(cosyvoice_path)
            
            # å¯¼å…¥ CosyVoice
            from cosyvoice.cli.cosyvoice import CosyVoice2
            from cosyvoice.utils.file_utils import load_wav
            import torchaudio
            
            # æ¨¡å‹é…ç½®
            model_dir = self.config.get('model_dir', '/home/rh/Program/MCP_Tools/CosyVoice/pretrained_models/CosyVoice2-0.5B')
            
            # æ£€æŸ¥æ¨¡å‹ç›®å½•
            if not Path(model_dir).exists():
                raise FileNotFoundError(f"CosyVoice æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            
            # åˆå§‹åŒ–æ¨¡å‹
            self.model = CosyVoice2(
                model_dir=model_dir,
                load_jit=self.config.get('load_jit', False),
                load_trt=self.config.get('load_trt', False),
                fp16=self.config.get('fp16', False)
            )
            
            # ä¿å­˜å·¥å…·å‡½æ•°
            self.load_wav = load_wav
            self.torchaudio = torchaudio
            
            # å‚è€ƒéŸ³é¢‘è®¾ç½®
            self.reference_audio_path = self.config.get('reference_audio', None)
            self.reference_text = self.config.get('reference_text', 'å‚è€ƒéŸ³é¢‘æ–‡æœ¬')
            
            logger.info(f"âœ… CosyVoice æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ - æ¨¡å‹è·¯å¾„: {model_dir}")
            
        except ImportError as e:
            logger.warning(f"âš ï¸ CosyVoice æœªæ­£ç¡®å®‰è£…: {str(e)}")
            self.model = None
        except Exception as e:
            logger.error(f"âŒ CosyVoice åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def synthesize(self, 
                        text: str, 
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        synthesis_mode: str = "zero_shot",
                        **kwargs) -> Dict[str, Any]:
        """CosyVoice è¯­éŸ³åˆæˆ - æ”¯æŒå¤šç§åˆæˆæ¨¡å¼"""
        start_time = time.time()
        
        try:
            if self.model is None:
                # æ¨¡æ‹Ÿåˆæˆç»“æœ
                await asyncio.sleep(1.5)
                
                processing_time = time.time() - start_time
                
                # åˆ›å»ºæ¨¡æ‹ŸéŸ³é¢‘æ•°æ®ï¼ˆé™éŸ³ï¼‰
                sample_rate = 22050
                duration = max(len(text) / 10, 1.0)  # åŸºäºæ–‡æœ¬é•¿åº¦ä¼°ç®—æ—¶é•¿
                audio_samples = int(sample_rate * duration)
                mock_audio = b'\x00' * (audio_samples * 2)  # 16ä½PCM
                
                audio_base64 = base64.b64encode(mock_audio).decode('utf-8')
                
                return {
                    "audio_data": audio_base64,
                    "format": AudioFormat.WAV,
                    "duration": duration,
                    "processing_time": processing_time,
                    "model_used": "mock_cosyvoice",
                    "synthesis_mode": synthesis_mode
                }
            
            # æ ¹æ®åˆæˆæ¨¡å¼é€‰æ‹©ä¸åŒçš„æ–¹æ³•
            if synthesis_mode == "zero_shot":
                result = await self._zero_shot_synthesis(text, kwargs)
            elif synthesis_mode == "cross_lingual":
                result = await self._cross_lingual_synthesis(text, kwargs)
            elif synthesis_mode == "instruct":
                result = await self._instruct_synthesis(text, kwargs)
            else:
                # é»˜è®¤ä½¿ç”¨é›¶æ ·æœ¬åˆæˆ
                result = await self._zero_shot_synthesis(text, kwargs)
            
            processing_time = time.time() - start_time
            result["processing_time"] = processing_time
            result["synthesis_mode"] = synthesis_mode
            
            log_speech_operation(
                logger, "synthesis", "cosyvoice", 
                len(text), len(result["audio_data"]), processing_time, 
                True, language
            )
            
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_speech_operation(
                logger, "synthesis", "cosyvoice", 
                len(text), 0, processing_time, 
                False, language, error_msg
            )
            
            raise Exception(f"CosyVoice åˆæˆå¤±è´¥: {error_msg}")
    
    async def _zero_shot_synthesis(self, text: str, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """é›¶æ ·æœ¬è¯­éŸ³åˆæˆ"""
        # è·å–å‚è€ƒéŸ³é¢‘
        reference_audio_path = kwargs.get('reference_audio', self.reference_audio_path)
        reference_text = kwargs.get('reference_text', self.reference_text)
        
        if not reference_audio_path or not Path(reference_audio_path).exists():
            raise FileNotFoundError(f"å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {reference_audio_path}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢éŸ³é¢‘æ ¼å¼
        reference_audio_ext = Path(reference_audio_path).suffix.lower()
        if reference_audio_ext != '.wav':
            logger.info(f"å‚è€ƒéŸ³é¢‘éWAVæ ¼å¼ ({reference_audio_ext})ï¼Œè¿›è¡Œæ ¼å¼è½¬æ¢")
            try:
                reference_audio_path = convert_audio_to_wav(reference_audio_path, sample_rate=self.model.sample_rate)
            except Exception as e:
                logger.error(f"å‚è€ƒéŸ³é¢‘è½¬æ¢å¤±è´¥: {str(e)}")
                raise ValueError(f"å‚è€ƒéŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
        
        # åŠ è½½å‚è€ƒéŸ³é¢‘
        reference_audio = self.load_wav(reference_audio_path, self.model.sample_rate)
        
        # æ‰§è¡Œåˆæˆ
        output_audio = None
        stream = kwargs.get('stream', False)
        
        for i, result in enumerate(self.model.inference_zero_shot(
            text, reference_text, reference_audio, stream=stream
        )):
            output_audio = result['tts_speech']
            if not stream:  # éæµå¼æ¨¡å¼åªå–ç¬¬ä¸€ä¸ªç»“æœ
                break
        
        if output_audio is None:
            raise Exception("é›¶æ ·æœ¬åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
        
        return await self._process_output_audio(output_audio)
    
    async def _cross_lingual_synthesis(self, text: str, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """è·¨è¯­è¨€è¯­éŸ³åˆæˆ"""
        reference_audio_path = kwargs.get('reference_audio', self.reference_audio_path)
        
        if not reference_audio_path or not Path(reference_audio_path).exists():
            raise FileNotFoundError(f"å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {reference_audio_path}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢éŸ³é¢‘æ ¼å¼
        reference_audio_ext = Path(reference_audio_path).suffix.lower()
        if reference_audio_ext != '.wav':
            logger.info(f"å‚è€ƒéŸ³é¢‘éWAVæ ¼å¼ ({reference_audio_ext})ï¼Œè¿›è¡Œæ ¼å¼è½¬æ¢")
            try:
                reference_audio_path = convert_audio_to_wav(reference_audio_path, sample_rate=self.model.sample_rate)
            except Exception as e:
                logger.error(f"å‚è€ƒéŸ³é¢‘è½¬æ¢å¤±è´¥: {str(e)}")
                raise ValueError(f"å‚è€ƒéŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
        
        # åŠ è½½å‚è€ƒéŸ³é¢‘
        reference_audio = self.load_wav(reference_audio_path, self.model.sample_rate)
        
        # æ‰§è¡Œè·¨è¯­è¨€åˆæˆ
        output_audio = None
        stream = kwargs.get('stream', False)
        
        for i, result in enumerate(self.model.inference_cross_lingual(
            text, reference_audio, stream=stream
        )):
            output_audio = result['tts_speech']
            if not stream:
                break
        
        if output_audio is None:
            raise Exception("è·¨è¯­è¨€åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
        
        return await self._process_output_audio(output_audio)
    
    async def _instruct_synthesis(self, text: str, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """æŒ‡ä»¤å¼è¯­éŸ³åˆæˆ"""
        reference_audio_path = kwargs.get('reference_audio', self.reference_audio_path)
        instruction = kwargs.get('instruction', 'ç”¨æ¸©å’Œçš„è¯­è°ƒæœ—è¯»')
        
        if not reference_audio_path or not Path(reference_audio_path).exists():
            raise FileNotFoundError(f"å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {reference_audio_path}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢éŸ³é¢‘æ ¼å¼
        reference_audio_ext = Path(reference_audio_path).suffix.lower()
        if reference_audio_ext != '.wav':
            logger.info(f"å‚è€ƒéŸ³é¢‘éWAVæ ¼å¼ ({reference_audio_ext})ï¼Œè¿›è¡Œæ ¼å¼è½¬æ¢")
            try:
                reference_audio_path = convert_audio_to_wav(reference_audio_path, sample_rate=self.model.sample_rate)
            except Exception as e:
                logger.error(f"å‚è€ƒéŸ³é¢‘è½¬æ¢å¤±è´¥: {str(e)}")
                raise ValueError(f"å‚è€ƒéŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
        
        # åŠ è½½å‚è€ƒéŸ³é¢‘
        reference_audio = self.load_wav(reference_audio_path, self.model.sample_rate)
        
        # æ‰§è¡ŒæŒ‡ä»¤å¼åˆæˆ
        output_audio = None
        stream = kwargs.get('stream', False)
        
        for i, result in enumerate(self.model.inference_instruct(
            text, instruction, reference_audio, stream=stream
        )):
            output_audio = result['tts_speech']
            if not stream:
                break
        
        if output_audio is None:
            raise Exception("æŒ‡ä»¤å¼åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
        
        return await self._process_output_audio(output_audio)
    
    async def _process_output_audio(self, output_audio) -> Dict[str, Any]:
        """å¤„ç†è¾“å‡ºéŸ³é¢‘"""
        import io
        
        # è½¬æ¢ä¸ºå­—èŠ‚æ•°æ®
        buffer = io.BytesIO()
        
        try:
            # æ˜ç¡®æŒ‡å®šéŸ³é¢‘æ ¼å¼å’Œä½æ·±åº¦
            self.torchaudio.save(
                buffer, 
                output_audio, 
                self.model.sample_rate, 
                format='wav',
                bits_per_sample=16,
                encoding='PCM_S'
            )
            buffer.seek(0)
            audio_data = buffer.read()
            
            # ç¼–ç ä¸ºbase64
            audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            
            # è®¡ç®—æ—¶é•¿
            duration = output_audio.shape[1] / self.model.sample_rate
            
            return {
                "audio_data": audio_base64,
                "format": AudioFormat.WAV,
                "duration": duration,
                "model_used": "cosyvoice",
                "sample_rate": self.model.sample_rate
            }
        except Exception as e:
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            logger.error(f"âŒ éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")
            raise Exception(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")

class MockSynthesizer(SpeechSynthesizer):
    """æ¨¡æ‹Ÿè¯­éŸ³åˆæˆå™¨"""
    
    async def _setup(self):
        """è®¾ç½®æ¨¡æ‹Ÿåˆæˆå™¨"""
        logger.info("âœ… æ¨¡æ‹Ÿåˆæˆå™¨è®¾ç½®æˆåŠŸ")
    
    async def synthesize(self, 
                       text: str, 
                       voice: Optional[str] = None,
                       language: str = "zh-CN",
                       speed: float = 1.0,
                       pitch: float = 1.0,
                       **kwargs) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿè¯­éŸ³åˆæˆ"""
        start_time = time.time()
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„é™éŸ³éŸ³é¢‘
        await asyncio.sleep(0.2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        # ç”ŸæˆéŸ³é¢‘æ—¶é•¿
        sample_rate = 16000
        duration = min(len(text) / 5, 10)  # æœ€é•¿10ç§’
        duration = max(duration, 1.0)  # æœ€çŸ­1ç§’
        
        # åˆ›å»ºé™éŸ³éŸ³é¢‘
        samples = int(duration * sample_rate)
        audio_data = bytes(samples * 2)  # 16-bit PCM
        
        processing_time = time.time() - start_time
        
        return {
            "audio_data": base64.b64encode(audio_data).decode('utf-8'),
            "format": AudioFormat.PCM_16,
            "duration": duration,
            "processing_time": processing_time,
            "model_used": "mock_synthesizer"
        }

class SpeechProcessor:
    """è¯­éŸ³å¤„ç†å™¨ä¸»ç±» - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self):
        self.recognizers: Dict[str, SpeechRecognizer] = {}
        self.synthesizers: Dict[str, SpeechSynthesizer] = {}
        self.default_recognizer = None
        self.default_synthesizer = None
        self.is_initialized = False
        
        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è¯»å–é…ç½®
        self.config = self._load_config()

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        return {
            # CosyVoice é…ç½®
            'cosyvoice': {
                'model_dir': os.getenv('COSYVOICE_MODEL_DIR', 'pretrained_models/CosyVoice2-0.5B'),
                'cosyvoice_path': os.getenv('COSYVOICE_PATH', 'tools/CosyVoice'),
                'reference_audio': os.getenv('COSYVOICE_REF_AUDIO', None),
                'reference_text': os.getenv('COSYVOICE_REF_TEXT', 'å‚è€ƒéŸ³é¢‘æ–‡æœ¬'),
                'load_jit': os.getenv('COSYVOICE_LOAD_JIT', 'false').lower() == 'true',
                'load_trt': os.getenv('COSYVOICE_LOAD_TRT', 'false').lower() == 'true',
                'fp16': os.getenv('COSYVOICE_FP16', 'false').lower() == 'true'
            },
            # SenseVoice é…ç½®
            'sensevoice': {
                'model': os.getenv('SENSEVOICE_MODEL', 'iic/SenseVoiceSmall'),
                'max_single_segment_time': int(os.getenv('SENSEVOICE_MAX_SEGMENT_TIME', '60000')),
                'batch_size_s': int(os.getenv('SENSEVOICE_BATCH_SIZE_S', '300')),
                'batch_size_threshold_s': int(os.getenv('SENSEVOICE_BATCH_THRESHOLD_S', '60')),
                'ncpu': int(os.getenv('SENSEVOICE_NCPU', '4')),
                'batch_size': int(os.getenv('SENSEVOICE_BATCH_SIZE', '1'))
            },
            # é€šç”¨é…ç½®
            'device': os.getenv('SPEECH_DEVICE', 'cpu')
        }

    async def initialize(self):
        """åˆå§‹åŒ–è¯­éŸ³å¤„ç†å™¨"""
        logger.info("ğŸ”§ åˆå§‹åŒ–è¯­éŸ³å¤„ç†å™¨ - ä¼˜åŒ–ç‰ˆæœ¬")
        
        # å°è¯•åˆå§‹åŒ–å¯ç”¨çš„è¯†åˆ«å™¨
        await self._try_initialize_recognizers()
        
        # å°è¯•åˆå§‹åŒ–å¯ç”¨çš„åˆæˆå™¨
        await self._try_initialize_synthesizers()
        
        self.is_initialized = True
        
        available_recognizers = list(self.recognizers.keys())
        available_synthesizers = list(self.synthesizers.keys())
        
        logger.info(f"âœ… è¯­éŸ³å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - å¯ç”¨è¯†åˆ«å™¨: {available_recognizers}")
        logger.info(f"  - é»˜è®¤è¯†åˆ«å™¨: {self.default_recognizer}")
        logger.info(f"  - å¯ç”¨åˆæˆå™¨: {available_synthesizers}")
        logger.info(f"  - é»˜è®¤åˆæˆå™¨: {self.default_synthesizer}")
        
        if not available_recognizers and not available_synthesizers:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„è¯­éŸ³å¤„ç†å¼•æ“ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

    async def _try_initialize_recognizers(self):
        """å°è¯•åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨"""
        
        # ä¼˜å…ˆå°è¯• SenseVoice
        try:
            config = self.config['sensevoice'].copy()
            config['device'] = self.config['device']
            
            recognizer = SenseVoiceRecognizer(config)
            if await recognizer.initialize():
                self.recognizers['sensevoice'] = recognizer
                if self.default_recognizer is None:
                    self.default_recognizer = 'sensevoice'
                logger.info("âœ… SenseVoice è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ SenseVoice è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
        # å¦‚æœæ²¡æœ‰å¯ç”¨çš„è¯†åˆ«å™¨ï¼Œæ·»åŠ æ¨¡æ‹Ÿè¯†åˆ«å™¨
        if not self.recognizers:
            recognizer = SenseVoiceRecognizer({'device': 'cpu'})  # æ¨¡æ‹Ÿæ¨¡å¼
            await recognizer.initialize()
            self.recognizers['mock'] = recognizer
            self.default_recognizer = 'mock'
            logger.info("âœ… æ¨¡æ‹Ÿè¯†åˆ«å™¨å·²å¯ç”¨")

    async def _try_initialize_synthesizers(self):
        """å°è¯•åˆå§‹åŒ–è¯­éŸ³åˆæˆå™¨"""
        
        # åªåˆå§‹åŒ– CosyVoice
        try:
            config = self.config['cosyvoice'].copy()
            config['device'] = self.config['device']
            
            # æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
            if Path(config['model_dir']).exists():
                synthesizer = CosyVoiceSynthesizer(config)
                if await synthesizer.initialize():
                    self.synthesizers['cosyvoice'] = synthesizer
                    self.default_synthesizer = 'cosyvoice'
                    logger.info("âœ… CosyVoice åˆæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
                else:
                    logger.error("âŒ CosyVoice åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥")
                    raise RuntimeError("CosyVoice åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥")
            else:
                logger.error(f"âŒ CosyVoice æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {config['model_dir']}")
                raise FileNotFoundError(f"CosyVoice æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {config['model_dir']}")
        except Exception as e:
            logger.error(f"âŒ CosyVoice åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    async def recognize(self, 
                       audio_data: bytes,
                       language: str = "zh-CN",
                       model_name: Optional[str] = None,
                       request_id: Optional[str] = None,
                       **kwargs) -> SpeechRecognitionResponse:
        """è¯­éŸ³è¯†åˆ« - ä¼˜åŒ–ç‰ˆ"""
        if not self.is_initialized:
            raise RuntimeError("è¯­éŸ³å¤„ç†å™¨æœªåˆå§‹åŒ–")
        
        # é€‰æ‹©è¯†åˆ«å™¨
        recognizer_name = model_name or self.default_recognizer
        if recognizer_name not in self.recognizers:
            # å›é€€åˆ°é»˜è®¤è¯†åˆ«å™¨
            recognizer_name = self.default_recognizer
            logger.warning(f"âš ï¸ æŒ‡å®šçš„è¯†åˆ«å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è¯†åˆ«å™¨: {recognizer_name}")
        
        recognizer = self.recognizers[recognizer_name]
        
        try:
            logger.info(f"ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ« - æ¨¡å‹: {recognizer_name}, è¯­è¨€: {language}")
            
            result = await recognizer.recognize(audio_data, language, **kwargs)
            
            logger.info(f"âœ… è¯­éŸ³è¯†åˆ«å®Œæˆ - æ–‡æœ¬é•¿åº¦: {len(result['text'])}")
            
            return SpeechRecognitionResponse(
                success=True,
                text=result["text"],
                language=result["language"],
                confidence=result["confidence"],
                processing_time=result["processing_time"],
                model_used=result["model_used"],
                request_id=request_id or generate_response_id(),
                timestamp=datetime.utcnow(),
                segments=result.get("segments", [])
            )
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")
            # è¿”å›é”™è¯¯å“åº”è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return SpeechRecognitionResponse(
                success=False,
                text="",
                language=language,
                confidence=0.0,
                processing_time=0.0,
                model_used=recognizer_name,
                request_id=request_id or generate_response_id(),
                timestamp=datetime.utcnow(),
                message=f"è¯†åˆ«å¤±è´¥: {str(e)}"
            )

    async def synthesize(self, 
                        text: str,
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        tts_model: Optional[str] = None,
                        request_id: Optional[str] = None,
                        **kwargs) -> SpeechSynthesisResponse:
        """è¯­éŸ³åˆæˆ - ä¼˜åŒ–ç‰ˆ"""
        if not self.is_initialized:
            raise RuntimeError("è¯­éŸ³å¤„ç†å™¨æœªåˆå§‹åŒ–")
        
        # å¼ºåˆ¶ä½¿ç”¨CosyVoiceåˆæˆå™¨ï¼Œå¿½ç•¥å…¶ä»–è®¾ç½®
        synthesizer_name = 'cosyvoice'
        if synthesizer_name not in self.synthesizers:
            raise ValueError(f"CosyVoiceè¯­éŸ³åˆæˆå™¨ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿CosyVoice2-0.5bæ¨¡å‹å·²å®‰è£…")
        
        synthesizer = self.synthesizers[synthesizer_name]
        
        try:
            logger.info(f"ğŸ”Š å¼€å§‹è¯­éŸ³åˆæˆ - æ¨¡å‹: {synthesizer_name}, æ–‡æœ¬é•¿åº¦: {len(text)}")
            
            result = await synthesizer.synthesize(
                text=text,
                voice=voice,
                language=language,
                speed=speed,
                pitch=pitch,
                **kwargs
            )
            
            logger.info(f"âœ… è¯­éŸ³åˆæˆå®Œæˆ - éŸ³é¢‘æ—¶é•¿: {result['duration']:.2f}s")
            
            return SpeechSynthesisResponse(
                success=True,
                audio_data=result["audio_data"],
                format=result["format"],
                duration=result["duration"],
                processing_time=result["processing_time"],
                model_used=result["model_used"],
                request_id=request_id or generate_response_id(),
                timestamp=datetime.utcnow(),
                synthesis_mode=result.get("synthesis_mode", "default"),
                sample_rate=result.get("sample_rate", 16000)
            )
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³åˆæˆå¤±è´¥: {str(e)}")
            raise

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            return {
                "healthy": self.is_initialized and (len(self.recognizers) > 0 or len(self.synthesizers) > 0),
                "recognizers": {
                    "available": list(self.recognizers.keys()),
                    "default": self.default_recognizer,
                    "count": len(self.recognizers)
                },
                "synthesizers": {
                    "available": list(self.synthesizers.keys()),
                    "default": self.default_synthesizer,
                    "count": len(self.synthesizers)
                },
                "config": {
                    "device": self.config['device'],
                    "cosyvoice_model_dir": self.config['cosyvoice']['model_dir'],
                    "sensevoice_model": self.config['sensevoice']['model']
                }
            }
        
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }

    async def get_available_voices(self, synthesizer_name: Optional[str] = None) -> Dict[str, List[str]]:
        """è·å–å¯ç”¨çš„å£°éŸ³åˆ—è¡¨"""
        voices = {}
        
        target_synthesizers = [synthesizer_name] if synthesizer_name else self.synthesizers.keys()
        
        for name in target_synthesizers:
            if name in self.synthesizers:
                synthesizer = self.synthesizers[name]
                if hasattr(synthesizer, 'get_available_voices'):
                    voices[name] = await synthesizer.get_available_voices()
                else:
                    voices[name] = ["default"]
        
        return voices

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†è¯­éŸ³å¤„ç†å™¨èµ„æº")
        
        # æ¸…ç†è¯†åˆ«å™¨
        for recognizer in self.recognizers.values():
            if hasattr(recognizer, 'cleanup'):
                try:
                    await recognizer.cleanup()
                except Exception as e:
                    logger.warning(f"æ¸…ç†è¯†åˆ«å™¨æ—¶å‡ºé”™: {str(e)}")
        
        # æ¸…ç†åˆæˆå™¨
        for synthesizer in self.synthesizers.values():
            if hasattr(synthesizer, 'cleanup'):
                try:
                    await synthesizer.cleanup()
                except Exception as e:
                    logger.warning(f"æ¸…ç†åˆæˆå™¨æ—¶å‡ºé”™: {str(e)}")
        
        self.recognizers.clear()
        self.synthesizers.clear()
        self.default_recognizer = None
        self.default_synthesizer = None
        self.is_initialized = False
        
        logger.info("âœ… è¯­éŸ³å¤„ç†å™¨èµ„æºæ¸…ç†å®Œæˆ")

# å…¨å±€å®ä¾‹
speech_processor = SpeechProcessor()

# ä¾¿æ·å‡½æ•°
async def initialize_speech_processor():
    """åˆå§‹åŒ–å…¨å±€è¯­éŸ³å¤„ç†å™¨"""
    await speech_processor.initialize()

async def recognize_speech(audio_data: bytes, **kwargs) -> SpeechRecognitionResponse:
    """ä¾¿æ·çš„è¯­éŸ³è¯†åˆ«å‡½æ•°"""
    return await speech_processor.recognize(audio_data, **kwargs)

async def synthesize_speech(text: str, **kwargs) -> SpeechSynthesisResponse:
    """ä¾¿æ·çš„è¯­éŸ³åˆæˆå‡½æ•°"""
    return await speech_processor.synthesize(text, **kwargs)