"""
è¯­éŸ³å¤„ç†å™¨
è´Ÿè´£è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³åˆæˆåŠŸèƒ½
"""

import asyncio
import base64
import tempfile
import time
import uuid
from typing import Dict, Any, Optional
from datetime import datetime
from pathlib import Path
import json
import os

from .logger import get_logger, log_speech_operation
from .models import SpeechRecognitionResponse, SpeechSynthesisResponse, AudioFormat
from .utils import generate_response_id

logger = get_logger(__name__)

class SpeechRecognizer:
    """è¯­éŸ³è¯†åˆ«å™¨åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_initialized = False
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–è¯†åˆ«å™¨"""
        try:
            await self._setup()
            self.is_initialized = True
            return True
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    async def _setup(self):
        """è®¾ç½®è¯†åˆ«å™¨ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        pass
    
    async def recognize(self, 
                       audio_data: bytes, 
                       language: str = "zh-CN",
                       **kwargs) -> Dict[str, Any]:
        """è¯†åˆ«è¯­éŸ³ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        raise NotImplementedError

class SensVoiceRecognizer(SpeechRecognizer):
    """SensVoice è¯­éŸ³è¯†åˆ«å™¨"""
    
    async def _setup(self):
        """è®¾ç½® SensVoice"""
        try:
            # è¿™é‡Œé›†æˆæ‚¨ä¹‹å‰åˆ›å»ºçš„ SensVoice MCP å®¢æˆ·ç«¯
            # æˆ–è€…ç›´æ¥å¯¼å…¥ funasr
            from funasr import AutoModel
            
            # åˆå§‹åŒ–æ¨¡å‹
            self.model = AutoModel(
                model="paraformer-zh",
                vad_model="fsmn-vad",
                punc_model="ct-punc",
                device=self.config.get('device', 'cpu')
            )
            
            logger.info("âœ… SensVoice æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
        except ImportError:
            logger.warning("âš ï¸ FunASR æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè¯†åˆ«å™¨")
            self.model = None
        except Exception as e:
            logger.error(f"âŒ SensVoice åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def recognize(self, 
                       audio_data: bytes, 
                       language: str = "zh-CN",
                       **kwargs) -> Dict[str, Any]:
        """SensVoice è¯­éŸ³è¯†åˆ«"""
        start_time = time.time()
        
        try:
            if self.model is None:
                # æ¨¡æ‹Ÿè¯†åˆ«ç»“æœ
                await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                
                processing_time = time.time() - start_time
                
                return {
                    "text": "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„è¯­éŸ³è¯†åˆ«ç»“æœ",
                    "language": language,
                    "confidence": 0.95,
                    "processing_time": processing_time,
                    "model_used": "mock_sensvoice"
                }
            
            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_file.write(audio_data)
                temp_audio_path = temp_file.name
            
            try:
                # æ‰§è¡Œè¯†åˆ«
                result = self.model.generate(
                    input=temp_audio_path,
                    **kwargs
                )
                
                processing_time = time.time() - start_time
                
                # æå–è¯†åˆ«ç»“æœ
                if result and len(result) > 0:
                    text = result[0].get('text', '')
                    confidence = result[0].get('confidence', 0.0)
                else:
                    text = ""
                    confidence = 0.0
                
                log_speech_operation(
                    logger, "recognition", "sensvoice", 
                    len(audio_data), len(text), processing_time, 
                    True, language
                )
                
                return {
                    "text": text,
                    "language": language,
                    "confidence": confidence,
                    "processing_time": processing_time,
                    "model_used": "sensvoice"
                }
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                Path(temp_audio_path).unlink(missing_ok=True)
                
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_speech_operation(
                logger, "recognition", "sensvoice", 
                len(audio_data), 0, processing_time, 
                False, language, error_msg
            )
            
            raise Exception(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {error_msg}")

class WhisperRecognizer(SpeechRecognizer):
    """Whisper è¯­éŸ³è¯†åˆ«å™¨"""
    
    async def _setup(self):
        """è®¾ç½® Whisper"""
        try:
            import whisper
            
            model_size = self.config.get('model_size', 'base')
            self.model = whisper.load_model(model_size)
            
            logger.info(f"âœ… Whisper æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_size}")
            
        except ImportError:
            logger.warning("âš ï¸ Whisper æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè¯†åˆ«å™¨")
            self.model = None
        except Exception as e:
            logger.error(f"âŒ Whisper åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def recognize(self, 
                       audio_data: bytes, 
                       language: str = "zh",
                       **kwargs) -> Dict[str, Any]:
        """Whisper è¯­éŸ³è¯†åˆ«"""
        start_time = time.time()
        
        try:
            if self.model is None:
                # æ¨¡æ‹Ÿè¯†åˆ«ç»“æœ
                await asyncio.sleep(1.0)
                
                processing_time = time.time() - start_time
                
                return {
                    "text": "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„ Whisper è¯†åˆ«ç»“æœ",
                    "language": language,
                    "confidence": 0.90,
                    "processing_time": processing_time,
                    "model_used": "mock_whisper"
                }
            
            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_file.write(audio_data)
                temp_audio_path = temp_file.name
            
            try:
                # æ‰§è¡Œè¯†åˆ«
                result = self.model.transcribe(
                    temp_audio_path,
                    language=language if language != "zh-CN" else "zh",
                    **kwargs
                )
                
                processing_time = time.time() - start_time
                
                text = result.get('text', '')
                
                # Whisper ä¸ç›´æ¥æä¾›ç½®ä¿¡åº¦ï¼Œä½¿ç”¨å¹³å‡æ¦‚ç‡
                segments = result.get('segments', [])
                confidence = 0.0
                if segments:
                    confidences = [seg.get('avg_logprob', 0.0) for seg in segments]
                    confidence = sum(confidences) / len(confidences)
                    confidence = max(0.0, min(1.0, (confidence + 1.0) / 2.0))  # è½¬æ¢ä¸º0-1èŒƒå›´
                
                log_speech_operation(
                    logger, "recognition", "whisper", 
                    len(audio_data), len(text), processing_time, 
                    True, language
                )
                
                return {
                    "text": text,
                    "language": language,
                    "confidence": confidence,
                    "processing_time": processing_time,
                    "model_used": "whisper"
                }
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                Path(temp_audio_path).unlink(missing_ok=True)
                
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_speech_operation(
                logger, "recognition", "whisper", 
                len(audio_data), 0, processing_time, 
                False, language, error_msg
            )
            
            raise Exception(f"Whisper è¯†åˆ«å¤±è´¥: {error_msg}")

class SpeechSynthesizer:
    """è¯­éŸ³åˆæˆå™¨åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_initialized = False
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–åˆæˆå™¨"""
        try:
            await self._setup()
            self.is_initialized = True
            return True
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    async def _setup(self):
        """è®¾ç½®åˆæˆå™¨ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        pass
    
    async def synthesize(self, 
                        text: str, 
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        **kwargs) -> Dict[str, Any]:
        """åˆæˆè¯­éŸ³ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        raise NotImplementedError

class MockSynthesizer(SpeechSynthesizer):
    """æ¨¡æ‹Ÿè¯­éŸ³åˆæˆå™¨"""
    
    async def _setup(self):
        """è®¾ç½®æ¨¡æ‹Ÿåˆæˆå™¨"""
        logger.info("âœ… æ¨¡æ‹Ÿåˆæˆå™¨è®¾ç½®æˆåŠŸ")
    
    async def synthesize(self, 
                       text: str, 
                       voice: Optional[str] = None,
                       language: str = "zh-CN",
                       speed: float = 1.0,
                       pitch: float = 1.0,
                       **kwargs) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿè¯­éŸ³åˆæˆ"""
        start_time = time.time()
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„é™éŸ³éŸ³é¢‘
        await asyncio.sleep(0.2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        # ç”Ÿæˆ2ç§’çš„é™éŸ³éŸ³é¢‘
        sample_rate = 16000
        duration = min(len(text) / 5, 10)  # æœ€é•¿10ç§’
        duration = max(duration, 1.0)  # æœ€çŸ­1ç§’
        
        # åˆ›å»ºé™éŸ³éŸ³é¢‘
        samples = int(duration * sample_rate)
        audio_data = bytes(samples * 2)  # 16-bit PCM
        
        processing_time = time.time() - start_time
        
        return {
            "audio_data": audio_data,
            "format": AudioFormat.PCM_16,
            "duration": duration,
            "processing_time": processing_time,
            "model_used": "mock_synthesizer"
        }

class CosyVoiceSynthesizer(SpeechSynthesizer):
    """CosyVoice è¯­éŸ³åˆæˆå™¨"""
    
    async def _setup(self):
        """è®¾ç½® CosyVoice"""
        try:
            # è¿™é‡Œé›†æˆæ‚¨ä¹‹å‰åˆ›å»ºçš„ CosyVoice MCP å®¢æˆ·ç«¯
            # æˆ–è€…ç›´æ¥å¯¼å…¥ CosyVoice
            import sys
            sys.path.append('third_party/Matcha-TTS')
            
            from cosyvoice.cli.cosyvoice import CosyVoice2
            from cosyvoice.utils.file_utils import load_wav
            import torchaudio
            
            model_dir = self.config.get('model_dir', 'pretrained_models/CosyVoice2-0.5B')
            
            self.model = CosyVoice2(model_dir, load_jit=False, load_trt=False, fp16=False)
            self.load_wav = load_wav
            self.torchaudio = torchaudio
            
            logger.info("âœ… CosyVoice æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
        except ImportError:
            logger.warning("âš ï¸ CosyVoice æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿåˆæˆå™¨")
            self.model = None
        except Exception as e:
            logger.error(f"âŒ CosyVoice åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def synthesize(self, 
                        text: str, 
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        **kwargs) -> Dict[str, Any]:
        """CosyVoice è¯­éŸ³åˆæˆ"""
        start_time = time.time()
        
        try:
            if self.model is None:
                # æ¨¡æ‹Ÿåˆæˆç»“æœ
                await asyncio.sleep(1.5)
                
                processing_time = time.time() - start_time
                
                # åˆ›å»ºæ¨¡æ‹ŸéŸ³é¢‘æ•°æ®ï¼ˆé™éŸ³ï¼‰
                sample_rate = 22050
                duration = 2.0  # 2ç§’
                audio_samples = int(sample_rate * duration)
                mock_audio = b'\x00' * (audio_samples * 2)  # 16ä½PCM
                
                audio_base64 = base64.b64encode(mock_audio).decode('utf-8')
                
                return {
                    "audio_data": audio_base64,
                    "format": AudioFormat.WAV,
                    "duration": duration,
                    "processing_time": processing_time,
                    "model_used": "mock_cosyvoice"
                }
            
            # ä½¿ç”¨é¢„è®¾çš„å‚è€ƒéŸ³é¢‘ï¼ˆéœ€è¦æä¾›ï¼‰
            reference_audio_path = self.config.get('reference_audio', 'reference.wav')
            reference_text = self.config.get('reference_text', 'å‚è€ƒéŸ³é¢‘æ–‡æœ¬')
            
            if not Path(reference_audio_path).exists():
                # å¦‚æœæ²¡æœ‰å‚è€ƒéŸ³é¢‘ï¼Œä½¿ç”¨ SFT æ¨¡å¼ï¼ˆå¦‚æœæ”¯æŒï¼‰
                logger.warning("âš ï¸ æœªæ‰¾åˆ°å‚è€ƒéŸ³é¢‘ï¼Œä½¿ç”¨é»˜è®¤åˆæˆæ¨¡å¼")
                
                # æ¨¡æ‹Ÿè¾“å‡º
                processing_time = time.time() - start_time
                mock_audio = b'\x00' * (22050 * 2 * 2)  # 2ç§’é™éŸ³
                audio_base64 = base64.b64encode(mock_audio).decode('utf-8')
                
                return {
                    "audio_data": audio_base64,
                    "format": AudioFormat.WAV,
                    "duration": 2.0,
                    "processing_time": processing_time,
                    "model_used": "cosyvoice_sft"
                }
            
            # åŠ è½½å‚è€ƒéŸ³é¢‘
            reference_audio = self.load_wav(reference_audio_path, 16000)
            
            # æ‰§è¡Œåˆæˆ
            output_audio = None
            for i, result in enumerate(self.model.inference_zero_shot(
                text, reference_text, reference_audio, stream=False
            )):
                output_audio = result['tts_speech']
                break  # åªå–ç¬¬ä¸€ä¸ªç»“æœ
            
            if output_audio is None:
                raise Exception("åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
            
            processing_time = time.time() - start_time
            
            # è½¬æ¢ä¸ºå­—èŠ‚æ•°æ®
            import io
            buffer = io.BytesIO()
            self.torchaudio.save(buffer, output_audio, self.model.sample_rate, format='wav')
            buffer.seek(0)
            audio_data = buffer.read()
            
            # ç¼–ç ä¸ºbase64
            audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            
            # è®¡ç®—æ—¶é•¿
            duration = output_audio.shape[1] / self.model.sample_rate
            
            log_speech_operation(
                logger, "synthesis", "cosyvoice", 
                len(text), len(audio_data), processing_time, 
                True, language
            )
            
            return {
                "audio_data": audio_base64,
                "format": AudioFormat.WAV,
                "duration": duration,
                "processing_time": processing_time,
                "model_used": "cosyvoice"
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_speech_operation(
                logger, "synthesis", "cosyvoice", 
                len(text), 0, processing_time, 
                False, language, error_msg
            )
            
            raise Exception(f"CosyVoice åˆæˆå¤±è´¥: {error_msg}")

class EdgeTTSSynthesizer(SpeechSynthesizer):
    """Edge TTS è¯­éŸ³åˆæˆå™¨"""
    
    async def _setup(self):
        """è®¾ç½® Edge TTS"""
        try:
            import edge_tts
            self.edge_tts = edge_tts
            
            logger.info("âœ… Edge TTS åˆå§‹åŒ–æˆåŠŸ")
            
        except ImportError:
            logger.warning("âš ï¸ Edge TTS æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿåˆæˆå™¨")
            self.edge_tts = None
        except Exception as e:
            logger.error(f"âŒ Edge TTS åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def synthesize(self, 
                        text: str, 
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        **kwargs) -> Dict[str, Any]:
        """Edge TTS è¯­éŸ³åˆæˆ"""
        start_time = time.time()
        
        try:
            if self.edge_tts is None:
                # æ¨¡æ‹Ÿåˆæˆç»“æœ
                await asyncio.sleep(0.8)
                
                processing_time = time.time() - start_time
                
                # åˆ›å»ºæ¨¡æ‹ŸéŸ³é¢‘æ•°æ®
                mock_audio = b'\x00' * (16000 * 2 * 2)  # 2ç§’ï¼Œ16kHzï¼Œ16ä½
                audio_base64 = base64.b64encode(mock_audio).decode('utf-8')
                
                return {
                    "audio_data": audio_base64,
                    "format": AudioFormat.WAV,
                    "duration": 2.0,
                    "processing_time": processing_time,
                    "model_used": "mock_edge_tts"
                }
            
            # é€‰æ‹©å£°éŸ³
            if not voice:
                voice = self._get_default_voice(language)
            
            # åˆ›å»ºTTSå®ä¾‹
            tts = self.edge_tts.Communicate(text, voice)
            
            # ç”ŸæˆéŸ³é¢‘
            audio_data = b""
            async for chunk in tts.stream():
                if chunk["type"] == "audio":
                    audio_data += chunk["data"]
            
            processing_time = time.time() - start_time
            
            # ç¼–ç ä¸ºbase64
            audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            
            # ä¼°ç®—æ—¶é•¿ï¼ˆç®€å•ä¼°ç®—ï¼Œå®é™…å¯èƒ½éœ€è¦è§£æéŸ³é¢‘æ–‡ä»¶ï¼‰
            duration = len(text) * 0.1  # ç®€å•ä¼°ç®—ï¼šæ¯ä¸ªå­—ç¬¦0.1ç§’
            
            log_speech_operation(
                logger, "synthesis", "edge_tts", 
                len(text), len(audio_data), processing_time, 
                True, language
            )
            
            return {
                "audio_data": audio_base64,
                "format": AudioFormat.MP3,  # Edge TTS é»˜è®¤è¾“å‡ºMP3
                "duration": duration,
                "processing_time": processing_time,
                "model_used": "edge_tts"
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_speech_operation(
                logger, "synthesis", "edge_tts", 
                len(text), 0, processing_time, 
                False, language, error_msg
            )
            
            raise Exception(f"Edge TTS åˆæˆå¤±è´¥: {error_msg}")
    
    def _get_default_voice(self, language: str) -> str:
        """è·å–é»˜è®¤å£°éŸ³"""
        voice_map = {
            "zh-CN": "zh-CN-XiaoxiaoNeural",
            "zh-TW": "zh-TW-HsiaoyuNeural", 
            "en-US": "en-US-AriaNeural",
            "ja-JP": "ja-JP-NanamiNeural",
            "ko-KR": "ko-KR-SunHiNeural"
        }
        return voice_map.get(language, "zh-CN-XiaoxiaoNeural")

class SpeechProcessor:
    """è¯­éŸ³å¤„ç†å™¨ä¸»ç±»"""
    
    def __init__(self):
        self.recognizers: Dict[str, Any] = {}
        self.synthesizers: Dict[str, Any] = {}
        self.default_recognizer = None
        self.default_synthesizer = None
        self.is_initialized = False
        
        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è¯»å–æ¨¡å‹è·¯å¾„
        self.cosyvoice_model_dir = os.getenv(
            'COSYVOICE_MODEL_DIR', 
            'pretrained_models/CosyVoice2-0.5B'
        )
        self.whisper_model_size = os.getenv('WHISPER_MODEL_SIZE', 'base')
        self.device = os.getenv('SPEECH_DEVICE', 'cpu')

    async def initialize(self):
        """åˆå§‹åŒ–è¯­éŸ³å¤„ç†å™¨"""
        logger.info("ğŸ”§ åˆå§‹åŒ–è¯­éŸ³å¤„ç†å™¨")
        
        # å°è¯•åˆå§‹åŒ–å¯ç”¨çš„è¯†åˆ«å™¨
        await self._try_initialize_recognizers()
        
        # å°è¯•åˆå§‹åŒ–å¯ç”¨çš„åˆæˆå™¨
        await self._try_initialize_synthesizers()
        
        self.is_initialized = True
        
        available_recognizers = list(self.recognizers.keys())
        available_synthesizers = list(self.synthesizers.keys())
        
        logger.info(f"âœ… è¯­éŸ³å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - å¯ç”¨è¯†åˆ«å™¨: {available_recognizers}")
        logger.info(f"  - å¯ç”¨åˆæˆå™¨: {available_synthesizers}")
        
        if not available_recognizers and not available_synthesizers:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„è¯­éŸ³å¤„ç†å¼•æ“ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    

    async def _try_initialize_recognizers(self):
        """å°è¯•åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨"""
        
        # å°è¯• SensVoice/FunASR
        try:
            from funasr import AutoModel
            
            model = AutoModel(
                model="paraformer-zh",
                vad_model="fsmn-vad",
                punc_model="ct-punc",
                device=self.device
            )
            
            self.recognizers['sensvoice'] = {
                'model': model,
                'type': 'sensvoice'
            }
            
            if self.default_recognizer is None:
                self.default_recognizer = 'sensvoice'
            
            logger.info("âœ… SensVoice è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except ImportError:
            logger.info("â„¹ï¸ FunASR æœªå®‰è£…ï¼Œè·³è¿‡ SensVoice è¯†åˆ«å™¨")
        except Exception as e:
            logger.warning(f"âš ï¸ SensVoice è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        # å°è¯• Whisper
        try:
            import whisper
            
            model = whisper.load_model(self.whisper_model_size)
            
            self.recognizers['whisper'] = {
                'model': model,
                'type': 'whisper'
            }
            
            if self.default_recognizer is None:
                self.default_recognizer = 'whisper'
            
            logger.info(f"âœ… Whisper è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {self.whisper_model_size})")
            
        except ImportError:
            logger.info("â„¹ï¸ Whisper æœªå®‰è£…ï¼Œè·³è¿‡ Whisper è¯†åˆ«å™¨")
        except Exception as e:
            logger.warning(f"âš ï¸ Whisper è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        # å¦‚æœæ²¡æœ‰å¯ç”¨çš„è¯†åˆ«å™¨ï¼Œæ·»åŠ æ¨¡æ‹Ÿè¯†åˆ«å™¨
        if not self.recognizers:
            self.recognizers['mock'] = {
                'model': None,
                'type': 'mock'
            }
            self.default_recognizer = 'mock'
            logger.info("âœ… æ¨¡æ‹Ÿè¯†åˆ«å™¨å·²å¯ç”¨")
    

    async def _try_initialize_synthesizers(self):
        """å°è¯•åˆå§‹åŒ–è¯­éŸ³åˆæˆå™¨"""
        
        # å°è¯• CosyVoice
        try:
            # æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
            if Path(self.cosyvoice_model_dir).exists():
                import sys
                sys.path.append('third_party/Matcha-TTS')
                
                from cosyvoice.cli.cosyvoice import CosyVoice2
                from cosyvoice.utils.file_utils import load_wav
                import torchaudio
                
                model = CosyVoice2(self.cosyvoice_model_dir, load_jit=False, load_trt=False, fp16=False)
                
                self.synthesizers['cosyvoice'] = CosyVoiceSynthesizer({'model_dir': self.cosyvoice_model_dir})
                await self.synthesizers['cosyvoice'].initialize()
                
                if self.default_synthesizer is None:
                    self.default_synthesizer = 'cosyvoice'
                
                logger.info("âœ… CosyVoice åˆæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.info(f"â„¹ï¸ CosyVoice æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {self.cosyvoice_model_dir}")
                
        except ImportError:
            logger.info("â„¹ï¸ CosyVoice ä¾èµ–æœªå®‰è£…ï¼Œè·³è¿‡ CosyVoice åˆæˆå™¨")
        except Exception as e:
            logger.warning(f"âš ï¸ CosyVoice åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        # å°è¯• Edge TTS
        try:
            import edge_tts
            
            self.synthesizers['edge_tts'] = EdgeTTSSynthesizer({})
            await self.synthesizers['edge_tts'].initialize()
            
            if self.default_synthesizer is None:
                self.default_synthesizer = 'edge_tts'
            
            logger.info("âœ… Edge TTS åˆæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except ImportError:
            logger.info("â„¹ï¸ Edge TTS æœªå®‰è£…ï¼Œè·³è¿‡ Edge TTS åˆæˆå™¨")
        except Exception as e:
            logger.warning(f"âš ï¸ Edge TTS åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        # å¦‚æœæ²¡æœ‰å¯ç”¨çš„åˆæˆå™¨ï¼Œæ·»åŠ æ¨¡æ‹Ÿåˆæˆå™¨
        if not self.synthesizers:
            self.synthesizers['mock'] = MockSynthesizer({})
            await self.synthesizers['mock'].initialize()
            self.default_synthesizer = 'mock'
            logger.info("âœ… æ¨¡æ‹Ÿåˆæˆå™¨å·²å¯ç”¨")
    

    async def recognize(self, 
                       audio_data: bytes,
                       language: str = "zh-CN",
                       model_name: Optional[str] = None,
                       request_id: Optional[str] = None) -> SpeechRecognitionResponse:
        """è¯­éŸ³è¯†åˆ« - æ”¹è¿›ç‰ˆ"""
        if not self.is_initialized:
            raise RuntimeError("è¯­éŸ³å¤„ç†å™¨æœªåˆå§‹åŒ–")
        
        # é€‰æ‹©è¯†åˆ«å™¨
        recognizer_name = model_name or self.default_recognizer
        if recognizer_name not in self.recognizers:
            # å›é€€åˆ°é»˜è®¤è¯†åˆ«å™¨
            recognizer_name = self.default_recognizer
            logger.warning(f"âš ï¸ æŒ‡å®šçš„è¯†åˆ«å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è¯†åˆ«å™¨: {recognizer_name}")
        
        recognizer = self.recognizers[recognizer_name]
        
        try:
            logger.info(f"ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ« - æ¨¡å‹: {recognizer_name}, è¯­è¨€: {language}")
            
            result = await self._perform_recognition(recognizer, audio_data, language)
            
            logger.info(f"âœ… è¯­éŸ³è¯†åˆ«å®Œæˆ - æ–‡æœ¬é•¿åº¦: {len(result['text'])}")
            
            return SpeechRecognitionResponse(
                success=True,
                text=result["text"],
                language=result["language"],
                confidence=result["confidence"],
                processing_time=result["processing_time"],
                model_used=result["model_used"],
                request_id=request_id or generate_response_id(),
                timestamp=datetime.utcnow()
            )
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")
            # è¿”å›é”™è¯¯å“åº”è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return SpeechRecognitionResponse(
                success=False,
                text="",
                language=language,
                confidence=0.0,
                processing_time=0.0,
                model_used=recognizer_name,
                request_id=request_id or generate_response_id(),
                timestamp=datetime.utcnow(),
                message=f"è¯†åˆ«å¤±è´¥: {str(e)}"
            )
    
    async def _perform_recognition(self, recognizer: Dict[str, Any], audio_data: bytes, language: str) -> Dict[str, Any]:
        """æ‰§è¡Œå…·ä½“çš„è¯­éŸ³è¯†åˆ«"""
        start_time = time.time()
        
        if recognizer['type'] == 'mock':
            # æ¨¡æ‹Ÿè¯†åˆ«
            await asyncio.sleep(0.5)
            processing_time = time.time() - start_time
            
            return {
                "text": "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„è¯­éŸ³è¯†åˆ«ç»“æœ",
                "language": language,
                "confidence": 0.95,
                "processing_time": processing_time,
                "model_used": "mock_recognizer"
            }
        
        elif recognizer['type'] == 'sensvoice':
            return await self._sensvoice_recognize(recognizer, audio_data, language, start_time)
        
        elif recognizer['type'] == 'whisper':
            return await self._whisper_recognize(recognizer, audio_data, language, start_time)
        
        else:
            raise ValueError(f"æœªçŸ¥çš„è¯†åˆ«å™¨ç±»å‹: {recognizer['type']}")
    
    async def _sensvoice_recognize(self, recognizer: Dict[str, Any], audio_data: bytes, language: str, start_time: float) -> Dict[str, Any]:
        """SensVoice è¯†åˆ«å®ç°"""
        model = recognizer['model']
        
        # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
            temp_file.write(audio_data)
            temp_audio_path = temp_file.name
        
        try:
            # æ‰§è¡Œè¯†åˆ«
            result = model.generate(input=temp_audio_path)
            
            processing_time = time.time() - start_time
            
            # æå–è¯†åˆ«ç»“æœ
            if result and len(result) > 0:
                text = result[0].get('text', '')
                confidence = result[0].get('confidence', 0.0)
            else:
                text = ""
                confidence = 0.0
            
            log_speech_operation(
                logger, "recognition", "sensvoice", 
                len(audio_data), len(text), processing_time, 
                True, language
            )
            
            return {
                "text": text,
                "language": language,
                "confidence": confidence,
                "processing_time": processing_time,
                "model_used": "sensvoice"
            }
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            Path(temp_audio_path).unlink(missing_ok=True)
    
    async def _whisper_recognize(self, recognizer: Dict[str, Any], audio_data: bytes, language: str, start_time: float) -> Dict[str, Any]:
        """Whisper è¯†åˆ«å®ç°"""
        model = recognizer['model']
        
        # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
            temp_file.write(audio_data)
            temp_audio_path = temp_file.name
        
        try:
            # æ‰§è¡Œè¯†åˆ«
            result = model.transcribe(
                temp_audio_path,
                language=language if language != "zh-CN" else "zh"
            )
            
            processing_time = time.time() - start_time
            
            text = result.get('text', '')
            
            # Whisper ä¸ç›´æ¥æä¾›ç½®ä¿¡åº¦ï¼Œä½¿ç”¨å¹³å‡æ¦‚ç‡ä¼°ç®—
            segments = result.get('segments', [])
            confidence = 0.0
            if segments:
                confidences = [seg.get('avg_logprob', 0.0) for seg in segments]
                confidence = sum(confidences) / len(confidences)
                confidence = max(0.0, min(1.0, (confidence + 1.0) / 2.0))
            
            log_speech_operation(
                logger, "recognition", "whisper", 
                len(audio_data), len(text), processing_time, 
                True, language
            )
            
            return {
                "text": text,
                "language": language,
                "confidence": confidence,
                "processing_time": processing_time,
                "model_used": "whisper"
            }
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            Path(temp_audio_path).unlink(missing_ok=True)
    
    async def synthesize(self, 
                        text: str,
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        tts_model: Optional[str] = None,
                        request_id: Optional[str] = None) -> SpeechSynthesisResponse:
        """è¯­éŸ³åˆæˆ"""
        if not self.is_initialized:
            raise RuntimeError("è¯­éŸ³å¤„ç†å™¨æœªåˆå§‹åŒ–")
        
        # é€‰æ‹©åˆæˆå™¨
        synthesizer_name = tts_model or self.default_synthesizer
        if synthesizer_name not in self.synthesizers:
            raise ValueError(f"è¯­éŸ³åˆæˆå™¨ä¸å¯ç”¨: {synthesizer_name}")
        
        synthesizer = self.synthesizers[synthesizer_name]
        
        try:
            logger.info(f"ğŸ”Š å¼€å§‹è¯­éŸ³åˆæˆ - æ¨¡å‹: {synthesizer_name}, æ–‡æœ¬é•¿åº¦: {len(text)}")
            
            # å¦‚æœsynthesizeræ˜¯SpeechSynthesizerçš„å®ä¾‹ï¼Œç›´æ¥è°ƒç”¨å…¶synthesizeæ–¹æ³•
            if isinstance(synthesizer, SpeechSynthesizer):
                result = await synthesizer.synthesize(
                    text=text,
                    voice=voice,
                    language=language,
                    speed=speed,
                    pitch=pitch
                )
            # å¦åˆ™ï¼ŒæŒ‰ç…§åŸæ¥çš„é€»è¾‘å¤„ç†å­—å…¸ç±»å‹çš„synthesizer
            else:
                if synthesizer['type'] == 'cosyvoice':
                    result = await self._cosyvoice_synthesize(synthesizer, text, voice, language, speed, pitch)
                elif synthesizer['type'] == 'edge_tts':
                    result = await self._edge_tts_synthesize(synthesizer, text, voice, language, speed, pitch)
                elif synthesizer['type'] == 'mock':
                    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„MockSynthesizeræ¥å¤„ç†
                    mock_synth = MockSynthesizer({})
                    await mock_synth.initialize()
                    result = await mock_synth.synthesize(
                        text=text, 
                        voice=voice, 
                        language=language,
                        speed=speed,
                        pitch=pitch
                    )
                else:
                    raise ValueError(f"æœªçŸ¥çš„åˆæˆå™¨ç±»å‹: {synthesizer['type']}")
            
            logger.info(f"âœ… è¯­éŸ³åˆæˆå®Œæˆ - éŸ³é¢‘æ—¶é•¿: {result['duration']:.2f}s")
            
            return SpeechSynthesisResponse(
                success=True,
                audio_data=result["audio_data"],
                format=result["format"],
                duration=result["duration"],
                processing_time=result["processing_time"],
                model_used=result["model_used"],
                request_id=request_id or generate_response_id(),
                timestamp=datetime.utcnow()
            )
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³åˆæˆå¤±è´¥: {str(e)}")
            raise
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            return {
                "healthy": self.is_initialized and (len(self.recognizers) > 0 or len(self.synthesizers) > 0),
                "recognizers": {
                    "available": list(self.recognizers.keys()),
                    "default": self.default_recognizer
                },
                "synthesizers": {
                    "available": list(self.synthesizers.keys()),
                    "default": self.default_synthesizer
                }
            }
        
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†è¯­éŸ³å¤„ç†å™¨èµ„æº")
        
        self.recognizers.clear()
        self.synthesizers.clear()
        self.default_recognizer = None
        self.default_synthesizer = None
        self.is_initialized = False
        
        logger.info("âœ… è¯­éŸ³å¤„ç†å™¨èµ„æºæ¸…ç†å®Œæˆ")