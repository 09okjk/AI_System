"""
è¯­éŸ³å¤„ç†å™¨ - ä¿®å¤éŸ³é¢‘è¾“å‡ºé—®é¢˜
ä¸»è¦ä¿®å¤CosyVoiceéŸ³é¢‘è¾“å‡ºå¼‚å¸¸çš„é—®é¢˜
"""

import asyncio
import base64
import tempfile
import time
import uuid
from typing import Dict, Any, Optional, List, Union
from datetime import datetime
from pathlib import Path
import json
import os
import numpy as np
import torch

from .logger import get_logger, log_speech_operation
from .models import SpeechRecognitionResponse, SpeechSynthesisResponse, AudioFormat
from .utils import generate_response_id

logger = get_logger(__name__)

# æ·»åŠ éŸ³é¢‘æ ¼å¼è½¬æ¢å‡½æ•°
def convert_audio_to_wav(input_path: str, output_path: Optional[str] = None, sample_rate: int = 16000) -> str:
    """
    å°†ä»»æ„éŸ³é¢‘æ ¼å¼è½¬æ¢ä¸ºWAVæ ¼å¼
    
    Args:
        input_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºWAVæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ç”Ÿæˆä¸€ä¸ªä¸´æ—¶æ–‡ä»¶
        sample_rate: è¾“å‡ºéŸ³é¢‘çš„é‡‡æ ·ç‡
        
    Returns:
        WAVæ–‡ä»¶çš„è·¯å¾„
    """
    try:
        import librosa
        import soundfile as sf
        
        # å¦‚æœæœªæŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œåˆ›å»ºä¸´æ—¶æ–‡ä»¶
        if not output_path:
            temp_dir = Path(tempfile.gettempdir()) / "ai_system_audio"
            temp_dir.mkdir(exist_ok=True)
            output_path = str(temp_dir / f"{uuid.uuid4()}.wav")
        
        # åŠ è½½éŸ³é¢‘æ–‡ä»¶
        logger.info(f"æ­£åœ¨è½¬æ¢éŸ³é¢‘: {input_path} -> {output_path}")
        y, sr = librosa.load(input_path, sr=sample_rate)
        
        # ä¿å­˜ä¸ºWAVæ ¼å¼
        sf.write(output_path, y, sample_rate, subtype='PCM_16')
        logger.info(f"éŸ³é¢‘è½¬æ¢å®Œæˆ: {output_path}")
        
        return output_path
    except Exception as e:
        logger.error(f"éŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")
        raise

class SpeechRecognizer:
    """è¯­éŸ³è¯†åˆ«å™¨åŸºç±»"""    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_initialized = False
        self.model = None
        self.device = config.get('device', 'cpu')

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–è¯†åˆ«å™¨"""
        try:
            await self._setup()
            self.is_initialized = True
            logger.info(f"âœ… {self.__class__.__name__} åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ {self.__class__.__name__} åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False

    async def _setup(self):
        """è®¾ç½®è¯†åˆ«å™¨ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        pass

    async def recognize(self, 
                        audio_data: bytes, 
                        language: str = "zh-CN",
                        **kwargs) -> Dict[str, Any]:
        """è¯†åˆ«è¯­éŸ³ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        raise NotImplementedError

class SenseVoiceRecognizer(SpeechRecognizer):
    """SenseVoice è¯­éŸ³è¯†åˆ«å™¨ - åŸºäºå®˜æ–¹å®ç°ä¼˜åŒ–"""
    
    async def _setup(self):
        """è®¾ç½® SenseVoice - æ ¹æ®å®˜æ–¹æœ€ä½³å®è·µ"""
        try:
            from funasr import AutoModel
            
            # æ ¹æ®å®˜æ–¹æ¨èçš„é…ç½®
            model_config = {
                "model": "iic/SenseVoiceSmall",  # ä½¿ç”¨å®˜æ–¹æ¨èçš„æ¨¡å‹
                "vad_model": "fsmn-vad",
                "punc_model": "ct-punc", 
                "device": self.device,
                "hub": "ms",  # ä½¿ç”¨ modelscope
                "ncpu": self.config.get('ncpu', 4),
                "batch_size": self.config.get('batch_size', 1)
            }
            
            # æ”¯æŒ VAD çš„é…ç½®
            vad_kwargs = {
                "max_single_segment_time": self.config.get('max_single_segment_time', 60000),  # 60ç§’
                "batch_size_s": self.config.get('batch_size_s', 300),  # åŠ¨æ€batchï¼Œæ€»éŸ³é¢‘æ—¶é•¿300ç§’
                "batch_size_threshold_s": self.config.get('batch_size_threshold_s', 60)  # é˜ˆå€¼60ç§’
            }
            
            # åˆå§‹åŒ–æ¨¡å‹
            self.model = AutoModel(
                model=model_config["model"],
                vad_model=model_config["vad_model"],
                vad_kwargs=vad_kwargs,
                punc_model=model_config["punc_model"],
                device=model_config["device"],
                hub=model_config["hub"],
                ncpu=model_config["ncpu"],
                batch_size=model_config["batch_size"]
            )
            
            # å­˜å‚¨æ¨¡å‹é…ç½®ç”¨äºåç»­ä½¿ç”¨
            self.model_config = model_config
            self.vad_kwargs = vad_kwargs
            
            logger.info(f"âœ… SenseVoice æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ - è®¾å¤‡: {self.device}")
            
        except ImportError:
            logger.warning("âš ï¸ FunASR æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè¯†åˆ«å™¨")
            self.model = None
        except Exception as e:
            logger.error(f"âŒ SenseVoice åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def recognize(self, 
                       audio_data: bytes, 
                       language: str = "zh-CN",
                       **kwargs) -> Dict[str, Any]:
        """SenseVoice è¯­éŸ³è¯†åˆ« - ä¼˜åŒ–ç‰ˆæœ¬"""
        start_time = time.time()
        
        try:
            if self.model is None:
                # æ¨¡æ‹Ÿè¯†åˆ«ç»“æœ
                await asyncio.sleep(0.5)
                
                processing_time = time.time() - start_time
                
                return {
                    "text": "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„è¯­éŸ³è¯†åˆ«ç»“æœ",
                    "language": language,
                    "confidence": 0.95,
                    "processing_time": processing_time,
                    "model_used": "mock_sensevoice",
                    "segments": []
                }
            
            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_file.write(audio_data)
                temp_audio_path = temp_file.name
            
            try:
                # ä½¿ç”¨å®˜æ–¹æ¨èçš„å‚æ•°æ‰§è¡Œè¯†åˆ«
                generation_kwargs = {
                    "hotword": kwargs.get('hotword', ''),  # çƒ­è¯
                    "batch_size_s": self.vad_kwargs.get('batch_size_s', 300),
                    "batch_size_threshold_s": self.vad_kwargs.get('batch_size_threshold_s', 60)
                }
                
                # æ·»åŠ è‡ªå®šä¹‰å‚æ•°
                generation_kwargs.update(kwargs)
                
                # æ‰§è¡Œè¯†åˆ«
                result = self.model.generate(
                    input=temp_audio_path,
                    **generation_kwargs
                )
                
                processing_time = time.time() - start_time
                
                # å¤„ç†è¯†åˆ«ç»“æœ
                if result and len(result) > 0:
                    # SenseVoice è¿”å›æ ¼å¼é€šå¸¸æ˜¯åˆ—è¡¨
                    first_result = result[0] if isinstance(result, list) else result
                    
                    # æå–æ–‡æœ¬
                    text = first_result.get('text', '') if isinstance(first_result, dict) else str(first_result)

                    # æå–ç½®ä¿¡åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    confidence = first_result.get('confidence', 0.9) if isinstance(first_result, dict) else 0.9
                    
                    # æå–æ—¶é—´æˆ³ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    segments = first_result.get('timestamps', []) if isinstance(first_result, dict) else []
                    
                else:
                    text = ""
                    confidence = 0.0
                    segments = []
                
                log_speech_operation(
                    logger, "recognition", "sensevoice", 
                    len(audio_data), len(text), processing_time, 
                    True, language
                )
                
                return {
                    "text": text,
                    "language": language,
                    "confidence": confidence,
                    "processing_time": processing_time,
                    "model_used": "sensevoice",
                    "segments": segments,
                    "raw_result": result  # ä¿ç•™åŸå§‹ç»“æœä»¥ä¾¿è°ƒè¯•
                }
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                Path(temp_audio_path).unlink(missing_ok=True)
                
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_speech_operation(
                logger, "recognition", "sensevoice", 
                len(audio_data), 0, processing_time, 
                False, language, error_msg
            )
            
            raise Exception(f"SenseVoice è¯­éŸ³è¯†åˆ«å¤±è´¥: {error_msg}")

class SpeechSynthesizer:
    """è¯­éŸ³åˆæˆå™¨åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.is_initialized = False
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–åˆæˆå™¨"""
        try:
            await self._setup()
            self.is_initialized = True
            return True
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    async def _setup(self):
        """è®¾ç½®åˆæˆå™¨ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        pass
    
    async def synthesize(self, 
                        text: str, 
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        **kwargs) -> Dict[str, Any]:
        """åˆæˆè¯­éŸ³ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        raise NotImplementedError

class CosyVoiceSynthesizer(SpeechSynthesizer):
    """CosyVoice è¯­éŸ³åˆæˆå™¨ - ä¿®å¤éŸ³é¢‘è¾“å‡ºé—®é¢˜"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        # Speakerç®¡ç†
        self.speakers_cache = {}  # ç¼“å­˜å·²åŠ è½½çš„speaker
        self.default_speaker_id = None  # é»˜è®¤speaker ID
        self.speaker_info_file = None  # speakerä¿¡æ¯æ–‡ä»¶è·¯å¾„
        
    async def _setup(self):
        """è®¾ç½® CosyVoice - æ ¹æ®å®˜æ–¹æœ€ä½³å®è·µ"""
        try:
            # è®¾ç½® CosyVoice è·¯å¾„ - ä¿®å¤è·¯å¾„é…ç½®
            import sys
            
            # æ·»åŠ  CosyVoice ä¸»ç›®å½•
            cosyvoice_path = self.config.get('cosyvoice_path', 'tools/CosyVoice')
            if cosyvoice_path not in sys.path:
                sys.path.append(cosyvoice_path)
                
            # æ·»åŠ  Matcha-TTS è·¯å¾„ï¼ˆæ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼‰
            matcha_path = os.path.join(cosyvoice_path, 'third_party/Matcha-TTS')
            if os.path.exists(matcha_path) and matcha_path not in sys.path:
                sys.path.append(matcha_path)
                
            logger.info(f"CosyVoice è·¯å¾„: {cosyvoice_path}")
            logger.info(f"Matcha-TTS è·¯å¾„: {matcha_path}")
            logger.info(f"å½“å‰ Python è·¯å¾„: {sys.path[-2:]}")  # æ˜¾ç¤ºæœ€åæ·»åŠ çš„è·¯å¾„
            
            # å¯¼å…¥ CosyVoice - æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            try:
                from cosyvoice.cli.cosyvoice import CosyVoice2
                from cosyvoice.utils.file_utils import load_wav
                import torchaudio
                logger.info("âœ… CosyVoice æ¨¡å—å¯¼å…¥æˆåŠŸ")
            except ImportError as import_err:
                logger.error(f"âŒ CosyVoice æ¨¡å—å¯¼å…¥å¤±è´¥: {import_err}")
                logger.error(f"æ£€æŸ¥è·¯å¾„: {[p for p in sys.path if 'CosyVoice' in p]}")
                raise
            
            # æ¨¡å‹é…ç½®
            model_dir = self.config.get('model_dir', '/home/rh/Program/MCP_Tools/CosyVoice/pretrained_models/CosyVoice2-0.5B')
            logger.info(f"CosyVoice æ¨¡å‹ç›®å½•: {model_dir}")
            logger.info(f"CosyVoice æ¨¡å‹ç›®å½•å­˜åœ¨: {Path(model_dir).exists()}")

            # æ£€æŸ¥æ¨¡å‹ç›®å½•
            if not Path(model_dir).exists():
                raise FileNotFoundError(f"CosyVoice æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            
            # åˆå§‹åŒ–æ¨¡å‹
            self.model = CosyVoice2(
                model_dir=model_dir,
                load_jit=self.config.get('load_jit', False),
                load_trt=self.config.get('load_trt', False),
                fp16=self.config.get('fp16', False)
            )
            
            # ä¿å­˜å·¥å…·å‡½æ•°
            self.load_wav = load_wav
            self.torchaudio = torchaudio
            
            # è®¾ç½®speakerä¿¡æ¯æ–‡ä»¶è·¯å¾„
            self.speaker_info_file = Path(model_dir) / "custom_speakers.json"
            
            # åŠ è½½å·²ä¿å­˜çš„speakerä¿¡æ¯
            await self._load_saved_speakers()
            
            # è®¾ç½®é»˜è®¤å‚è€ƒéŸ³é¢‘å’Œspeaker
            await self._setup_default_speaker()
            
            logger.info(f"âœ… CosyVoice æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ - æ¨¡å‹è·¯å¾„: {model_dir}")
            logger.info(f"ğŸ“¢ é»˜è®¤speaker ID: {self.default_speaker_id}")
            logger.info(f"ğŸµ æ¨¡å‹é‡‡æ ·ç‡: {self.model.sample_rate}")
            
        except ImportError as e:
            logger.warning(f"âš ï¸ CosyVoice æœªæ­£ç¡®å®‰è£…: {str(e)}")
            self.model = None
        except Exception as e:
            logger.error(f"âŒ CosyVoice åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    async def _load_saved_speakers(self):
        """åŠ è½½å·²ä¿å­˜çš„speakerä¿¡æ¯"""
        try:
            if self.speaker_info_file and Path(self.speaker_info_file).exists():
                with open(self.speaker_info_file, 'r', encoding='utf-8') as f:
                    speaker_data = json.load(f)
                    
                # å°è¯•åŠ è½½ä¿å­˜çš„speakerä¿¡æ¯åˆ°æ¨¡å‹ä¸­
                if hasattr(self.model, 'load_spkinfo') and 'speakers' in speaker_data:
                    try:
                        # CosyVoice2 å¯èƒ½éœ€è¦ç‰¹å®šçš„speakerä¿¡æ¯æ ¼å¼
                        self.model.load_spkinfo()
                        logger.info(f"âœ… å·²åŠ è½½ {len(speaker_data['speakers'])} ä¸ªä¿å­˜çš„speaker")
                    except Exception as e:
                        logger.warning(f"âš ï¸ åŠ è½½speakerä¿¡æ¯å¤±è´¥: {str(e)}")
                
                self.speakers_cache = speaker_data.get('speakers', {})
                self.default_speaker_id = speaker_data.get('default_speaker_id')
                
                logger.info(f"ğŸ“‚ åŠ è½½äº† {len(self.speakers_cache)} ä¸ªç¼“å­˜çš„speaker")
                
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½speakerä¿¡æ¯æ–‡ä»¶å¤±è´¥: {str(e)}")
            self.speakers_cache = {}
    
    async def _setup_default_speaker(self):
        """è®¾ç½®é»˜è®¤speaker"""
        reference_audio_path = self.config.get('reference_audio', None)
        reference_text = self.config.get('reference_text', 'å‚è€ƒéŸ³é¢‘æ–‡æœ¬')
        
        if reference_audio_path and Path(reference_audio_path).exists():
            try:
                # å¦‚æœæ²¡æœ‰é»˜è®¤speakerï¼Œåˆ›å»ºä¸€ä¸ª
                if not self.default_speaker_id:
                    speaker_id = await self._add_speaker(
                        reference_text=reference_text,
                        reference_audio_path=reference_audio_path,
                        speaker_id='default_speaker'
                    )
                    self.default_speaker_id = speaker_id
                    logger.info(f"âœ… åˆ›å»ºé»˜è®¤speaker: {speaker_id}")
                else:
                    logger.info(f"ğŸ“¢ ä½¿ç”¨å·²å­˜åœ¨çš„é»˜è®¤speaker: {self.default_speaker_id}")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ è®¾ç½®é»˜è®¤speakerå¤±è´¥: {str(e)}")
                self.default_speaker_id = None
        else:
            logger.warning("âš ï¸ æœªé…ç½®é»˜è®¤å‚è€ƒéŸ³é¢‘ï¼Œéœ€è¦åœ¨åˆæˆæ—¶æä¾›")
    
    async def _add_speaker(self, reference_text: str, reference_audio_path: str, speaker_id: str = None) -> str:
        """æ·»åŠ æ–°çš„speakeråˆ°æ¨¡å‹ä¸­"""
        try:
            # ç”Ÿæˆspeaker ID
            if not speaker_id:
                speaker_id = f"speaker_{uuid.uuid4().hex[:8]}"
            
            # æ£€æŸ¥speakeræ˜¯å¦å·²å­˜åœ¨
            if speaker_id in self.speakers_cache:
                logger.info(f"ğŸ“¢ Speaker {speaker_id} å·²å­˜åœ¨ï¼Œè·³è¿‡æ·»åŠ ")
                return speaker_id
            
            # ç¡®ä¿éŸ³é¢‘æ ¼å¼æ­£ç¡®
            processed_audio_path = await self._prepare_reference_audio(reference_audio_path)
            
            # åŠ è½½å‚è€ƒéŸ³é¢‘
            reference_audio = self.load_wav(processed_audio_path, self.model.sample_rate)
            
            # æ·»åŠ zero-shot speakeråˆ°æ¨¡å‹
            success = self.model.add_zero_shot_spk(reference_text, reference_audio, speaker_id)
            
            if success:
                # ç¼“å­˜speakerä¿¡æ¯
                self.speakers_cache[speaker_id] = {
                    'reference_text': reference_text,
                    'reference_audio_path': reference_audio_path,
                    'created_at': datetime.utcnow().isoformat()
                }
                
                # ä¿å­˜speakerä¿¡æ¯
                await self._save_speaker_info()
                
                logger.info(f"âœ… æˆåŠŸæ·»åŠ speaker: {speaker_id}")
                return speaker_id
            else:
                raise Exception(f"æ¨¡å‹æ·»åŠ speakerå¤±è´¥: {speaker_id}")
                
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ speakerå¤±è´¥: {str(e)}")
            raise
    
    async def _prepare_reference_audio(self, audio_path: str) -> str:
        """å‡†å¤‡å‚è€ƒéŸ³é¢‘ï¼ˆç¡®ä¿æ ¼å¼æ­£ç¡®ï¼‰"""
        try:
            # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not Path(audio_path).exists():
                raise FileNotFoundError(f"å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            
            # æ£€æŸ¥éŸ³é¢‘æ ¼å¼
            audio_ext = Path(audio_path).suffix.lower()
            
            # å¦‚æœä¸æ˜¯WAVæ ¼å¼ï¼Œè¿›è¡Œè½¬æ¢
            if audio_ext != '.wav':
                logger.info(f"å‚è€ƒéŸ³é¢‘éWAVæ ¼å¼ ({audio_ext})ï¼Œè¿›è¡Œæ ¼å¼è½¬æ¢")
                converted_path = convert_audio_to_wav(audio_path, sample_rate=self.model.sample_rate)
                return converted_path
            else:
                # éªŒè¯é‡‡æ ·ç‡æ˜¯å¦æ­£ç¡®
                try:
                    import librosa
                    y, sr = librosa.load(audio_path, sr=None)
                    if sr != self.model.sample_rate:
                        logger.info(f"å‚è€ƒéŸ³é¢‘é‡‡æ ·ç‡ä¸åŒ¹é… ({sr} vs {self.model.sample_rate})ï¼Œé‡æ–°é‡‡æ ·")
                        converted_path = convert_audio_to_wav(audio_path, sample_rate=self.model.sample_rate)
                        return converted_path
                except Exception:
                    pass  # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨åŸæ–‡ä»¶
                
                return audio_path
                
        except Exception as e:
            logger.error(f"âŒ å‡†å¤‡å‚è€ƒéŸ³é¢‘å¤±è´¥: {str(e)}")
            raise
    
    async def _save_speaker_info(self):
        """ä¿å­˜speakerä¿¡æ¯åˆ°æ–‡ä»¶"""
        try:
            if self.speaker_info_file:
                speaker_data = {
                    'speakers': self.speakers_cache,
                    'default_speaker_id': self.default_speaker_id,
                    'updated_at': datetime.utcnow().isoformat()
                }
                
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                self.speaker_info_file.parent.mkdir(parents=True, exist_ok=True)
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                with open(self.speaker_info_file, 'w', encoding='utf-8') as f:
                    json.dump(speaker_data, f, ensure_ascii=False, indent=2)
                
                # åŒæ—¶ä¿å­˜æ¨¡å‹çš„speakerä¿¡æ¯
                if hasattr(self.model, 'save_spkinfo'):
                    try:
                        self.model.save_spkinfo()
                    except Exception as e:
                        logger.warning(f"âš ï¸ ä¿å­˜æ¨¡å‹speakerä¿¡æ¯å¤±è´¥: {str(e)}")
                
                logger.info(f"ğŸ’¾ Speakerä¿¡æ¯å·²ä¿å­˜åˆ°: {self.speaker_info_file}")
                
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜speakerä¿¡æ¯å¤±è´¥: {str(e)}")
    
    async def synthesize(self, 
                        text: str, 
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        synthesis_mode: str = "zero_shot",
                        reference_audio: Optional[str] = None,
                        reference_text: Optional[str] = None,
                        speaker_id: Optional[str] = None,
                        instruction: Optional[str] = None,
                        **kwargs) -> Dict[str, Any]:
        """CosyVoice è¯­éŸ³åˆæˆ - ä¿®å¤éŸ³é¢‘è¾“å‡ºé—®é¢˜"""
        start_time = time.time()
        
        try:
            if self.model is None:
                # æ¨¡æ‹Ÿåˆæˆç»“æœ
                await asyncio.sleep(1.5)
                
                processing_time = time.time() - start_time
                
                # åˆ›å»ºæ¨¡æ‹ŸéŸ³é¢‘æ•°æ®ï¼ˆé™éŸ³ï¼‰
                sample_rate = 22050
                duration = max(len(text) / 10, 1.0)  # åŸºäºæ–‡æœ¬é•¿åº¦ä¼°ç®—æ—¶é•¿
                audio_samples = int(sample_rate * duration)
                mock_audio = b'\x00' * (audio_samples * 2)  # 16ä½PCM
                
                audio_base64 = base64.b64encode(mock_audio).decode('utf-8')
                
                return {
                    "audio_data": audio_base64,
                    "format": AudioFormat.WAV,
                    "duration": duration,
                    "processing_time": processing_time,
                    "model_used": "mock_cosyvoice",
                    "synthesis_mode": synthesis_mode
                }
            
            # è·å–æˆ–åˆ›å»ºspeaker
            target_speaker_id = await self._get_or_create_speaker(
                speaker_id=speaker_id,
                reference_audio=reference_audio,
                reference_text=reference_text
            )
            
            # æ ¹æ®åˆæˆæ¨¡å¼é€‰æ‹©ä¸åŒçš„æ–¹æ³•
            if synthesis_mode == "zero_shot":
                result = await self._zero_shot_synthesis_with_speaker(text, target_speaker_id, kwargs)
            elif synthesis_mode == "cross_lingual":
                result = await self._cross_lingual_synthesis_with_speaker(text, target_speaker_id, kwargs)
            elif synthesis_mode == "instruct":
                result = await self._instruct_synthesis_with_speaker(text, target_speaker_id, instruction, kwargs)
            else:
                # é»˜è®¤ä½¿ç”¨é›¶æ ·æœ¬åˆæˆ
                result = await self._zero_shot_synthesis_with_speaker(text, target_speaker_id, kwargs)
            
            processing_time = time.time() - start_time
            result["processing_time"] = processing_time
            result["synthesis_mode"] = synthesis_mode
            result["speaker_id"] = target_speaker_id
            
            log_speech_operation(
                logger, "synthesis", "cosyvoice", 
                len(text), len(result["audio_data"]), processing_time, 
                True, language
            )
            
            logger.info(f"âœ… åˆæˆå®Œæˆ - Speaker: {target_speaker_id}, æ¨¡å¼: {synthesis_mode}, æ—¶é•¿: {result.get('duration', 0):.2f}s")
            
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_speech_operation(
                logger, "synthesis", "cosyvoice", 
                len(text), 0, processing_time, 
                False, language, error_msg
            )
            
            raise Exception(f"CosyVoice åˆæˆå¤±è´¥: {error_msg}")
    
    async def _get_or_create_speaker(self, 
                                speaker_id: Optional[str] = None,
                                reference_audio: Optional[str] = None,
                                reference_text: Optional[str] = None) -> str:
        """è·å–æˆ–åˆ›å»ºspeaker - å¢å¼ºéªŒè¯"""
        try:
            # å¦‚æœæŒ‡å®šäº†speaker_idä¸”å­˜åœ¨ï¼ŒéªŒè¯å…¶å®Œæ•´æ€§
            if speaker_id and speaker_id in self.speakers_cache:
                speaker_info = self.speakers_cache[speaker_id]
                
                # éªŒè¯speakerä¿¡æ¯å®Œæ•´æ€§
                if not speaker_info.get('reference_text'):
                    logger.warning(f"âš ï¸ Speaker {speaker_id} ç¼ºå°‘å‚è€ƒæ–‡æœ¬")
                if not speaker_info.get('reference_audio_path') or not Path(speaker_info['reference_audio_path']).exists():
                    logger.warning(f"âš ï¸ Speaker {speaker_id} å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
                    
                logger.info(f"ğŸ“¢ ä½¿ç”¨å·²å­˜åœ¨çš„speaker: {speaker_id}")
                return speaker_id
            
            # å¦‚æœæä¾›äº†æ–°çš„å‚è€ƒéŸ³é¢‘ï¼Œåˆ›å»ºæ–°speaker
            if reference_audio and reference_text:
                new_speaker_id = speaker_id or f"custom_speaker_{uuid.uuid4().hex[:8]}"
                return await self._add_speaker(reference_text, reference_audio, new_speaker_id)
            
            # ä½¿ç”¨é»˜è®¤speaker
            if self.default_speaker_id:
                logger.info(f"ğŸ“¢ ä½¿ç”¨é»˜è®¤speaker: {self.default_speaker_id}")
                return self.default_speaker_id
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•å¯ç”¨speakerï¼ŒæŠ›å‡ºé”™è¯¯
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„speakerï¼Œè¯·æä¾›reference_audioå’Œreference_text")
            
        except Exception as e:
            logger.error(f"âŒ è·å–æˆ–åˆ›å»ºspeakerå¤±è´¥: {str(e)}")
            raise
    
    async def _zero_shot_synthesis_with_speaker(self, text: str, speaker_id: str, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šspeakerè¿›è¡Œé›¶æ ·æœ¬è¯­éŸ³åˆæˆ - ä¿®å¤ç‰ˆæœ¬"""
        try:
            # è·å–speakerçš„å‚è€ƒä¿¡æ¯
            if speaker_id not in self.speakers_cache:
                raise ValueError(f"Speaker {speaker_id} ä¸å­˜åœ¨")
            
            speaker_info = self.speakers_cache[speaker_id]
            reference_text = speaker_info['reference_text']
            reference_audio_path = speaker_info['reference_audio_path']
            
            # å‡†å¤‡å‚è€ƒéŸ³é¢‘
            processed_audio_path = await self._prepare_reference_audio(reference_audio_path)
            reference_audio = self.load_wav(processed_audio_path, self.model.sample_rate)
            
            output_audio = None
            stream = kwargs.get('stream', False)
            
            logger.info(f"ğŸ¤ å¼€å§‹é›¶æ ·æœ¬åˆæˆ - Speaker: {speaker_id}, æ–‡æœ¬: {text[:50]}...")
            logger.info(f"ğŸ“ ä½¿ç”¨å‚è€ƒæ–‡æœ¬: {reference_text[:30]}...")
            
            # æ–¹æ³•1ï¼šä½¿ç”¨å®Œæ•´çš„å‚è€ƒéŸ³é¢‘å’Œæ–‡æœ¬ï¼ˆæ¨èï¼‰
            try:
                for i, result in enumerate(self.model.inference_zero_shot(
                    text, reference_text, reference_audio, stream=stream
                )):
                    output_audio = result['tts_speech']
                    logger.info(f"ğŸ”Š ç”ŸæˆéŸ³é¢‘å¼ é‡å½¢çŠ¶: {output_audio.shape}")
                    logger.info(f"ğŸ”Š éŸ³é¢‘æ•°æ®ç±»å‹: {output_audio.dtype}")
                    logger.info(f"ğŸ”Š éŸ³é¢‘å€¼èŒƒå›´: [{output_audio.min():.6f}, {output_audio.max():.6f}]")
                    if not stream:
                        break
            except Exception as e:
                logger.warning(f"âš ï¸ æ–¹æ³•1å¤±è´¥ï¼Œå°è¯•æ–¹æ³•2: {str(e)}")
                
                # æ–¹æ³•2ï¼šå°è¯•ä½¿ç”¨speaker_idä½†æä¾›å‚è€ƒä¿¡æ¯
                for i, result in enumerate(self.model.inference_zero_shot(
                    text, reference_text, reference_audio, zero_shot_spk_id=speaker_id, stream=stream
                )):
                    output_audio = result['tts_speech']
                    logger.info(f"ğŸ”Š ç”ŸæˆéŸ³é¢‘å¼ é‡å½¢çŠ¶: {output_audio.shape}")
                    if not stream:
                        break
            
            if output_audio is None:
                raise Exception("é›¶æ ·æœ¬åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
            
            return await self._process_output_audio(output_audio)
            
        except Exception as e:
            logger.error(f"âŒ é›¶æ ·æœ¬åˆæˆå¤±è´¥: {str(e)}")
            raise
    
    async def _cross_lingual_synthesis_with_speaker(self, text: str, speaker_id: str, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šspeakerè¿›è¡Œè·¨è¯­è¨€è¯­éŸ³åˆæˆ"""
        try:
            # è·å–speakerçš„å‚è€ƒéŸ³é¢‘
            if speaker_id not in self.speakers_cache:
                raise ValueError(f"Speaker {speaker_id} ä¸å­˜åœ¨")
            
            speaker_info = self.speakers_cache[speaker_id]
            reference_audio_path = speaker_info['reference_audio_path']
            
            # å‡†å¤‡å‚è€ƒéŸ³é¢‘
            processed_audio_path = await self._prepare_reference_audio(reference_audio_path)
            reference_audio = self.load_wav(processed_audio_path, self.model.sample_rate)
            
            # æ‰§è¡Œè·¨è¯­è¨€åˆæˆ
            output_audio = None
            stream = kwargs.get('stream', False)
            
            for i, result in enumerate(self.model.inference_cross_lingual(
                text, reference_audio, stream=stream
            )):
                output_audio = result['tts_speech']
                if not stream:
                    break
            
            if output_audio is None:
                raise Exception("è·¨è¯­è¨€åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
            
            return await self._process_output_audio(output_audio)
            
        except Exception as e:
            logger.error(f"âŒ è·¨è¯­è¨€åˆæˆå¤±è´¥: {str(e)}")
            raise
    
    async def _instruct_synthesis_with_speaker(self, text: str, speaker_id: str, instruction: Optional[str], kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šspeakerè¿›è¡ŒæŒ‡ä»¤å¼è¯­éŸ³åˆæˆ"""
        try:
            # è·å–speakerçš„å‚è€ƒéŸ³é¢‘
            if speaker_id not in self.speakers_cache:
                raise ValueError(f"Speaker {speaker_id} ä¸å­˜åœ¨")
            
            speaker_info = self.speakers_cache[speaker_id]
            reference_audio_path = speaker_info['reference_audio_path']
            
            # å‡†å¤‡å‚è€ƒéŸ³é¢‘
            processed_audio_path = await self._prepare_reference_audio(reference_audio_path)
            reference_audio = self.load_wav(processed_audio_path, self.model.sample_rate)
            
            # ä½¿ç”¨é»˜è®¤æŒ‡ä»¤å¦‚æœæœªæä¾›
            if not instruction:
                instruction = 'ç”¨æ¸©å’Œæ¸…æ™°çš„å£°éŸ³æœ—è¯»'
            
            # æ‰§è¡ŒæŒ‡ä»¤å¼åˆæˆ
            output_audio = None
            stream = kwargs.get('stream', False)
            
            for i, result in enumerate(self.model.inference_instruct2(
                text, instruction, reference_audio, stream=stream
            )):
                output_audio = result['tts_speech']
                if not stream:
                    break
            
            if output_audio is None:
                raise Exception("æŒ‡ä»¤å¼åˆæˆå¤±è´¥ï¼Œæœªç”ŸæˆéŸ³é¢‘")
            
            return await self._process_output_audio(output_audio)
            
        except Exception as e:
            logger.error(f"âŒ æŒ‡ä»¤å¼åˆæˆå¤±è´¥: {str(e)}")
            raise

    async def _process_output_audio(self, output_audio) -> Dict[str, Any]:
        """å¤„ç†è¾“å‡ºéŸ³é¢‘ - ä¿®å¤é‡‡æ ·ç‡ä¸åŒ¹é…é—®é¢˜"""
        import io
        import tempfile
        
        try:
            logger.info(f"ğŸ”§ å¼€å§‹å¤„ç†éŸ³é¢‘ - å¼ é‡å½¢çŠ¶: {output_audio.shape}")
            logger.info(f"ğŸ”§ éŸ³é¢‘æ•°æ®ç±»å‹: {output_audio.dtype}")
            logger.info(f"ğŸ”§ è®¾å¤‡: {output_audio.device}")
            logger.info(f"ğŸ”§ åŸå§‹é‡‡æ ·ç‡: {self.model.sample_rate}")
            
            # ç¡®ä¿éŸ³é¢‘åœ¨CPUä¸Šä¸”ä¸ºæ­£ç¡®çš„æ•°æ®ç±»å‹
            if output_audio.device.type != 'cpu':
                output_audio = output_audio.cpu()
                logger.info("ğŸ“± éŸ³é¢‘å·²ç§»è‡³CPU")
            
            # ç¡®ä¿éŸ³é¢‘ä¸ºfloat32ç±»å‹
            if output_audio.dtype != torch.float32:
                output_audio = output_audio.float()
                logger.info(f"ğŸ”„ éŸ³é¢‘ç±»å‹å·²è½¬æ¢ä¸º: {output_audio.dtype}")
            
            # æ£€æŸ¥éŸ³é¢‘ç»´åº¦ï¼Œç¡®ä¿æ˜¯æ­£ç¡®çš„æ ¼å¼ [channels, samples]
            if len(output_audio.shape) == 1:
                output_audio = output_audio.unsqueeze(0)
                logger.info(f"ğŸ“ æ·»åŠ é€šé“ç»´åº¦: {output_audio.shape}")
            elif len(output_audio.shape) == 3:
                output_audio = output_audio.squeeze(0)
                logger.info(f"ğŸ“ å‹ç¼©ç»´åº¦: {output_audio.shape}")
            
            # æ£€æŸ¥é€šé“æ•°ï¼Œå¦‚æœæ˜¯å¤šé€šé“ï¼Œåªä½¿ç”¨ç¬¬ä¸€ä¸ªé€šé“
            if output_audio.shape[0] > 1:
                output_audio = output_audio[0:1]
                logger.info(f"ğŸµ ä½¿ç”¨å•å£°é“: {output_audio.shape}")
            
            # å½’ä¸€åŒ–éŸ³é¢‘åˆ°åˆé€‚çš„èŒƒå›´
            max_val = output_audio.abs().max()
            if max_val > 1.0:
                output_audio = output_audio / max_val
                logger.info(f"ğŸ”Š éŸ³é¢‘å·²å½’ä¸€åŒ–ï¼Œæœ€å¤§å€¼ä» {max_val:.6f} å½’ä¸€åŒ–åˆ° 1.0")
            
            # **å…³é”®ä¿®å¤ï¼šé‡‡æ ·ç‡é‡é‡‡æ ·**
            target_sample_rate = 22050  # ä½¿ç”¨æ ‡å‡†é‡‡æ ·ç‡
            if self.model.sample_rate != target_sample_rate:
                logger.info(f"ğŸ”„ é‡é‡‡æ ·: {self.model.sample_rate}Hz -> {target_sample_rate}Hz")
                
                # ä½¿ç”¨ torchaudio è¿›è¡Œé‡é‡‡æ ·
                import torchaudio.transforms as T
                resampler = T.Resample(
                    orig_freq=self.model.sample_rate,
                    new_freq=target_sample_rate,
                    dtype=output_audio.dtype
                )
                output_audio = resampler(output_audio)
                logger.info(f"âœ… é‡é‡‡æ ·å®Œæˆ - æ–°å½¢çŠ¶: {output_audio.shape}")
                
                # æ›´æ–°é‡‡æ ·ç‡
                actual_sample_rate = target_sample_rate
            else:
                actual_sample_rate = self.model.sample_rate
            
            # æ£€æŸ¥éŸ³é¢‘æ˜¯å¦åŒ…å«NaNæˆ–æ— ç©·å¤§
            if torch.isnan(output_audio).any():
                logger.error("âŒ æ£€æµ‹åˆ°NaNå€¼ï¼Œç”¨é›¶æ›¿æ¢")
                output_audio = torch.nan_to_num(output_audio, nan=0.0)
            
            if torch.isinf(output_audio).any():
                logger.error("âŒ æ£€æµ‹åˆ°æ— ç©·å¤§å€¼ï¼Œç”¨é›¶æ›¿æ¢")
                output_audio = torch.nan_to_num(output_audio, posinf=0.0, neginf=0.0)
            
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ä¿å­˜éŸ³é¢‘
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                temp_audio_path = temp_file.name
            
            try:
                # ä½¿ç”¨torchaudioä¿å­˜éŸ³é¢‘ï¼Œä½¿ç”¨æ ‡å‡†å‚æ•°
                self.torchaudio.save(
                    temp_audio_path,
                    output_audio,
                    actual_sample_rate,  # ä½¿ç”¨å®é™…çš„é‡‡æ ·ç‡
                    format='wav',
                    encoding='PCM_S',
                    bits_per_sample=16
                )
                
                logger.info(f"ğŸ’¾ éŸ³é¢‘å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {temp_audio_path} (é‡‡æ ·ç‡: {actual_sample_rate})")
                
                # è¯»å–ä¿å­˜çš„WAVæ–‡ä»¶
                with open(temp_audio_path, 'rb') as f:
                    audio_data = f.read()
                
                logger.info(f"ğŸ“ WAVæ–‡ä»¶å¤§å°: {len(audio_data)} å­—èŠ‚")
                
                # **éªŒè¯éŸ³é¢‘æ–‡ä»¶**
                try:
                    import wave
                    with wave.open(temp_audio_path, 'rb') as wav_file:
                        frames = wav_file.getnframes()
                        sample_rate = wav_file.getframerate()
                        channels = wav_file.getnchannels()
                        duration = frames / sample_rate
                        logger.info(f"ğŸ“Š WAVéªŒè¯ - æ—¶é•¿: {duration:.2f}s, é‡‡æ ·ç‡: {sample_rate}, é€šé“: {channels}")
                except Exception as e:
                    logger.warning(f"âš ï¸ WAVæ–‡ä»¶éªŒè¯å¤±è´¥: {str(e)}")
                
                # ç¼–ç ä¸ºbase64
                audio_base64 = base64.b64encode(audio_data).decode('utf-8')
                
                # è®¡ç®—æ—¶é•¿
                duration = output_audio.shape[1] / actual_sample_rate
                
                logger.info(f"âœ… éŸ³é¢‘å¤„ç†å®Œæˆ - æ—¶é•¿: {duration:.2f}s, Base64é•¿åº¦: {len(audio_base64)}, é‡‡æ ·ç‡: {actual_sample_rate}")
                
                test_audio_path = f"/tmp/test_audio_{int(time.time())}.wav"
                try:
                    import shutil
                    shutil.copy2(temp_audio_path, test_audio_path)
                    logger.info(f"ğŸµ æµ‹è¯•éŸ³é¢‘å·²ä¿å­˜åˆ°: {test_audio_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜æµ‹è¯•éŸ³é¢‘å¤±è´¥: {str(e)}")
                
                return {
                    "audio_data": audio_base64,
                    "format": AudioFormat.WAV,
                    "duration": duration,
                    "model_used": "cosyvoice",
                    "sample_rate": actual_sample_rate,  # è¿”å›å®é™…ä½¿ç”¨çš„é‡‡æ ·ç‡
                    "channels": output_audio.shape[0],
                    "audio_shape": list(output_audio.shape)
                }
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    Path(temp_audio_path).unlink(missing_ok=True)
                except Exception:
                    pass
                    
        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")
            logger.error(f"éŸ³é¢‘å¼ é‡ä¿¡æ¯: shape={output_audio.shape}, dtype={output_audio.dtype}")
            raise Exception(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {str(e)}")
    
    async def get_speaker_list(self) -> Dict[str, Any]:
        """è·å–å¯ç”¨çš„speakeråˆ—è¡¨"""
        return {
            'speakers': list(self.speakers_cache.keys()),
            'default_speaker': self.default_speaker_id,
            'count': len(self.speakers_cache)
        }
    
    async def remove_speaker(self, speaker_id: str) -> bool:
        """åˆ é™¤æŒ‡å®šçš„speaker"""
        try:
            if speaker_id in self.speakers_cache:
                del self.speakers_cache[speaker_id]
                
                # å¦‚æœåˆ é™¤çš„æ˜¯é»˜è®¤speakerï¼Œæ¸…é™¤é»˜è®¤è®¾ç½®
                if self.default_speaker_id == speaker_id:
                    self.default_speaker_id = None
                
                # ä¿å­˜æ›´æ–°åçš„speakerä¿¡æ¯
                await self._save_speaker_info()
                
                logger.info(f"âœ… å·²åˆ é™¤speaker: {speaker_id}")
                return True
            else:
                logger.warning(f"âš ï¸ Speakerä¸å­˜åœ¨: {speaker_id}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤speakerå¤±è´¥: {str(e)}")
            return False

class MockSynthesizer(SpeechSynthesizer):
    """æ¨¡æ‹Ÿè¯­éŸ³åˆæˆå™¨"""
    
    async def _setup(self):
        """è®¾ç½®æ¨¡æ‹Ÿåˆæˆå™¨"""
        logger.info("âœ… æ¨¡æ‹Ÿåˆæˆå™¨è®¾ç½®æˆåŠŸ")
    
    async def synthesize(self, 
                       text: str, 
                       voice: Optional[str] = None,
                       language: str = "zh-CN",
                       speed: float = 1.0,
                       pitch: float = 8.0,
                       **kwargs) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿè¯­éŸ³åˆæˆ"""
        logger.info(f"ğŸ”Š å¼€å§‹æ¨¡æ‹Ÿè¯­éŸ³åˆæˆ - æ–‡æœ¬é•¿åº¦: {len(text)}")
        start_time = time.time()
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„é™éŸ³éŸ³é¢‘
        await asyncio.sleep(0.2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        # ç”ŸæˆéŸ³é¢‘æ—¶é•¿
        sample_rate = 16000
        duration = min(len(text) / 5, 10)  # æœ€é•¿10ç§’
        duration = max(duration, 1.0)  # æœ€çŸ­1ç§’
        
        # åˆ›å»ºé™éŸ³éŸ³é¢‘
        samples = int(duration * sample_rate)
        audio_data = bytes(samples * 2)  # 16-bit PCM
        
        processing_time = time.time() - start_time
        
        return {
            "audio_data": base64.b64encode(audio_data).decode('utf-8'),
            "format": AudioFormat.PCM_16,
            "duration": duration,
            "processing_time": processing_time,
            "model_used": "mock_synthesizer"
        }

class SpeechProcessor:
    """è¯­éŸ³å¤„ç†å™¨ä¸»ç±» - ä¿®å¤ç‰ˆæœ¬"""
    
    def __init__(self):
        self.recognizers: Dict[str, SpeechRecognizer] = {}
        self.synthesizers: Dict[str, SpeechSynthesizer] = {}
        self.default_recognizer = None
        self.default_synthesizer = None
        self.is_initialized = False
        
        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è¯»å–é…ç½®
        self.config = self._load_config()

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        return {
            # CosyVoice é…ç½®
            'cosyvoice': {
                'model_dir': os.getenv('COSYVOICE_MODEL_DIR', '/home/rh/Program/MCP_Tools/CosyVoice/pretrained_models/CosyVoice2-0.5B'),
                'cosyvoice_path': os.getenv('COSYVOICE_PATH', 'tools/CosyVoice'),
                'reference_audio': os.getenv('COSYVOICE_REF_AUDIO', None),
                'reference_text': os.getenv('COSYVOICE_REF_TEXT', 'å‚è€ƒéŸ³é¢‘æ–‡æœ¬'),
                'load_jit': os.getenv('COSYVOICE_LOAD_JIT', 'false').lower() == 'true',
                'load_trt': os.getenv('COSYVOICE_LOAD_TRT', 'false').lower() == 'true',
                'fp16': os.getenv('COSYVOICE_FP16', 'false').lower() == 'true'
            },
            # SenseVoice é…ç½®
            'sensevoice': {
                'model': os.getenv('SENSEVOICE_MODEL', 'iic/SenseVoiceSmall'),
                'max_single_segment_time': int(os.getenv('SENSEVOICE_MAX_SEGMENT_TIME', '60000')),
                'batch_size_s': int(os.getenv('SENSEVOICE_BATCH_SIZE_S', '300')),
                'batch_size_threshold_s': int(os.getenv('SENSEVOICE_BATCH_THRESHOLD_S', '60')),
                'ncpu': int(os.getenv('SENSEVOICE_NCPU', '4')),
                'batch_size': int(os.getenv('SENSEVOICE_BATCH_SIZE', '1'))
            },
            # é€šç”¨é…ç½®
            'device': os.getenv('SPEECH_DEVICE', 'cpu')
        }

    async def initialize(self):
        """åˆå§‹åŒ–è¯­éŸ³å¤„ç†å™¨"""
        logger.info("ğŸ”§ åˆå§‹åŒ–è¯­éŸ³å¤„ç†å™¨ - ä¿®å¤ç‰ˆæœ¬")
        
        # å°è¯•åˆå§‹åŒ–å¯ç”¨çš„è¯†åˆ«å™¨
        await self._try_initialize_recognizers()
        
        # å°è¯•åˆå§‹åŒ–å¯ç”¨çš„åˆæˆå™¨
        await self._try_initialize_synthesizers()
        
        self.is_initialized = True
        
        available_recognizers = list(self.recognizers.keys())
        available_synthesizers = list(self.synthesizers.keys())
        
        logger.info(f"âœ… è¯­éŸ³å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - å¯ç”¨è¯†åˆ«å™¨: {available_recognizers}")
        logger.info(f"  - é»˜è®¤è¯†åˆ«å™¨: {self.default_recognizer}")
        logger.info(f"  - å¯ç”¨åˆæˆå™¨: {available_synthesizers}")
        logger.info(f"  - é»˜è®¤åˆæˆå™¨: {self.default_synthesizer}")
        
        if not available_recognizers and not available_synthesizers:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„è¯­éŸ³å¤„ç†å¼•æ“ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

    async def _try_initialize_recognizers(self):
        """å°è¯•åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨"""
        
        # ä¼˜å…ˆå°è¯• SenseVoice
        try:
            config = self.config['sensevoice'].copy()
            config['device'] = self.config['device']
            
            recognizer = SenseVoiceRecognizer(config)
            if await recognizer.initialize():
                self.recognizers['sensevoice'] = recognizer
                if self.default_recognizer is None:
                    self.default_recognizer = 'sensevoice'
                logger.info("âœ… SenseVoice è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ SenseVoice è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
        # å¦‚æœæ²¡æœ‰å¯ç”¨çš„è¯†åˆ«å™¨ï¼Œæ·»åŠ æ¨¡æ‹Ÿè¯†åˆ«å™¨
        if not self.recognizers:
            recognizer = SenseVoiceRecognizer({'device': 'cpu'})  # æ¨¡æ‹Ÿæ¨¡å¼
            await recognizer.initialize()
            self.recognizers['mock'] = recognizer
            self.default_recognizer = 'mock'
            logger.info("âœ… æ¨¡æ‹Ÿè¯†åˆ«å™¨å·²å¯ç”¨")

    async def _try_initialize_synthesizers(self):
        """å°è¯•åˆå§‹åŒ–è¯­éŸ³åˆæˆå™¨"""
        
        # åªåˆå§‹åŒ– CosyVoice
        try:
            config = self.config['cosyvoice'].copy()
            config['device'] = self.config['device']
            
            # æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
            if Path(config['model_dir']).exists():
                synthesizer = CosyVoiceSynthesizer(config)
                if await synthesizer.initialize():
                    self.synthesizers['cosyvoice'] = synthesizer
                    self.default_synthesizer = 'cosyvoice'
                    logger.info("âœ… CosyVoice åˆæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
                else:
                    logger.error("âŒ CosyVoice åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥")
                    raise RuntimeError("CosyVoice åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥")
            else:
                logger.error(f"âŒ CosyVoice æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {config['model_dir']}")
                raise FileNotFoundError(f"CosyVoice æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {config['model_dir']}")
        except Exception as e:
            logger.error(f"âŒ CosyVoice åˆæˆå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    async def recognize(self, 
                       audio_data: bytes,
                       language: str = "zh-CN",
                       model_name: Optional[str] = None,
                       request_id: Optional[str] = None,
                       **kwargs) -> SpeechRecognitionResponse:
        """è¯­éŸ³è¯†åˆ« - ä¼˜åŒ–ç‰ˆ"""
        if not self.is_initialized:
            raise RuntimeError("è¯­éŸ³å¤„ç†å™¨æœªåˆå§‹åŒ–")
        
        # é€‰æ‹©è¯†åˆ«å™¨
        recognizer_name = model_name or self.default_recognizer
        if recognizer_name not in self.recognizers:
            # å›é€€åˆ°é»˜è®¤è¯†åˆ«å™¨
            recognizer_name = self.default_recognizer
            logger.warning(f"âš ï¸ æŒ‡å®šçš„è¯†åˆ«å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è¯†åˆ«å™¨: {recognizer_name}")
        
        recognizer = self.recognizers[recognizer_name]
        
        try:
            logger.info(f"ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ« - æ¨¡å‹: {recognizer_name}, è¯­è¨€: {language}")
            
            result = await recognizer.recognize(audio_data, language, **kwargs)
            
            logger.info(f"âœ… è¯­éŸ³è¯†åˆ«å®Œæˆ - æ–‡æœ¬é•¿åº¦: {len(result['text'])}")
            
            return SpeechRecognitionResponse(
                success=True,
                text=result["text"],
                language=result["language"],
                confidence=result["confidence"],
                processing_time=result["processing_time"],
                model_used=result["model_used"],
                request_id=request_id or generate_response_id(),
                timestamp=datetime.utcnow(),
                segments=result.get("segments", [])
            )
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")
            # è¿”å›é”™è¯¯å“åº”è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return SpeechRecognitionResponse(
                success=False,
                text="",
                language=language,
                confidence=0.0,
                processing_time=0.0,
                model_used=recognizer_name,
                request_id=request_id or generate_response_id(),
                timestamp=datetime.utcnow(),
                message=f"è¯†åˆ«å¤±è´¥: {str(e)}"
            )

    async def synthesize(self, 
                        text: str,
                        voice: Optional[str] = None,
                        language: str = "zh-CN",
                        speed: float = 1.0,
                        pitch: float = 1.0,
                        tts_model: Optional[str] = None,
                        request_id: Optional[str] = None,
                        **kwargs) -> SpeechSynthesisResponse:
        """è¯­éŸ³åˆæˆ - ä¿®å¤ç‰ˆæœ¬ï¼Œæ”¯æŒéŸ³è‰²ä¸€è‡´æ€§"""
        if not self.is_initialized:
            raise RuntimeError("è¯­éŸ³å¤„ç†å™¨æœªåˆå§‹åŒ–")
        
        # å¼ºåˆ¶ä½¿ç”¨CosyVoiceåˆæˆå™¨ï¼Œå¿½ç•¥å…¶ä»–è®¾ç½®
        synthesizer_name = 'cosyvoice'
        if synthesizer_name not in self.synthesizers:
            raise ValueError(f"CosyVoiceè¯­éŸ³åˆæˆå™¨ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿CosyVoice2-0.5bæ¨¡å‹å·²å®‰è£…")
        
        synthesizer = self.synthesizers[synthesizer_name]
        
        try:
            logger.info(f"ğŸ”Š å¼€å§‹è¯­éŸ³åˆæˆ - æ¨¡å‹: {synthesizer_name}, æ–‡æœ¬é•¿åº¦: {len(text)}")
            
            result = await synthesizer.synthesize(
                text=text,
                voice=voice,
                language=language,
                speed=speed,
                pitch=pitch,
                **kwargs
            )
            
            logger.info(f"âœ… è¯­éŸ³åˆæˆå®Œæˆ - éŸ³é¢‘æ—¶é•¿: {result['duration']:.2f}s")
            
            return SpeechSynthesisResponse(
                success=True,
                audio_data=result["audio_data"],
                format=result["format"],
                duration=result["duration"],
                processing_time=result["processing_time"],
                model_used=result["model_used"],
                request_id=request_id or generate_response_id(),
                timestamp=datetime.utcnow(),
                synthesis_mode=result.get("synthesis_mode", "default"),
                sample_rate=result.get("sample_rate", 16000)
            )
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³åˆæˆå¤±è´¥: {str(e)}")
            raise

    async def add_speaker(self, 
                         reference_audio_path: str, 
                         reference_text: str, 
                         speaker_id: Optional[str] = None) -> Dict[str, Any]:
        """æ·»åŠ æ–°çš„speaker"""
        if 'cosyvoice' not in self.synthesizers:
            raise ValueError("CosyVoiceåˆæˆå™¨ä¸å¯ç”¨")
        
        synthesizer = self.synthesizers['cosyvoice']
        if hasattr(synthesizer, '_add_speaker'):
            try:
                new_speaker_id = await synthesizer._add_speaker(reference_text, reference_audio_path, speaker_id)
                return {
                    'success': True,
                    'speaker_id': new_speaker_id,
                    'message': f'æˆåŠŸæ·»åŠ speaker: {new_speaker_id}'
                }
            except Exception as e:
                return {
                    'success': False,
                    'error': str(e),
                    'message': f'æ·»åŠ speakerå¤±è´¥: {str(e)}'
                }
        else:
            return {
                'success': False,
                'error': 'CosyVoiceåˆæˆå™¨ä¸æ”¯æŒæ·»åŠ speaker',
                'message': 'CosyVoiceåˆæˆå™¨ä¸æ”¯æŒæ·»åŠ speaker'
            }

    async def get_speaker_list(self) -> Dict[str, Any]:
        """è·å–å¯ç”¨çš„speakeråˆ—è¡¨"""
        if 'cosyvoice' not in self.synthesizers:
            return {'speakers': [], 'default_speaker': None, 'count': 0}
        
        synthesizer = self.synthesizers['cosyvoice']
        if hasattr(synthesizer, 'get_speaker_list'):
            return await synthesizer.get_speaker_list()
        else:
            return {'speakers': [], 'default_speaker': None, 'count': 0}

    async def remove_speaker(self, speaker_id: str) -> Dict[str, Any]:
        """åˆ é™¤æŒ‡å®šçš„speaker"""
        if 'cosyvoice' not in self.synthesizers:
            return {'success': False, 'error': 'CosyVoiceåˆæˆå™¨ä¸å¯ç”¨'}
        
        synthesizer = self.synthesizers['cosyvoice']
        if hasattr(synthesizer, 'remove_speaker'):
            success = await synthesizer.remove_speaker(speaker_id)
            return {
                'success': success,
                'message': f'Speaker {speaker_id} åˆ é™¤{"æˆåŠŸ" if success else "å¤±è´¥"}'
            }
        else:
            return {'success': False, 'error': 'CosyVoiceåˆæˆå™¨ä¸æ”¯æŒåˆ é™¤speaker'}

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            return {
                "healthy": self.is_initialized and (len(self.recognizers) > 0 or len(self.synthesizers) > 0),
                "recognizers": {
                    "available": list(self.recognizers.keys()),
                    "default": self.default_recognizer,
                    "count": len(self.recognizers)
                },
                "synthesizers": {
                    "available": list(self.synthesizers.keys()),
                    "default": self.default_synthesizer,
                    "count": len(self.synthesizers)
                },
                "config": {
                    "device": self.config['device'],
                    "cosyvoice_model_dir": self.config['cosyvoice']['model_dir'],
                    "sensevoice_model": self.config['sensevoice']['model']
                }
            }
        
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }

    async def get_available_voices(self, synthesizer_name: Optional[str] = None) -> Dict[str, List[str]]:
        """è·å–å¯ç”¨çš„å£°éŸ³åˆ—è¡¨"""
        voices = {}
        
        target_synthesizers = [synthesizer_name] if synthesizer_name else self.synthesizers.keys()
        
        for name in target_synthesizers:
            if name in self.synthesizers:
                synthesizer = self.synthesizers[name]
                if hasattr(synthesizer, 'get_available_voices'):
                    voices[name] = await synthesizer.get_available_voices()
                else:
                    voices[name] = ["default"]
        
        return voices

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†è¯­éŸ³å¤„ç†å™¨èµ„æº")
        
        # æ¸…ç†è¯†åˆ«å™¨
        for recognizer in self.recognizers.values():
            if hasattr(recognizer, 'cleanup'):
                try:
                    await recognizer.cleanup()
                except Exception as e:
                    logger.warning(f"æ¸…ç†è¯†åˆ«å™¨æ—¶å‡ºé”™: {str(e)}")
        
        # æ¸…ç†åˆæˆå™¨
        for synthesizer in self.synthesizers.values():
            if hasattr(synthesizer, 'cleanup'):
                try:
                    await synthesizer.cleanup()
                except Exception as e:
                    logger.warning(f"æ¸…ç†åˆæˆå™¨æ—¶å‡ºé”™: {str(e)}")
        
        self.recognizers.clear()
        self.synthesizers.clear()
        self.default_recognizer = None
        self.default_synthesizer = None
        self.is_initialized = False
        
        logger.info("âœ… è¯­éŸ³å¤„ç†å™¨èµ„æºæ¸…ç†å®Œæˆ")

# å…¨å±€å®ä¾‹
speech_processor = SpeechProcessor()

# ä¾¿æ·å‡½æ•°
async def initialize_speech_processor():
    """åˆå§‹åŒ–å…¨å±€è¯­éŸ³å¤„ç†å™¨"""
    await speech_processor.initialize()

async def recognize_speech(audio_data: bytes, **kwargs) -> SpeechRecognitionResponse:
    """ä¾¿æ·çš„è¯­éŸ³è¯†åˆ«å‡½æ•°"""
    return await speech_processor.recognize(audio_data, **kwargs)

async def synthesize_speech(text: str, **kwargs) -> SpeechSynthesisResponse:
    """ä¾¿æ·çš„è¯­éŸ³åˆæˆå‡½æ•°"""
    return await speech_processor.synthesize(text, **kwargs)

async def add_speaker(reference_audio_path: str, reference_text: str, speaker_id: Optional[str] = None) -> Dict[str, Any]:
    """ä¾¿æ·çš„æ·»åŠ speakerå‡½æ•°"""
    return await speech_processor.add_speaker(reference_audio_path, reference_text, speaker_id)

async def get_speaker_list() -> Dict[str, Any]:
    """ä¾¿æ·çš„è·å–speakeråˆ—è¡¨å‡½æ•°"""
    return await speech_processor.get_speaker_list()

async def remove_speaker(speaker_id: str) -> Dict[str, Any]:
    """ä¾¿æ·çš„åˆ é™¤speakerå‡½æ•°"""
    return await speech_processor.remove_speaker(speaker_id)