"""
MongoDB æ•°æ®ç®¡ç†æ¨¡å—
"""

import os
import io
import json
import base64
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorDatabase, AsyncIOMotorGridFSBucket
from pymongo.errors import DuplicateKeyError, ConnectionFailure
from bson import ObjectId
from bson.errors import InvalidId
import asyncio

from .models import (
    DataDocumentCreate, DataDocumentUpdate, DataDocumentResponse,
    DataDocumentQuery, DataDocumentListResponse, DataItemContent,
    DataDocumentSearchResponse, DataStatisticsResponse
)
from .logger import get_logger

class MongoDBManager:
    """MongoDB æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self):
        self.client: Optional[AsyncIOMotorClient] = None
        self.db: Optional[AsyncIOMotorDatabase] = None
        self.collection_name = "data_documents"
        self.logger = get_logger(__name__)
        
        # é…ç½®
        self.host = os.getenv("MONGODB_HOST", "localhost")
        self.port = int(os.getenv("MONGODB_PORT", "27017"))
        self.database_name = os.getenv("MONGODB_DATABASE", "ai_system")
        self.username = os.getenv("MONGODB_USERNAME")
        self.password = os.getenv("MONGODB_PASSWORD")
        
    async def initialize(self):
        """åˆå§‹åŒ–MongoDBè¿æ¥"""
        try:
            # æ„å»ºè¿æ¥å­—ç¬¦ä¸²
            if self.username and self.password:
                connection_string = f"mongodb://{self.username}:{self.password}@{self.host}:{self.port}/{self.database_name}"
            else:
                connection_string = f"mongodb://{self.host}:{self.port}"
        
            self.logger.info(f"ğŸ”Œ è¿æ¥ MongoDB: {self.host}:{self.port}")
        
            # åˆ›å»ºå®¢æˆ·ç«¯
            self.client = AsyncIOMotorClient(
                connection_string,
                serverSelectionTimeoutMS=5000,
                connectTimeoutMS=10000,
                socketTimeoutMS=10000
            )
            
            # é€‰æ‹©æ•°æ®åº“
            self.db = self.client[self.database_name]
            
            # åˆå§‹åŒ–GridFS
            self.fs = AsyncIOMotorGridFSBucket(self.db)
            
            # æµ‹è¯•è¿æ¥
            await self.client.admin.command('ping')
            self.logger.info("âœ… MongoDB è¿æ¥æˆåŠŸ")
            
            # åˆ›å»ºç´¢å¼•
            await self._create_indexes()
            
            return True
            
        except ConnectionFailure as e:
            self.logger.error(f"âŒ MongoDB è¿æ¥å¤±è´¥: {e}")
            raise
        except Exception as e:
            self.logger.error(f"âŒ MongoDB åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _create_indexes(self):
        """åˆ›å»ºç´¢å¼•"""
        try:
            collection = self.db[self.collection_name]
            
            # åˆ›å»ºç´¢å¼•
            await collection.create_index("name")
            await collection.create_index("tags")
            await collection.create_index("created_at")
            await collection.create_index("updated_at")
            await collection.create_index([("name", "text"), ("description", "text")])
            
            self.logger.info("âœ… MongoDB ç´¢å¼•åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.client:
            self.client.close()
            self.logger.info("âœ… MongoDB è¿æ¥å·²å…³é—­")
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            if not self.client:
                return {"healthy": False, "error": "MongoDB å®¢æˆ·ç«¯æœªåˆå§‹åŒ–"}
            
            # æµ‹è¯•è¿æ¥
            await self.client.admin.command('ping')
            
            # è·å–æ•°æ®åº“çŠ¶æ€
            stats = await self.db.command("dbStats")
            
            return {
                "healthy": True,
                "database": self.database_name,
                "collections": stats.get("collections", 0),
                "objects": stats.get("objects", 0),
                "dataSize": stats.get("dataSize", 0),
                "storageSize": stats.get("storageSize", 0)
            }
            
        except Exception as e:
            return {"healthy": False, "error": str(e)}
    
    # ==================== æ•°æ®æ–‡æ¡£ç®¡ç† ====================
    
    async def create_document(self, document: DataDocumentCreate) -> DataDocumentResponse:
        """åˆ›å»ºæ•°æ®æ–‡æ¡£"""
        try:
            collection = self.db[self.collection_name]
            
            # æ£€æŸ¥åç§°æ˜¯å¦é‡å¤
            existing = await collection.find_one({"name": document.name})
            if existing:
                raise ValueError(f"æ•°æ®æ–‡æ¡£åç§° '{document.name}' å·²å­˜åœ¨")
            
            # å‡†å¤‡æ–‡æ¡£æ•°æ®
            doc_data = document.dict()
            
            # å¤„ç†å›¾ç‰‡æ•°æ®ï¼Œå°†å¤§å›¾ç‰‡å­˜å‚¨åˆ°GridFS
            modified_data_list = []
            for idx, item in enumerate(document.data_list):
                item_dict = item.dict()
                
                # å¦‚æœæœ‰å›¾ç‰‡æ•°æ®ï¼Œå­˜å‚¨åˆ°GridFS
                if item.image:
                    try:
                        # è§£ç base64
                        image_data = base64.b64decode(item.image)
                        
                        # ç”Ÿæˆæ–‡ä»¶å
                        filename = item.image_filename or f"image_{document.name}_{idx}.png"
                        
                        # ä¸Šä¼ åˆ°GridFS
                        file_id = await self.fs.upload_from_stream(
                            filename,
                            io.BytesIO(image_data),
                            metadata={
                                "document_name": document.name,
                                "sequence": item.sequence,
                                "mimetype": item.image_mimetype or "image/png"
                            }
                        )
                        
                        # æ›¿æ¢imageå­—æ®µä¸ºGridFSæ–‡ä»¶IDå¼•ç”¨
                        item_dict["image"] = None
                        item_dict["image_file_id"] = str(file_id)
                        self.logger.info(f"å›¾ç‰‡å·²å­˜å‚¨åˆ°GridFS: {filename}, ID: {file_id}")
                    except Exception as e:
                        self.logger.error(f"å­˜å‚¨å›¾ç‰‡åˆ°GridFSå¤±è´¥: {e}")
                        raise
                
                modified_data_list.append(item_dict)
            
            # æ›´æ–°æ–‡æ¡£æ•°æ®
            doc_data["data_list"] = modified_data_list
            doc_data.update({
                "_id": ObjectId(),
                "created_at": datetime.utcnow(),
                "updated_at": datetime.utcnow(),
                "version": 1
            })
            
            # æ’å…¥æ–‡æ¡£
            result = await collection.insert_one(doc_data)
            
            # è·å–æ’å…¥çš„æ–‡æ¡£
            created_doc = await collection.find_one({"_id": result.inserted_id})
            
            # è½¬æ¢ä¸ºå“åº”æ¨¡å‹
            response_data = dict(created_doc)
            response_data["id"] = str(response_data.pop("_id"))
            
            self.logger.info(f"âœ… åˆ›å»ºæ•°æ®æ–‡æ¡£: {document.name}")
            return DataDocumentResponse(**response_data)
            
        except ValueError:
            raise
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºæ•°æ®æ–‡æ¡£å¤±è´¥: {e}")
            raise
        
    async def get_document(self, document_id: str) -> DataDocumentResponse:
        """è·å–æ•°æ®æ–‡æ¡£"""
        try:
            collection = self.db[self.collection_name]
            
            # æŸ¥æ‰¾æ–‡æ¡£
            document = await collection.find_one({"_id": ObjectId(document_id)})
            
            if not document:
                raise ValueError(f"æœªæ‰¾åˆ°IDä¸º {document_id} çš„æ•°æ®æ–‡æ¡£")
            
            self.logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£ {document_id} çš„å›¾ç‰‡æ•°æ®ï¼Œæ•°æ®é¡¹æ•°é‡: {len(document.get('data_list', []))}")
            
            # å¤„ç†GridFSæ–‡ä»¶å¼•ç”¨
            for idx, item in enumerate(document["data_list"]):
                if "image_file_id" in item and item["image_file_id"]:
                    image_file_id = item["image_file_id"]
                    self.logger.info(f"ğŸ–¼ï¸ å¤„ç†æ•°æ®é¡¹ {idx} (sequence: {item.get('sequence', 'N/A')}) çš„å›¾ç‰‡ï¼ŒGridFS ID: {image_file_id}")
                    
                    try:
                        # éªŒè¯ObjectIdæ ¼å¼
                        if not ObjectId.is_valid(image_file_id):
                            self.logger.error(f"âŒ æ— æ•ˆçš„ObjectIdæ ¼å¼: {image_file_id}")
                            item["image_load_error"] = f"æ— æ•ˆçš„æ–‡ä»¶IDæ ¼å¼: {image_file_id}"
                            continue
                        
                        file_object_id = ObjectId(image_file_id)
                        
                        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                        try:
                            file_info = await self.fs.find({"_id": file_object_id}).to_list(length=1)
                            if not file_info:
                                self.logger.error(f"âŒ GridFSä¸­æœªæ‰¾åˆ°æ–‡ä»¶ID: {image_file_id}")
                                item["image_load_error"] = f"æ–‡ä»¶ä¸å­˜åœ¨: {image_file_id}"
                                continue
                            
                            file_metadata = file_info[0]
                            self.logger.info(f"ğŸ“ æ‰¾åˆ°GridFSæ–‡ä»¶: {file_metadata.get('filename', 'unknown')}, å¤§å°: {file_metadata.get('length', 0)} bytes")
                        except Exception as e:
                            self.logger.error(f"âŒ æ£€æŸ¥GridFSæ–‡ä»¶å­˜åœ¨æ€§å¤±è´¥: {e}")
                            item["image_load_error"] = f"æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {str(e)}"
                            continue
                        
                        # è·å–GridFSä¸­çš„æ–‡ä»¶
                        self.logger.debug(f"ğŸ“¥ å¼€å§‹ä¸‹è½½GridFSæ–‡ä»¶: {image_file_id}")
                        grid_out = await self.fs.open_download_stream(file_object_id)
                        
                        # è¯»å–æ–‡ä»¶å†…å®¹
                        chunks = []
                        total_size = 0
                        async for chunk in grid_out:
                            chunks.append(chunk)
                            total_size += len(chunk)
                        
                        if not chunks:
                            self.logger.error(f"âŒ GridFSæ–‡ä»¶ä¸ºç©º: {image_file_id}")
                            item["image_load_error"] = f"æ–‡ä»¶å†…å®¹ä¸ºç©º: {image_file_id}"
                            continue
                        
                        # è½¬æ¢ä¸ºbase64
                        image_data = b''.join(chunks)
                        image_base64 = base64.b64encode(image_data).decode('utf-8')
                        
                        # éªŒè¯base64æ•°æ®
                        if len(image_base64) < 100:  # å¤ªçŸ­å¯èƒ½ä¸æ˜¯æœ‰æ•ˆå›¾ç‰‡
                            self.logger.warning(f"âš ï¸ å›¾ç‰‡base64æ•°æ®å¯èƒ½æ— æ•ˆï¼Œé•¿åº¦: {len(image_base64)}")
                        
                        item["image"] = image_base64
                        
                        # å¦‚æœæ²¡æœ‰mimetypeï¼Œä»metadataä¸­è·å–
                        if not item.get("image_mimetype") and hasattr(grid_out, "metadata") and grid_out.metadata:
                            item["image_mimetype"] = grid_out.metadata.get("mimetype", "image/png")
                        
                        self.logger.info(f"âœ… æˆåŠŸåŠ è½½å›¾ç‰‡ {image_file_id}, åŸå§‹å¤§å°: {total_size} bytes, Base64é•¿åº¦: {len(image_base64)}")
                        
                        # æ¸…é™¤é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœä¹‹å‰æœ‰çš„è¯ï¼‰
                        if "image_load_error" in item:
                            del item["image_load_error"]
                            
                    except InvalidId as e:
                        error_msg = f"æ— æ•ˆçš„ObjectId: {image_file_id}"
                        self.logger.error(f"âŒ {error_msg}: {e}")
                        item["image_load_error"] = error_msg
                    except Exception as e:
                        error_msg = f"ä»GridFSè·å–å›¾ç‰‡å¤±è´¥: {str(e)}"
                        self.logger.error(f"âŒ æ–‡ä»¶ID {image_file_id}: {error_msg}")
                        item["image_load_error"] = error_msg
                        
                        # æ·»åŠ è¯¦ç»†çš„å¼‚å¸¸ä¿¡æ¯åˆ°æ—¥å¿—
                        import traceback
                        self.logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
                else:
                    # æ²¡æœ‰image_file_idçš„æƒ…å†µ
                    if not item.get("image"):
                        self.logger.debug(f"ğŸ“ æ•°æ®é¡¹ {idx} (sequence: {item.get('sequence', 'N/A')}) æ²¡æœ‰å›¾ç‰‡æ•°æ®")
            
            # è½¬æ¢ä¸ºå“åº”æ¨¡å‹
            response_data = dict(document)
            response_data["id"] = str(response_data.pop("_id"))
            
            self.logger.info(f"âœ… å®Œæˆæ–‡æ¡£ {document_id} çš„å›¾ç‰‡å¤„ç†")
            return DataDocumentResponse(**response_data)
            
        except ValueError:
            raise
        except Exception as e:
            self.logger.error(f"âŒ è·å–æ•°æ®æ–‡æ¡£å¤±è´¥: {e}")
            import traceback
            self.logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            raise
    
    async def update_document(self, document_id: str, update_data: DataDocumentUpdate) -> Optional[DataDocumentResponse]:
        """æ›´æ–°æ•°æ®æ–‡æ¡£"""
        try:
            collection = self.db[self.collection_name]
            
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
            existing = await collection.find_one({"_id": ObjectId(document_id)})
            if not existing:
                return None
            
            # å¦‚æœæ›´æ–°åç§°ï¼Œæ£€æŸ¥æ˜¯å¦é‡å¤
            update_dict = {k: v for k, v in update_data.dict().items() if v is not None}
            if "name" in update_dict and update_dict["name"] != existing["name"]:
                name_exists = await collection.find_one({
                    "name": update_dict["name"],
                    "_id": {"$ne": ObjectId(document_id)}
                })
                if name_exists:
                    raise ValueError(f"æ•°æ®æ–‡æ¡£åç§° '{update_dict['name']}' å·²å­˜åœ¨")
            
            # å¤„ç†å›¾ç‰‡æ•°æ®ï¼Œå°†å¤§å›¾ç‰‡å­˜å‚¨åˆ°GridFS
            if "data_list" in update_dict and update_dict["data_list"]:
                modified_data_list = []
                for idx, item in enumerate(update_dict["data_list"]):
                    # æ£€æŸ¥itemæ˜¯å¦ä¸ºå­—å…¸æˆ–Pydanticæ¨¡å‹
                    if hasattr(item, 'dict'):
                        item_dict = item.dict()
                    else:
                        item_dict = item  # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œåˆ™ç›´æ¥ä½¿ç”¨
                    
                    # å¦‚æœæœ‰å›¾ç‰‡æ•°æ®ï¼Œå­˜å‚¨åˆ°GridFS
                    image_data = None
                    image_field = item_dict.get("image") if isinstance(item_dict, dict) else getattr(item, "image", None)
                    
                    if image_field:
                        try:
                            # è§£ç base64
                            image_data = base64.b64decode(image_field)
                            
                            # ç”Ÿæˆæ–‡ä»¶å
                            doc_name = update_dict.get("name", existing["name"])
                            
                            # è·å–æ–‡ä»¶åå’Œåºå·
                            image_filename = item_dict.get("image_filename", "") if isinstance(item_dict, dict) else getattr(item, "image_filename", "")
                            sequence = item_dict.get("sequence", idx+1) if isinstance(item_dict, dict) else getattr(item, "sequence", idx+1)
                            mimetype = item_dict.get("image_mimetype", "image/png") if isinstance(item_dict, dict) else getattr(item, "image_mimetype", "image/png")
                            
                            # ä½¿ç”¨æä¾›çš„æ–‡ä»¶åæˆ–ç”Ÿæˆä¸€ä¸ª
                            filename = image_filename or f"image_{doc_name}_{idx}.png"
                            
                            # ä¸Šä¼ åˆ°GridFS
                            file_id = await self.fs.upload_from_stream(
                                filename,
                                io.BytesIO(image_data),
                                metadata={
                                    "document_name": doc_name,
                                    "sequence": sequence,
                                    "mimetype": mimetype
                                }
                            )
                            
                            # æ›¿æ¢imageå­—æ®µä¸ºGridFSæ–‡ä»¶IDå¼•ç”¨
                            item_dict["image"] = None
                            item_dict["image_file_id"] = str(file_id)
                            self.logger.info(f"å›¾ç‰‡å·²å­˜å‚¨åˆ°GridFS: {filename}, ID: {file_id}")
                        except Exception as e:
                            self.logger.error(f"å­˜å‚¨å›¾ç‰‡åˆ°GridFSå¤±è´¥: {e}")
                            raise
                    
                    modified_data_list.append(item_dict)
                
                # æ›´æ–°data_listå­—æ®µ
                update_dict["data_list"] = modified_data_list
        
            # æ›´æ–°æ–‡æ¡£
            update_dict.update({
                "updated_at": datetime.utcnow(),
                "version": existing["version"] + 1
            })
            
            result = await collection.update_one(
                {"_id": ObjectId(document_id)},
                {"$set": update_dict}
            )
            
            if result.modified_count == 0:
                return None
            
            # è·å–æ›´æ–°åçš„æ–‡æ¡£
            updated_doc = await collection.find_one({"_id": ObjectId(document_id)})
            
            # è½¬æ¢ä¸ºå“åº”æ¨¡å‹
            response_data = dict(updated_doc)
            response_data["id"] = str(response_data.pop("_id"))
            
            self.logger.info(f"âœ… æ›´æ–°æ•°æ®æ–‡æ¡£: {document_id}")
            return DataDocumentResponse(**response_data)
            
        except ValueError:
            raise
        except Exception as e:
            self.logger.error(f"âŒ æ›´æ–°æ•°æ®æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    async def delete_document(self, document_id: str) -> bool:
        """åˆ é™¤æ•°æ®æ–‡æ¡£"""
        try:
            collection = self.db[self.collection_name]
            
            result = await collection.delete_one({"_id": ObjectId(document_id)})
            
            if result.deleted_count > 0:
                self.logger.info(f"âœ… åˆ é™¤æ•°æ®æ–‡æ¡£: {document_id}")
                return True
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ é™¤æ•°æ®æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    async def list_documents(self, query: DataDocumentQuery) -> DataDocumentListResponse:
        """åˆ—å‡ºæ•°æ®æ–‡æ¡£"""
        try:
            collection = self.db[self.collection_name]
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            filter_dict = {}
            
            if query.name:
                filter_dict["name"] = {"$regex": query.name, "$options": "i"}
            
            if query.tags:
                filter_dict["tags"] = {"$in": query.tags}
            
            # è®¡ç®—æ€»æ•°
            total = await collection.count_documents(filter_dict)
            
            # è®¡ç®—åˆ†é¡µ
            skip = (query.page - 1) * query.page_size
            total_pages = (total + query.page_size - 1) // query.page_size
            
            # æŸ¥è¯¢æ–‡æ¡£
            cursor = collection.find(filter_dict).sort(
                query.sort_by, query.sort_order
            ).skip(skip).limit(query.page_size)
            
            documents = []
            async for doc in cursor:
                response_data = dict(doc)
                response_data["id"] = str(response_data.pop("_id"))
                
                # åˆ—è¡¨æŸ¥è¯¢æ—¶ä¸åŠ è½½å›¾ç‰‡å†…å®¹ï¼Œåªä¿ç•™å…ƒæ•°æ®
                for item in response_data.get("data_list", []):
                    if "image_file_id" in item and item["image_file_id"]:
                        # ä¿ç•™å›¾ç‰‡å…ƒæ•°æ®ï¼Œä½†ä¸åŠ è½½å®é™…å›¾ç‰‡å†…å®¹
                        item["has_image"] = True
                        if not item.get("image"):
                            item["image"] = None  # ç¡®ä¿ä¸ºNoneè€Œä¸æ˜¯æœªå®šä¹‰
                    else:
                        item["has_image"] = False
                
                documents.append(DataDocumentResponse(**response_data))
            
            return DataDocumentListResponse(
                success=True,
                documents=documents,
                total=total,
                page=query.page,
                page_size=query.page_size,
                total_pages=total_pages
            )
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ—å‡ºæ•°æ®æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    async def search_documents(self, search_text: str, limit: int = 10) -> DataDocumentSearchResponse:
        """æœç´¢æ•°æ®æ–‡æ¡£"""
        try:
            collection = self.db[self.collection_name]
            
            start_time = datetime.utcnow()
            
            # ä½¿ç”¨æ–‡æœ¬æœç´¢
            cursor = collection.find(
                {"$text": {"$search": search_text}},
                {"score": {"$meta": "textScore"}}
            ).sort([("score", {"$meta": "textScore"})]).limit(limit)
            
            results = []
            async for doc in cursor:
                response_data = dict(doc)
                response_data["id"] = str(response_data.pop("_id"))
                # ç§»é™¤scoreå­—æ®µ
                response_data.pop("score", None)
                
                # æœç´¢ç»“æœä¹Ÿä¸åŠ è½½å›¾ç‰‡å†…å®¹
                for item in response_data.get("data_list", []):
                    if "image_file_id" in item and item["image_file_id"]:
                        item["has_image"] = True
                        if not item.get("image"):
                            item["image"] = None
                    else:
                        item["has_image"] = False
                
                results.append(DataDocumentResponse(**response_data))
            
            search_time = (datetime.utcnow() - start_time).total_seconds()
            
            return DataDocumentSearchResponse(
                success=True,
                results=results,
                total_matches=len(results),
                search_time=search_time
            )
            
        except Exception as e:
            self.logger.error(f"âŒ æœç´¢æ•°æ®æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    async def get_statistics(self) -> DataStatisticsResponse:
        """è·å–æ•°æ®ç»Ÿè®¡"""
        try:
            collection = self.db[self.collection_name]
            
            # èšåˆç»Ÿè®¡ - æ›´æ–°ç»Ÿè®¡é€»è¾‘ä»¥åŒ…å«GridFSå›¾ç‰‡
            pipeline = [
                {
                    "$group": {
                        "_id": None,
                        "total_documents": {"$sum": 1},
                        "total_items": {"$sum": {"$size": "$data_list"}},
                        "total_images": {
                            "$sum": {
                                "$size": {
                                    "$filter": {
                                        "input": "$data_list",
                                        "cond": {
                                            "$or": [
                                                {"$ne": ["$$this.image", None]},
                                                {"$ne": ["$$this.image_file_id", None]}
                                            ]
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            ]
            
            stats_cursor = collection.aggregate(pipeline)
            stats = await stats_cursor.to_list(length=1)
            
            if not stats:
                stats = [{
                    "total_documents": 0,
                    "total_items": 0,
                    "total_images": 0
                }]
            
            # è·å–æ ‡ç­¾ç»Ÿè®¡
            tag_pipeline = [
                {"$unwind": "$tags"},
                {"$group": {"_id": "$tags", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}},
                {"$limit": 10}
            ]
            
            tag_cursor = collection.aggregate(tag_pipeline)
            most_used_tags = []
            async for tag_stat in tag_cursor:
                most_used_tags.append({
                    "tag": tag_stat["_id"],
                    "count": tag_stat["count"]
                })
            
            # è·å–æœ€è¿‘æ´»åŠ¨
            recent_cursor = collection.find().sort("updated_at", -1).limit(5)
            recent_activity = []
            async for doc in recent_cursor:
                recent_activity.append({
                    "id": str(doc["_id"]),
                    "name": doc["name"],
                    "updated_at": doc["updated_at"],
                    "action": "updated"
                })
            
            # è·å–å­˜å‚¨å¤§å°
            db_stats = await self.db.command("dbStats")
            storage_size = f"{db_stats.get('storageSize', 0) / 1024 / 1024:.2f} MB"
            
            return DataStatisticsResponse(
                success=True,
                total_documents=stats[0]["total_documents"],
                total_items=stats[0]["total_items"],
                total_images=stats[0]["total_images"],
                storage_size=storage_size,
                most_used_tags=most_used_tags,
                recent_activity=recent_activity
            )
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}")
            raise
    
    # ==================== æ•°æ®é¡¹ç®¡ç† ====================
    
    async def add_data_item(self, document_id: str, item: DataItemContent) -> bool:
        """å‘æ–‡æ¡£æ·»åŠ æ•°æ®é¡¹"""
        try:
            collection = self.db[self.collection_name]
            
            # æ£€æŸ¥åºå·æ˜¯å¦é‡å¤
            existing_doc = await collection.find_one({"_id": ObjectId(document_id)})
            if not existing_doc:
                raise ValueError("æ–‡æ¡£ä¸å­˜åœ¨")
            
            existing_sequences = [item["sequence"] for item in existing_doc.get("data_list", [])]
            if item.sequence in existing_sequences:
                raise ValueError(f"åºå· {item.sequence} å·²å­˜åœ¨")
            
            # æ·»åŠ æ•°æ®é¡¹
            result = await collection.update_one(
                {"_id": ObjectId(document_id)},
                {
                    "$push": {"data_list": item.dict()},
                    "$set": {"updated_at": datetime.utcnow()},
                    "$inc": {"version": 1}
                }
            )
            
            if result.modified_count > 0:
                self.logger.info(f"âœ… æ·»åŠ æ•°æ®é¡¹åˆ°æ–‡æ¡£: {document_id}")
                return True
            return False
            
        except ValueError:
            raise
        except Exception as e:
            self.logger.error(f"âŒ æ·»åŠ æ•°æ®é¡¹å¤±è´¥: {e}")
            raise
    
    async def update_data_item(self, document_id: str, sequence: int, item: DataItemContent) -> bool:
        """æ›´æ–°æ–‡æ¡£ä¸­çš„æ•°æ®é¡¹"""
        try:
            collection = self.db[self.collection_name]
            
            result = await collection.update_one(
                {"_id": ObjectId(document_id), "data_list.sequence": sequence},
                {
                    "$set": {
                        "data_list.$": item.dict(),
                        "updated_at": datetime.utcnow()
                    },
                    "$inc": {"version": 1}
                }
            )
            
            if result.modified_count > 0:
                self.logger.info(f"âœ… æ›´æ–°æ•°æ®é¡¹: {document_id}, åºå·: {sequence}")
                return True
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ æ›´æ–°æ•°æ®é¡¹å¤±è´¥: {e}")
            raise
    
    async def delete_data_item(self, document_id: str, sequence: int) -> bool:
        """åˆ é™¤æ–‡æ¡£ä¸­çš„æ•°æ®é¡¹"""
        try:
            collection = self.db[self.collection_name]
            
            result = await collection.update_one(
                {"_id": ObjectId(document_id)},
                {
                    "$pull": {"data_list": {"sequence": sequence}},
                    "$set": {"updated_at": datetime.utcnow()},
                    "$inc": {"version": 1}
                }
            )
            
            if result.modified_count > 0:
                self.logger.info(f"âœ… åˆ é™¤æ•°æ®é¡¹: {document_id}, åºå·: {sequence}")
                return True
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ é™¤æ•°æ®é¡¹å¤±è´¥: {e}")
            raise
    
    # ==================== æ–°å¢è°ƒè¯•æ–¹æ³• ====================
    
    async def debug_gridfs_file(self, file_id: str) -> Dict[str, Any]:
        """è°ƒè¯•GridFSæ–‡ä»¶ä¿¡æ¯"""
        try:
            if not ObjectId.is_valid(file_id):
                return {"error": f"æ— æ•ˆçš„ObjectId: {file_id}"}
            
            file_object_id = ObjectId(file_id)
            
            # æŸ¥æ‰¾æ–‡ä»¶ä¿¡æ¯
            file_info = await self.fs.find({"_id": file_object_id}).to_list(length=1)
            
            if not file_info:
                return {"error": f"GridFSä¸­æœªæ‰¾åˆ°æ–‡ä»¶: {file_id}"}
            
            file_data = file_info[0]
            
            # å°è¯•è¯»å–æ–‡ä»¶å†…å®¹
            try:
                grid_out = await self.fs.open_download_stream(file_object_id)
                chunks = []
                async for chunk in grid_out:
                    chunks.append(chunk)
                
                content_size = sum(len(chunk) for chunk in chunks)
                
                return {
                    "file_id": file_id,
                    "filename": file_data.get("filename", "unknown"),
                    "length": file_data.get("length", 0),
                    "upload_date": file_data.get("uploadDate"),
                    "metadata": file_data.get("metadata", {}),
                    "content_readable": True,
                    "actual_content_size": content_size,
                    "chunks_count": len(chunks)
                }
            except Exception as e:
                return {
                    "file_id": file_id,
                    "filename": file_data.get("filename", "unknown"),
                    "length": file_data.get("length", 0),
                    "upload_date": file_data.get("uploadDate"),
                    "metadata": file_data.get("metadata", {}),
                    "content_readable": False,
                    "read_error": str(e)
                }
                
        except Exception as e:
            return {"error": f"è°ƒè¯•GridFSæ–‡ä»¶å¤±è´¥: {str(e)}"}