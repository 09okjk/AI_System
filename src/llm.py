"""
LLM (Large Language Model) ç®¡ç†å™¨
è´Ÿè´£ç®¡ç†å’Œè°ƒç”¨å„ç§ LLM æ¨¡å‹
"""

import asyncio
import uuid
import time
import json
from typing import Dict, Any, List, Optional, AsyncGenerator
from datetime import datetime
import openai
from anthropic import Anthropic
import aiohttp

from .logger import get_logger, log_llm_call
from .models import LLMConfigCreate, LLMConfigUpdate, LLMConfigResponse, LLMProvider, ChatMessage
from .utils import generate_response_id, session_manager

logger = get_logger(__name__)

class LLMClient:
    """LLM å®¢æˆ·ç«¯åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.client = None
        self.is_initialized = False
        
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        try:
            await self._setup_client()
            self.is_initialized = True
            return True
        except Exception as e:
            logger.error(f"âŒ LLM å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ [{self.config['name']}]: {str(e)}")
            return False
    
    async def _setup_client(self):
        """è®¾ç½®å®¢æˆ·ç«¯ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        raise NotImplementedError
    
    async def chat(self, 
                  message: str, 
                  system_prompt: Optional[str] = None,
                  history: Optional[List[ChatMessage]] = None,
                  **kwargs) -> Dict[str, Any]:
        """èŠå¤©ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        raise NotImplementedError
    
    async def stream_chat(self, 
                         message: str, 
                         system_prompt: Optional[str] = None,
                         history: Optional[List[ChatMessage]] = None,
                         **kwargs) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼èŠå¤©ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        raise NotImplementedError
    
    async def test_connection(self) -> Dict[str, Any]:
        """æµ‹è¯•è¿æ¥"""
        try:
            start_time = time.time()
            result = await self.chat("Hello", system_prompt="Respond with 'OK'")
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "message": "è¿æ¥æµ‹è¯•æˆåŠŸ",
                "response": result.get("content", ""),
                "processing_time": processing_time
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }

class DashScopeClient(LLMClient):
    """é˜¿é‡Œäº‘ DashScope å®¢æˆ·ç«¯"""
    
    async def _setup_client(self):
        """è®¾ç½® DashScope å®¢æˆ·ç«¯"""
        api_key = self.config.get('api_key')
        base_url = self.config.get('base_url', 'https://dashscope.aliyuncs.com/compatible-mode/v1')
        
        if not api_key:
            raise ValueError("DashScope API key is required")
        
        self.client = openai.AsyncOpenAI(
            api_key=api_key,
            base_url=base_url
        )
    
    async def chat(self, 
                  message: str, 
                  system_prompt: Optional[str] = None,
                  history: Optional[List[ChatMessage]] = None,
                  **kwargs) -> Dict[str, Any]:
        """DashScope èŠå¤©"""
        try:
            start_time = time.time()
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            if system_prompt or self.config.get('system_prompt'):
                messages.append({
                    "role": "system",
                    "content": system_prompt or self.config.get('system_prompt')
                })
            
            # æ·»åŠ å†å²æ¶ˆæ¯
            if history:
                for msg in history:
                    messages.append({
                        "role": msg.role,
                        "content": msg.content
                    })
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append({
                "role": "user",
                "content": message
            })
            
            # è°ƒç”¨ API
            response = await self.client.chat.completions.create(
                model=self.config['model_name'],
                messages=messages,
                temperature=self.config.get('temperature', 0.7),
                top_p=self.config.get('top_p', 0.9),
                max_tokens=self.config.get('max_tokens', 2000),
                **kwargs
            )
            
            processing_time = time.time() - start_time
            
            # æå–å“åº”å†…å®¹
            content = response.choices[0].message.content
            
            # è®¡ç®— token ä½¿ç”¨é‡
            token_usage = {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
            
            # è®°å½•æ—¥å¿—
            log_llm_call(
                logger, self.config['name'], 
                token_usage["prompt_tokens"], 
                token_usage["completion_tokens"],
                processing_time, True
            )
            
            return {
                "content": content,
                "model_name": self.config['name'],
                "processing_time": processing_time,
                "token_usage": token_usage
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_llm_call(
                logger, self.config['name'], 0, 0, 
                processing_time, False, error_msg
            )
            
            raise e
    
    async def stream_chat(self, 
                         message: str, 
                         system_prompt: Optional[str] = None,
                         history: Optional[List[ChatMessage]] = None,
                         **kwargs) -> AsyncGenerator[Dict[str, Any], None]:
        """DashScope æµå¼èŠå¤©"""
        try:
            start_time = time.time()
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            if system_prompt or self.config.get('system_prompt'):
                messages.append({
                    "role": "system",
                    "content": system_prompt or self.config.get('system_prompt')
                })
            
            if history:
                for msg in history:
                    messages.append({
                        "role": msg.role,
                        "content": msg.content
                    })
            
            messages.append({
                "role": "user",
                "content": message
            })
            
            # æµå¼è°ƒç”¨ API
            stream = await self.client.chat.completions.create(
                model=self.config['model_name'],
                messages=messages,
                temperature=self.config.get('temperature', 0.7),
                top_p=self.config.get('top_p', 0.9),
                max_tokens=self.config.get('max_tokens', 2000),
                stream=True,
                **kwargs
            )
            
            full_content = ""
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    content_chunk = chunk.choices[0].delta.content
                    full_content += content_chunk
                    
                    yield {
                        "type": "content_chunk",
                        "content": content_chunk,
                        "full_content": full_content
                    }
            
            processing_time = time.time() - start_time
            
            # å‘é€å®Œæˆä¿¡å·
            yield {
                "type": "done",
                "content": full_content,
                "model_name": self.config['name'],
                "processing_time": processing_time
            }
            
            log_llm_call(
                logger, self.config['name'], 0, len(full_content), 
                processing_time, True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_llm_call(
                logger, self.config['name'], 0, 0, 
                processing_time, False, error_msg
            )
            
            yield {
                "type": "error",
                "error": error_msg
            }

class XinferenceClient(LLMClient):
    """Xinference å®¢æˆ·ç«¯"""
    
    async def _setup_client(self):
        """è®¾ç½® Xinference å®¢æˆ·ç«¯"""
        base_url = self.config.get('base_url', 'http://localhost:9997/v1')
        
        self.client = openai.AsyncOpenAI(
            api_key="xinference",  # Xinference ä¸éœ€è¦çœŸå®çš„ API key
            base_url=base_url
        )
    
    async def chat(self, 
                  message: str, 
                  system_prompt: Optional[str] = None,
                  history: Optional[List[ChatMessage]] = None,
                  **kwargs) -> Dict[str, Any]:
        """Xinference èŠå¤©"""
        try:
            start_time = time.time()
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            if system_prompt or self.config.get('system_prompt'):
                messages.append({
                    "role": "system",
                    "content": system_prompt or self.config.get('system_prompt')
                })
            
            if history:
                for msg in history:
                    messages.append({
                        "role": msg.role,
                        "content": msg.content
                    })
            
            messages.append({
                "role": "user",
                "content": message
            })
            
            logger.info(f"è°ƒç”¨ LLM æ¨¡å‹ [è¯·æ±‚ID: {request_id}], messages: {messages}")
            # è°ƒç”¨ API
            response = await self.client.chat.completions.create(
                model=self.config['model_name'],
                messages=messages,
                temperature=self.config.get('temperature', 0.7),
                top_p=self.config.get('top_p', 0.9),
                max_tokens=self.config.get('max_tokens', 2000),
                **kwargs
            )
            
            processing_time = time.time() - start_time
            
            content = response.choices[0].message.content
            
            # Xinference å¯èƒ½ä¸è¿”å›è¯¦ç»†çš„ token ä½¿ç”¨ä¿¡æ¯
            token_usage = {
                "prompt_tokens": getattr(response.usage, 'prompt_tokens', 0),
                "completion_tokens": getattr(response.usage, 'completion_tokens', 0),
                "total_tokens": getattr(response.usage, 'total_tokens', 0)
            }
            
            log_llm_call(
                logger, self.config['name'], 
                token_usage["prompt_tokens"], 
                token_usage["completion_tokens"],
                processing_time, True
            )
            
            return {
                "content": content,
                "model_name": self.config['name'],
                "processing_time": processing_time,
                "token_usage": token_usage
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_llm_call(
                logger, self.config['name'], 0, 0, 
                processing_time, False, error_msg
            )
            
            raise e
    
    async def stream_chat(self, 
                         message: str, 
                         system_prompt: Optional[str] = None,
                         history: Optional[List[ChatMessage]] = None,
                         **kwargs) -> AsyncGenerator[Dict[str, Any], None]:
        """Xinference æµå¼èŠå¤©"""
        # å®ç°ä¸ DashScope ç±»ä¼¼çš„æµå¼é€»è¾‘
        try:
            start_time = time.time()
            
            messages = []
            
            if system_prompt or self.config.get('system_prompt'):
                messages.append({
                    "role": "system",
                    "content": system_prompt or self.config.get('system_prompt')
                })
            
            if history:
                for msg in history:
                    messages.append({
                        "role": msg.role,
                        "content": msg.content
                    })
            
            messages.append({
                "role": "user",
                "content": message
            })
            
            stream = await self.client.chat.completions.create(
                model=self.config['model_name'],
                messages=messages,
                temperature=self.config.get('temperature', 0.7),
                top_p=self.config.get('top_p', 0.9),
                max_tokens=self.config.get('max_tokens', 2000),
                stream=True,
                **kwargs
            )
            
            full_content = ""
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    content_chunk = chunk.choices[0].delta.content
                    full_content += content_chunk
                    
                    yield {
                        "type": "content_chunk",
                        "content": content_chunk,
                        "full_content": full_content
                    }
            
            processing_time = time.time() - start_time
            
            yield {
                "type": "done",
                "content": full_content,
                "model_name": self.config['name'],
                "processing_time": processing_time
            }
            
            log_llm_call(
                logger, self.config['name'], 0, len(full_content), 
                processing_time, True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_llm_call(
                logger, self.config['name'], 0, 0, 
                processing_time, False, error_msg
            )
            
            yield {
                "type": "error",
                "error": error_msg
            }

class OpenAIClient(LLMClient):
    """OpenAI å®¢æˆ·ç«¯"""
    
    async def _setup_client(self):
        """è®¾ç½® OpenAI å®¢æˆ·ç«¯"""
        api_key = self.config.get('api_key')
        base_url = self.config.get('base_url')
        
        if not api_key:
            raise ValueError("OpenAI API key is required")
        
        kwargs = {"api_key": api_key}
        if base_url:
            kwargs["base_url"] = base_url
        
        self.client = openai.AsyncOpenAI(**kwargs)
    
    async def chat(self, 
                  message: str, 
                  system_prompt: Optional[str] = None,
                  history: Optional[List[ChatMessage]] = None,
                  **kwargs) -> Dict[str, Any]:
        """OpenAI èŠå¤©"""
        # å®ç°ä¸ DashScope ç±»ä¼¼çš„é€»è¾‘
        try:
            start_time = time.time()
            
            messages = []
            
            if system_prompt or self.config.get('system_prompt'):
                messages.append({
                    "role": "system",
                    "content": system_prompt or self.config.get('system_prompt')
                })
            
            if history:
                for msg in history:
                    messages.append({
                        "role": msg.role,
                        "content": msg.content
                    })
            
            messages.append({
                "role": "user",
                "content": message
            })
            
            response = await self.client.chat.completions.create(
                model=self.config['model_name'],
                messages=messages,
                temperature=self.config.get('temperature', 0.7),
                top_p=self.config.get('top_p', 0.9),
                max_tokens=self.config.get('max_tokens', 2000),
                **kwargs
            )
            
            processing_time = time.time() - start_time
            
            content = response.choices[0].message.content
            
            token_usage = {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
            
            log_llm_call(
                logger, self.config['name'], 
                token_usage["prompt_tokens"], 
                token_usage["completion_tokens"],
                processing_time, True
            )
            
            return {
                "content": content,
                "model_name": self.config['name'],
                "processing_time": processing_time,
                "token_usage": token_usage
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_llm_call(
                logger, self.config['name'], 0, 0, 
                processing_time, False, error_msg
            )
            
            raise e
    
    async def stream_chat(self, 
                         message: str, 
                         system_prompt: Optional[str] = None,
                         history: Optional[List[ChatMessage]] = None,
                         **kwargs) -> AsyncGenerator[Dict[str, Any], None]:
        """OpenAI æµå¼èŠå¤©"""
        # ä¸å…¶ä»–å®¢æˆ·ç«¯ç±»ä¼¼çš„å®ç°
        try:
            start_time = time.time()
            
            messages = []
            
            if system_prompt or self.config.get('system_prompt'):
                messages.append({
                    "role": "system",
                    "content": system_prompt or self.config.get('system_prompt')
                })
            
            if history:
                for msg in history:
                    messages.append({
                        "role": msg.role,
                        "content": msg.content
                    })
            
            messages.append({
                "role": "user",
                "content": message
            })
            
            stream = await self.client.chat.completions.create(
                model=self.config['model_name'],
                messages=messages,
                temperature=self.config.get('temperature', 0.7),
                top_p=self.config.get('top_p', 0.9),
                max_tokens=self.config.get('max_tokens', 2000),
                stream=True,
                **kwargs
            )
            
            full_content = ""
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    content_chunk = chunk.choices[0].delta.content
                    full_content += content_chunk
                    
                    yield {
                        "type": "content_chunk",
                        "content": content_chunk,
                        "full_content": full_content
                    }
            
            processing_time = time.time() - start_time
            
            yield {
                "type": "done",
                "content": full_content,
                "model_name": self.config['name'],
                "processing_time": processing_time
            }
            
            log_llm_call(
                logger, self.config['name'], 0, len(full_content), 
                processing_time, True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            log_llm_call(
                logger, self.config['name'], 0, 0, 
                processing_time, False, error_msg
            )
            
            yield {
                "type": "error",
                "error": error_msg
            }

class LLMManager:
    """LLM ç®¡ç†å™¨"""
    
    def __init__(self):
        self.configs: Dict[str, Dict[str, Any]] = {}
        self.clients: Dict[str, LLMClient] = {}
        self.is_initialized = False
        
        # å®¢æˆ·ç«¯ç±»å‹æ˜ å°„
        self.client_classes = {
            LLMProvider.DASHSCOPE: DashScopeClient,
            LLMProvider.XINFERENCE: XinferenceClient,
            LLMProvider.OPENAI: OpenAIClient,
            # å¯ä»¥æ·»åŠ æ›´å¤šæä¾›å•†
        }
    
    async def initialize(self, configs: Dict[str, Dict[str, Any]]):
        """åˆå§‹åŒ– LLM ç®¡ç†å™¨"""
        logger.info("ğŸ”§ åˆå§‹åŒ– LLM ç®¡ç†å™¨")
        
        self.configs = configs.copy()
        
        # åˆå§‹åŒ–å¯ç”¨çš„ LLM å®¢æˆ·ç«¯
        for config_id, config in self.configs.items():
            if config.get('enabled', True):
                await self.initialize_client(config_id)
        
        self.is_initialized = True
        logger.info(f"âœ… LLM ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ - å…± {len(self.configs)} ä¸ªé…ç½®")
    
    async def initialize_client(self, config_id: str) -> bool:
        """åˆå§‹åŒ–æŒ‡å®šçš„ LLM å®¢æˆ·ç«¯"""
        if config_id not in self.configs:
            logger.error(f"âŒ LLM é…ç½®ä¸å­˜åœ¨: {config_id}")
            return False
        
        config = self.configs[config_id]
        provider = LLMProvider(config['provider'])
        
        if provider not in self.client_classes:
            logger.error(f"âŒ ä¸æ”¯æŒçš„ LLM æä¾›å•†: {provider}")
            return False
        
        try:
            # åˆ›å»ºå®¢æˆ·ç«¯
            client_class = self.client_classes[provider]
            client = client_class(config)
            
            # åˆå§‹åŒ–å®¢æˆ·ç«¯
            success = await client.initialize()
            
            if success:
                self.clients[config_id] = client
                logger.info(f"âœ… LLM å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {config['name']}")
                
                # æ›´æ–°é…ç½®çŠ¶æ€
                self.configs[config_id]['status'] = 'active'
            else:
                logger.error(f"âŒ LLM å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {config['name']}")
                self.configs[config_id]['status'] = 'error'
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ LLM å®¢æˆ·ç«¯åˆå§‹åŒ–å¼‚å¸¸ [{config['name']}]: {str(e)}")
            self.configs[config_id]['status'] = 'error'
            return False
    
    async def get_all_configs(self) -> List[LLMConfigResponse]:
        """è·å–æ‰€æœ‰ LLM é…ç½®"""
        configs = []
        
        for config_id, config in self.configs.items():
            config_response = LLMConfigResponse(
                id=config_id,
                name=config['name'],
                provider=LLMProvider(config['provider']),
                model_name=config['model_name'],
                api_key=config.get('api_key', ''),
                base_url=config.get('base_url'),
                max_tokens=config.get('max_tokens'),
                temperature=config.get('temperature', 0.7),
                top_p=config.get('top_p', 0.9),
                system_prompt=config.get('system_prompt'),
                is_default=config.get('is_default', False),
                enabled=config.get('enabled', True),
                status=config.get('status', 'inactive'),
                created_at=datetime.fromisoformat(config.get('created_at', datetime.utcnow().isoformat())),
                updated_at=datetime.fromisoformat(config.get('updated_at', datetime.utcnow().isoformat())),
                last_used=datetime.fromisoformat(config['last_used']) if config.get('last_used') else None
            )
            
            configs.append(config_response)
        
        return configs
    
    async def create_config(self, config_create: LLMConfigCreate) -> LLMConfigResponse:
        """åˆ›å»º LLM é…ç½®"""
        config_id = str(uuid.uuid4())
        now = datetime.utcnow()
        
        config_dict = {
            "id": config_id,
            "name": config_create.name,
            "provider": config_create.provider.value,
            "model_name": config_create.model_name,
            "api_key": config_create.api_key,
            "base_url": config_create.base_url,
            "max_tokens": config_create.max_tokens,
            "temperature": config_create.temperature,
            "top_p": config_create.top_p,
            "system_prompt": config_create.system_prompt,
            "is_default": config_create.is_default,
            "enabled": config_create.enabled,
            "status": "inactive",
            "created_at": now.isoformat(),
            "updated_at": now.isoformat()
        }
        
        self.configs[config_id] = config_dict
        
        # å¦‚æœè®¾ç½®ä¸ºå¯ç”¨ï¼Œç«‹å³åˆå§‹åŒ–
        if config_create.enabled:
            await self.initialize_client(config_id)
        
        return LLMConfigResponse(
            id=config_id,
            status=self.configs[config_id]['status'],
            created_at=now,
            updated_at=now,
            **config_create.dict()
        )
    
    async def update_config(self, config_id: str, config_update: LLMConfigUpdate) -> LLMConfigResponse:
        """æ›´æ–° LLM é…ç½®"""
        if config_id not in self.configs:
            raise ValueError(f"LLM é…ç½®ä¸å­˜åœ¨: {config_id}")
        
        config = self.configs[config_id].copy()
        
        # æ›´æ–°å­—æ®µ
        update_dict = config_update.dict(exclude_unset=True)
        for key, value in update_dict.items():
            if key == "provider" and value:
                config[key] = value.value
            elif value is not None:
                config[key] = value
        
        config["updated_at"] = datetime.utcnow().isoformat()
        self.configs[config_id] = config
        
        # å¦‚æœå®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œé‡æ–°åˆå§‹åŒ–ä»¥åº”ç”¨æ–°é…ç½®
        if config_id in self.clients:
            del self.clients[config_id]
            if config.get('enabled', True):
                await self.initialize_client(config_id)
        
        return LLMConfigResponse(
            id=config_id,
            status=self.configs[config_id]['status'],
            created_at=datetime.fromisoformat(config['created_at']),
            updated_at=datetime.fromisoformat(config['updated_at']),
            name=config['name'],
            provider=LLMProvider(config['provider']),
            model_name=config['model_name'],
            api_key=config.get('api_key', ''),
            base_url=config.get('base_url'),
            max_tokens=config.get('max_tokens'),
            temperature=config.get('temperature', 0.7),
            top_p=config.get('top_p', 0.9),
            system_prompt=config.get('system_prompt'),
            is_default=config.get('is_default', False),
            enabled=config.get('enabled', True),
            last_used=datetime.fromisoformat(config['last_used']) if config.get('last_used') else None
        )
    
    async def delete_config(self, config_id: str):
        """åˆ é™¤ LLM é…ç½®"""
        if config_id in self.configs:
            # åœæ­¢å®¢æˆ·ç«¯
            if config_id in self.clients:
                del self.clients[config_id]
            
            # åˆ é™¤é…ç½®
            del self.configs[config_id]
            
            logger.info(f"ğŸ—‘ï¸ åˆ é™¤ LLM é…ç½®: {config_id}")
    
    async def get_config(self, config_id: str) -> Optional[Dict[str, Any]]:
        """è·å–æŒ‡å®š LLM é…ç½®"""
        return self.configs.get(config_id)
    
    async def test_config(self, config_id: str) -> Dict[str, Any]:
        """æµ‹è¯• LLM é…ç½®"""
        if config_id not in self.configs:
            return {"success": False, "error": "é…ç½®ä¸å­˜åœ¨"}
        
        config = self.configs[config_id]
        provider = LLMProvider(config['provider'])
        
        if provider not in self.client_classes:
            return {"success": False, "error": f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}"}
        
        try:
            # åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯è¿›è¡Œæµ‹è¯•
            client_class = self.client_classes[provider]
            test_client = client_class(config)
            
            success = await test_client.initialize()
            if not success:
                return {"success": False, "error": "å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥"}
            
            # æµ‹è¯•è¿æ¥
            test_result = await test_client.test_connection()
            return test_result
        
        except Exception as e:
            return {"success": False, "error": f"æµ‹è¯•å¤±è´¥: {str(e)}"}
    
    async def chat(self, 
                  model_name: Optional[str],
                  message: str,
                  system_prompt: Optional[str] = None,
                  session_id: Optional[str] = None,
                  stream: bool = False,
                  tools: Optional[List[str]] = None,
                  request_id: Optional[str] = None,
                  **kwargs) -> Dict[str, Any]:
        """èŠå¤©"""
        
        # é€‰æ‹©æ¨¡å‹
        client = await self._get_client(model_name)
        if not client:
            raise ValueError(f"æ¨¡å‹ä¸å¯ç”¨: {model_name}")
        
        # è·å–ä¼šè¯å†å²
        history = []
        if session_id:
            history_msgs = await session_manager.get_messages(session_id, limit=10)
            history = [ChatMessage(role=msg['role'], content=msg['content']) for msg in history_msgs]
        
        try:
            # æ‰§è¡ŒèŠå¤©
            result = await client.chat(
                message=message,
                system_prompt=system_prompt,
                history=history,
                **kwargs
            )
            
            # ä¿å­˜åˆ°ä¼šè¯
            if session_id:
                await session_manager.add_message(session_id, "user", message)
                await session_manager.add_message(session_id, "assistant", result["content"])
            
            # æ›´æ–°é…ç½®ä½¿ç”¨æ—¶é—´
            config_id = self._get_config_id_by_client(client)
            if config_id:
                self.configs[config_id]['last_used'] = datetime.utcnow().isoformat()
            
            result['session_id'] = session_id or generate_response_id()
            return result
            
        except Exception as e:
            logger.error(f"âŒ LLM èŠå¤©å¤±è´¥: {str(e)}")
            raise
    
    async def stream_chat(self, 
                         model_name: Optional[str],
                         message: str,
                         system_prompt: Optional[str] = None,
                         session_id: Optional[str] = None,
                         tools: Optional[List[str]] = None,
                         request_id: Optional[str] = None,
                         **kwargs) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼èŠå¤©"""
        
        # é€‰æ‹©æ¨¡å‹
        client = await self._get_client(model_name)
        if not client:
            yield {"type": "error", "error": f"æ¨¡å‹ä¸å¯ç”¨: {model_name}"}
            return
        
        # è·å–ä¼šè¯å†å²
        history = []
        if session_id:
            history_msgs = await session_manager.get_messages(session_id, limit=10)
            history = [ChatMessage(role=msg['role'], content=msg['content']) for msg in history_msgs]
        
        try:
            full_content = ""
            
            async for chunk in client.stream_chat(
                message=message,
                system_prompt=system_prompt,
                history=history,
                **kwargs
            ):
                if chunk.get("type") == "content_chunk":
                    full_content = chunk.get("full_content", "")
                
                chunk['session_id'] = session_id or generate_response_id()
                yield chunk
            
            # ä¿å­˜åˆ°ä¼šè¯
            if session_id and full_content:
                await session_manager.add_message(session_id, "user", message)
                await session_manager.add_message(session_id, "assistant", full_content)
            
            # æ›´æ–°é…ç½®ä½¿ç”¨æ—¶é—´
            config_id = self._get_config_id_by_client(client)
            if config_id:
                self.configs[config_id]['last_used'] = datetime.utcnow().isoformat()
            
        except Exception as e:
            logger.error(f"âŒ LLM æµå¼èŠå¤©å¤±è´¥: {str(e)}")
            yield {"type": "error", "error": str(e)}
    
    async def _get_client(self, model_name: Optional[str]) -> Optional[LLMClient]:
        """è·å–å®¢æˆ·ç«¯"""
        if model_name:
            # æ ¹æ®æ¨¡å‹åç§°æŸ¥æ‰¾
            for config_id, config in self.configs.items():
                if config['name'] == model_name and config_id in self.clients:
                    return self.clients[config_id]
        
        # ä½¿ç”¨é»˜è®¤æ¨¡å‹
        for config_id, config in self.configs.items():
            if config.get('is_default') and config_id in self.clients:
                return self.clients[config_id]
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
        for config_id in self.clients:
            return self.clients[config_id]
        
        return None
    
    def _get_config_id_by_client(self, client: LLMClient) -> Optional[str]:
        """æ ¹æ®å®¢æˆ·ç«¯è·å–é…ç½®ID"""
        for config_id, c in self.clients.items():
            if c == client:
                return config_id
        return None
    
    async def get_models_status(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰æ¨¡å‹çŠ¶æ€"""
        status = {
            "total_configs": len(self.configs),
            "active_clients": len(self.clients),
            "models": {}
        }
        
        for config_id, config in self.configs.items():
            status["models"][config_id] = {
                "name": config['name'],
                "provider": config['provider'],
                "is_active": config_id in self.clients,
                "is_default": config.get('is_default', False),
                "enabled": config.get('enabled', True),
                "last_used": config.get('last_used')
            }
        
        return status
    
    async def get_active_sessions_count(self) -> int:
        """è·å–æ´»è·ƒä¼šè¯æ•°é‡"""
        return await session_manager.get_active_sessions_count()
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            enabled_count = len([c for c in self.configs.values() if c.get('enabled', True)])
            active_count = len(self.clients)
            
            return {
                "healthy": active_count > 0,
                "total_configs": len(self.configs),
                "enabled_configs": enabled_count,
                "active_clients": active_count
            }
        
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e)
            }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç† LLM ç®¡ç†å™¨èµ„æº")
        
        # æ¸…ç†æ‰€æœ‰å®¢æˆ·ç«¯
        self.clients.clear()
        self.configs.clear()
        self.is_initialized = False
        
        logger.info("âœ… LLM ç®¡ç†å™¨èµ„æºæ¸…ç†å®Œæˆ")